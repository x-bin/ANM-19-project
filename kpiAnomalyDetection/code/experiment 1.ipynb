{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np   \n",
    "import sklearn\n",
    "import keras\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../input/train.csv')\n",
    "test_data = pd.read_csv('../../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.90163934, 1.78688526, 2.        , ..., 2.61403513, 2.73684216,\n",
      "       2.4912281 ]), array([27.23077011, 29.84615326, 28.61538506, ..., 37.25      ,\n",
      "       36.66666794, 37.        ]), array([1.66666663, 1.83333337, 1.75      , ..., 5.        , 4.91666651,\n",
      "       4.66666651]), array([0., 0., 0., ..., 0., 0., 0.]), array([2.25      , 3.1500001 , 3.04999995, ..., 5.44999981, 6.80000019,\n",
      "       5.69999981]), array([9.13373376e+08, 9.01925824e+08, 9.15506240e+08, ...,\n",
      "       9.60516800e+08, 9.61848768e+08, 9.62367296e+08]), array([1.06734995e+09, 1.05601555e+09, 1.05589376e+09, ...,\n",
      "       1.00463066e+09, 1.09299085e+09, 1.08772019e+09]), array([1204.625 , 1182.25  , 1155.1875, ..., 1371.375 , 1381.0625,\n",
      "       1376.9375]), array([ 0.83281839, -0.04328661,  0.06622652, ...,  0.50427902,\n",
      "        0.61379214,  0.50427902]), array([0.19757504, 0.14987727, 0.07889324, ..., 0.08827026, 0.09042697,\n",
      "       0.07923706]), array([-1.04808758, -1.01906734, -0.96832112, ...,  1.03450159,\n",
      "        0.77182508,  1.00960628]), array([-1.19854279, -1.37442899, -1.23424938, ...,  1.11240377,\n",
      "        1.07539237,  1.335341  ]), array([0.13975353, 0.13973408, 0.14569608, ..., 0.14824456, 0.14803056,\n",
      "       0.15281625]), array([0.10381233, 0.10144436, 0.10412569, ..., 0.13573759, 0.13261388,\n",
      "       0.12666015]), array([0.09563687, 0.09369217, 0.1014476 , ..., 0.13831563, 0.14613896,\n",
      "       0.12443224]), array([7934., 7782., 7349., ..., 3508., 3341., 3210.]), array([8140., 7861., 7628., ..., 3182., 3452., 3501.]), array([ 628.,  766.,  858., ..., 1668., 1698., 1670.]), array([1848., 1850., 1923., ..., 1476., 1625., 1588.]), array([1839., 1842., 1947., ..., 1497., 1532., 1587.]), array([2109., 2064., 2089., ..., 2485., 2565., 2587.]), array([2065., 2099., 1940., ..., 2300., 2384., 2291.]), array([2010., 1912., 1888., ..., 2451., 2475., 2651.]), array([2015., 1942., 2054., ..., 2795., 2726., 2638.]), array([0.04368586, 0.04314696, 0.04283443, ..., 0.21278852, 0.21410872,\n",
      "       0.21487935]), array([0.06569227, 0.06518169, 0.0652405 , ..., 0.16059695, 0.16039321,\n",
      "       0.15962814])]\n",
      "[array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "id_list, id_indexes = np.unique(train_data['KPI ID'], return_index=True)\n",
    "id_indexes.sort()\n",
    "id_indexes = np.append(id_indexes, len(train_data))   \n",
    "timeseries_all = []\n",
    "timeseries_label = []\n",
    "\n",
    "for i in np.arange(len(id_indexes)-1):\n",
    "    timeseries_all.append(np.asarray(train_data['value'][id_indexes[i]:id_indexes[i+1]]))\n",
    "    timeseries_label.append(np.asarray(train_data['label'][id_indexes[i]:id_indexes[i+1]]))\n",
    "print(timeseries_all)\n",
    "print(timeseries_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = 0\n",
    "for i in range(26):\n",
    "    for j in range(len(timeseries_all[i])):\n",
    "        if abs(timeseries_all[i][j] - train_data['value'][sum1+j])>1e-6:\n",
    "            print(i,j,timeseries_all[i][j],train_data['value'][sum1+j])\n",
    "        if timeseries_label[i][j] != train_data['label'][sum1+j]:\n",
    "            print(i,j,timeseries_label[i][j],train_data['label'][sum1+j])\n",
    "    sum1 += len(timeseries_all[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.71929836, 2.45614028, 2.33333325, ..., 1.46753252, 1.44155848,\n",
      "       1.4545455 ]), array([36.58333206, 35.83333206, 36.58333206, ..., 22.83333397,\n",
      "       22.66666603, 22.08333397]), array([4.91666651, 4.91666651, 4.75      , ..., 2.        , 2.        ,\n",
      "       2.        ]), array([0., 0., 0., ..., 0., 0., 0.]), array([5.        , 5.5       , 5.1500001 , ..., 3.5       , 3.8499999 ,\n",
      "       2.79999995]), array([9.63912960e+08, 9.64395008e+08, 1.03828864e+09, ...,\n",
      "       1.18040589e+09, 1.17810714e+09, 1.17690176e+09]), array([1.08574938e+09, 1.07270912e+09, 1.06353952e+09, ...,\n",
      "       9.30710976e+08, 9.27908288e+08, 9.30293248e+08]), array([1414.8125  , 1343.8125  , 1367.4375  , ..., 1463.705933,\n",
      "       1487.529419, 1431.411743]), array([ 0.61379214,  0.72330527,  0.72330527, ...,  0.06622652,\n",
      "       -0.04328661, -0.26231286]), array([ 0.08339421,  0.07542374,  0.07589259, ..., -0.29456217,\n",
      "       -0.29134273, -0.28743564]), array([0.8444066 , 0.40438471, 0.92587745, ..., 0.12666432, 0.20291227,\n",
      "       0.21687184]), array([ 1.55096568,  1.34636007,  1.2904476 , ..., -0.03715866,\n",
      "       -0.04183558, -0.00561505]), array([0.15569544, 0.15411967, 0.15874972, ..., 0.06907714, 0.06541979,\n",
      "       0.06592559]), array([0.13436497, 0.12777471, 0.13799619, ..., 0.10366487, 0.10001522,\n",
      "       0.10048587]), array([0.13218766, 0.13275537, 0.13674607, ..., 0.09728213, 0.09286509,\n",
      "       0.08449186]), array([3163., 3218., 3380., ..., 6440., 6432., 6073.]), array([3263., 3018., 2863., ..., 6248., 6310., 6058.]), array([1620., 1609., 1610., ..., 1867., 1849., 1816.]), array([1603., 1629., 1562., ..., 1638., 1641., 1681.]), array([1596., 1491., 1411., ..., 1724., 1656., 1619.]), array([2580., 2645., 2527., ..., 1800., 1739., 1742.]), array([2290., 2350., 2371., ..., 2262., 2328., 2194.]), array([2713., 2734., 2561., ...,  770.,  678.,  724.]), array([2706., 2614., 2591., ..., 1954., 1954., 1873.]), array([0.21680924, 0.2175812 , 0.21920324, ..., 0.18768188, 0.18563179,\n",
      "       0.18363579]), array([0.16136951, 0.15721134, 0.16209025, ..., 0.2122068 , 0.20901665,\n",
      "       0.20777578])]\n"
     ]
    }
   ],
   "source": [
    "test_id_list, test_id_indexes = np.unique(test_data['KPI ID'], return_index=True)\n",
    "test_id_indexes.sort()\n",
    "test_id_indexes = np.append(test_id_indexes, len(test_data))   \n",
    "testseries_all = []\n",
    "\n",
    "for i in np.arange(len(test_id_indexes)-1):\n",
    "    testseries_all.append(np.asarray(test_data['value'][test_id_indexes[i]:test_id_indexes[i+1]]))\n",
    "print(testseries_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train_test_all.txt','wb')\n",
    "pickle.dump(timeseries_all,file)\n",
    "pickle.dump(timeseries_label,file)\n",
    "pickle.dump(testseries_all,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 112627\n",
      "128613 110185\n",
      "129035 112113\n",
      "128679 108747\n",
      "129128 111171\n",
      "128971 111004\n",
      "128667 112786\n",
      "128853 112116\n",
      "8784 8784\n",
      "10960 10960\n",
      "8248 8249\n",
      "8247 8248\n",
      "8784 8784\n",
      "8784 8784\n",
      "8784 8784\n",
      "147024 147024\n",
      "147009 147010\n",
      "100254 108901\n",
      "147629 147629\n",
      "147668 147669\n",
      "147689 147690\n",
      "137925 137925\n",
      "129453 129454\n",
      "147680 147681\n",
      "65449 65450\n",
      "65436 65436\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(timeseries_all)):\n",
    "    print(len(timeseries_all[i]),len(testseries_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "for i in range(26):\n",
    "    print(i)\n",
    "    for j in range(len(testseries_all[i])):\n",
    "        if abs(testseries_all[i][j] - test_data['value'][sum1+j])>1e-6:\n",
    "            print(i,j,testseries_all[i][j],test_data['value'][sum1+j])\n",
    "    sum1 += len(testseries_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.16579029, 0.15009279, 0.17924528, ..., 0.26324065, 0.28003973,\n",
      "       0.24644158]), array([0.25469729, 0.28511779, 0.27080227, ..., 0.37123471, 0.36444976,\n",
      "       0.36832687]), array([0.08658008, 0.0952381 , 0.09090909, ..., 0.25974026, 0.25541125,\n",
      "       0.24242423]), array([0., 0., 0., ..., 0., 0., 0.]), array([0.00440788, 0.00617103, 0.00597512, ..., 0.01067685, 0.01332158,\n",
      "       0.01116662]), array([0.36525055, 0.35480202, 0.36719728, ..., 0.4082798 , 0.40949553,\n",
      "       0.40996881]), array([0.4686479 , 0.4575223 , 0.45740275, ..., 0.40708398, 0.49381646,\n",
      "       0.4886429 ]), array([0.03054516, 0.02918036, 0.02752964, ..., 0.04071636, 0.04130726,\n",
      "       0.04105565]), array([0.58974359, 0.38461538, 0.41025641, ..., 0.51282051, 0.53846154,\n",
      "       0.51282051]), array([0.38908621, 0.37983683, 0.36607186, ..., 0.36789022, 0.36830844,\n",
      "       0.36613853]), array([0.2827225 , 0.28839568, 0.2983161 , ..., 0.68984923, 0.63849842,\n",
      "       0.68498243]), array([0.47658635, 0.45154548, 0.47150282, ..., 0.80559514, 0.80032584,\n",
      "       0.83733465]), array([0.60420126, 0.60410692, 0.63301887, ..., 0.64537736, 0.64433962,\n",
      "       0.66754717]), array([0.53557592, 0.52172507, 0.53740881, ..., 0.72231447, 0.70404313,\n",
      "       0.66921833]), array([0.36346626, 0.35465536, 0.38979307, ..., 0.55683211, 0.59227747,\n",
      "       0.49393021]), array([0.44033744, 0.43190143, 0.40786991, ..., 0.19469419, 0.18542569,\n",
      "       0.17815518]), array([0.44784331, 0.4324934 , 0.4196743 , ..., 0.17506602, 0.18992077,\n",
      "       0.19261664]), array([0.1810842 , 0.22087659, 0.24740484, ..., 0.48096886, 0.48961938,\n",
      "       0.48154556]), array([0.38348205, 0.38389707, 0.39904545, ..., 0.30628761, 0.33720689,\n",
      "       0.32952895]), array([0.44733641, 0.44806616, 0.47360739, ..., 0.36414498, 0.37265872,\n",
      "       0.38603746]), array([0.51602643, 0.5050159 , 0.51113286, ..., 0.60802545, 0.62759971,\n",
      "       0.63298263]), array([0.48135198, 0.48927739, 0.45221445, ..., 0.53613054, 0.55571096,\n",
      "       0.53403263]), array([0.39434962, 0.37512262, 0.37041397, ..., 0.4808711 , 0.48557975,\n",
      "       0.52010987]), array([0.42763158, 0.41213922, 0.43590832, ..., 0.59316638, 0.57852292,\n",
      "       0.5598472 ]), array([0.12793934, 0.1263611 , 0.12544584, ..., 0.623177  , 0.62704339,\n",
      "       0.62930024]), array([0.06569427, 0.06518368, 0.06524249, ..., 0.16060183, 0.16039809,\n",
      "       0.159633  ])]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "timeseries_scaled = []\n",
    "for i in range(len(timeseries_all)):\n",
    "    timeseries_scaled.append(minmax_scale(timeseries_all[i]))\n",
    "print(timeseries_scaled)\n",
    "print(len(timeseries_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "def get_feature_logs(time_series):\n",
    "    return np.log(time_series + 1e-2)\n",
    "def get_feature_SARIMA_residuals(time_series):\n",
    "    predict = SARIMAX(time_series,\n",
    "                      trend='n', \n",
    "                      order=(5,1,1), \n",
    "                      measurement_error=True).fit().get_prediction()\n",
    "    return time_series - predict.predicted_mean\n",
    "\n",
    "def get_feature_AddES_residuals(time_series):\n",
    "    predict = ExponentialSmoothing(time_series, trend='add').fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues\n",
    "\n",
    "def get_feature_SimpleES_residuals(time_series):\n",
    "    predict = SimpleExpSmoothing(time_series).fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues\n",
    "\n",
    "def get_feature_Holt_residuals(time_series):\n",
    "    predict = Holt(time_series).fit(smoothing_level=1)\n",
    "    return time_series - predict.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeseries_features(time_series, time_series_label, Windows, delay):\n",
    "  \n",
    "    data = []\n",
    "    data_label = []\n",
    "    data_label_vital = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    time_series_logs = get_feature_logs(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        datum = []\n",
    "        datum_label = time_series_label[i]        \n",
    "        \n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "\n",
    "        datum.append(time_series_logs[i])\n",
    "        \n",
    "\n",
    "        datum.append(diff_plain)\n",
    "        \n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  \n",
    "\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    " \n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    " \n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            \n",
    "            \n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            datum.append(mean_w)  \n",
    "            \n",
    "            datum.append(var_w)\n",
    "            \n",
    "            datum.append(diff_mean_1)\n",
    "            \n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_1)\n",
    "            \n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_mean_w)\n",
    "            \n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_w)\n",
    "            \n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            \n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            \n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "        data_label.append(np.asarray(datum_label))\n",
    "\n",
    "        if datum_label == 1 and sum(time_series_label[i-delay:i]) < delay:\n",
    "            data_label_vital.append(np.asarray(1))\n",
    "        else:\n",
    "            data_label_vital.append(np.asarray(0))\n",
    "            \n",
    "    return data, data_label, data_label_vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "W = np.asarray([2, 5, 10, 25, 50, 100, 200, 300, 400, 500])\n",
    "delay = 7\n",
    "scaler_list = [] \n",
    "timeseries_features = None\n",
    "timeseries_features_label = []\n",
    "timeseries_features_label_vital = []\n",
    "\n",
    "for i in range(len(timeseries_all)):\n",
    "    features_temp,label_temp,label_vital_temp = get_timeseries_features(timeseries_scaled[i], timeseries_label[i], W, delay) \n",
    "    print(i)\n",
    "    assert(len(features_temp)==len(label_temp))\n",
    "    assert(len(label_temp) == len(label_vital_temp))\n",
    "    scaler_temp = StandardScaler()\n",
    "    features_temp = scaler_temp.fit_transform(features_temp)\n",
    "    scaler_list.append(scaler_temp)\n",
    "    if i==0:\n",
    "        timeseries_features = features_temp\n",
    "    else:\n",
    "        timeseries_features = np.concatenate((timeseries_features, features_temp), axis = 0)\n",
    "        \n",
    "    timeseries_features_label = timeseries_features_label + label_temp\n",
    "    timeseries_features_label_vital = timeseries_features_label_vital + label_vital_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341, 128)\n",
      "2450341 2450341\n"
     ]
    }
   ],
   "source": [
    "print(timeseries_features.shape)\n",
    "print(len(timeseries_features_label),len(timeseries_features_label_vital))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "timeseries_features_label = np.array(timeseries_features_label)\n",
    "timeseries_features_label_vital = np.array(timeseries_features_label_vital)\n",
    "file = open('timeseries_features2.txt','wb')\n",
    "pickle.dump(timeseries_features,file)\n",
    "pickle.dump(scaler_list,file)\n",
    "pickle.dump(timeseries_features_label,file)\n",
    "pickle.dump(timeseries_features_label_vital,file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim = 128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53219\n",
      "4853\n"
     ]
    }
   ],
   "source": [
    "print(sum(timeseries_features_label))\n",
    "print(sum(timeseries_features_label_vital))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 3270662.0\n",
      "[ 1.  1.  1. ... 45. 45. 45.] 5665517.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label) - sum(timeseries_features_label)) / sum(timeseries_features_label))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label)) - timeseries_features_label\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (4*ratio) * timeseries_features_label_vital + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2450341/2450341 [==============================] - 38s 16us/step - loss: 0.8567 - acc: 0.7551 - binary_accuracy: 0.7551\n",
      "Epoch 2/100\n",
      "2450341/2450341 [==============================] - 33s 13us/step - loss: 0.6455 - acc: 0.8543 - binary_accuracy: 0.8543\n",
      "Epoch 3/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.5439 - acc: 0.9000 - binary_accuracy: 0.9000\n",
      "Epoch 4/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.4758 - acc: 0.9204 - binary_accuracy: 0.9204\n",
      "Epoch 5/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.4319 - acc: 0.9332 - binary_accuracy: 0.9332\n",
      "Epoch 6/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.4051 - acc: 0.9399 - binary_accuracy: 0.9399\n",
      "Epoch 7/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.3802 - acc: 0.9465 - binary_accuracy: 0.9465\n",
      "Epoch 8/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.3600 - acc: 0.9482 - binary_accuracy: 0.9482\n",
      "Epoch 9/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.3419 - acc: 0.9520 - binary_accuracy: 0.9520\n",
      "Epoch 10/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.3281 - acc: 0.9548 - binary_accuracy: 0.9548\n",
      "Epoch 11/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.3189 - acc: 0.9555 - binary_accuracy: 0.9555\n",
      "Epoch 12/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.3187 - acc: 0.9566 - binary_accuracy: 0.9566\n",
      "Epoch 13/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.3003 - acc: 0.9589 - binary_accuracy: 0.9589\n",
      "Epoch 14/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2878 - acc: 0.9597 - binary_accuracy: 0.9597\n",
      "Epoch 15/100\n",
      "2450341/2450341 [==============================] - 33s 13us/step - loss: 0.2893 - acc: 0.9605 - binary_accuracy: 0.9605\n",
      "Epoch 16/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2762 - acc: 0.9618 - binary_accuracy: 0.9618\n",
      "Epoch 17/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2728 - acc: 0.9620 - binary_accuracy: 0.9620\n",
      "Epoch 18/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2662 - acc: 0.9626 - binary_accuracy: 0.9626\n",
      "Epoch 19/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2637 - acc: 0.9629 - binary_accuracy: 0.9629\n",
      "Epoch 20/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2581 - acc: 0.9635 - binary_accuracy: 0.9635\n",
      "Epoch 21/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2576 - acc: 0.9637 - binary_accuracy: 0.9637\n",
      "Epoch 22/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2472 - acc: 0.9645 - binary_accuracy: 0.9645\n",
      "Epoch 23/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2443 - acc: 0.9648 - binary_accuracy: 0.9648\n",
      "Epoch 24/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2509 - acc: 0.9642 - binary_accuracy: 0.9642\n",
      "Epoch 25/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2350 - acc: 0.9658 - binary_accuracy: 0.9658\n",
      "Epoch 26/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2330 - acc: 0.9656 - binary_accuracy: 0.9656\n",
      "Epoch 27/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2273 - acc: 0.9668 - binary_accuracy: 0.9668\n",
      "Epoch 28/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2278 - acc: 0.9670 - binary_accuracy: 0.9670\n",
      "Epoch 29/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2243 - acc: 0.9676 - binary_accuracy: 0.9676\n",
      "Epoch 30/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2289 - acc: 0.9673 - binary_accuracy: 0.9673\n",
      "Epoch 31/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2207 - acc: 0.9682 - binary_accuracy: 0.9682\n",
      "Epoch 32/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.2192 - acc: 0.9682 - binary_accuracy: 0.9682\n",
      "Epoch 33/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2173 - acc: 0.9681 - binary_accuracy: 0.9681\n",
      "Epoch 34/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2102 - acc: 0.9689 - binary_accuracy: 0.9689\n",
      "Epoch 35/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2223 - acc: 0.9672 - binary_accuracy: 0.9672\n",
      "Epoch 36/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2070 - acc: 0.9685 - binary_accuracy: 0.9685\n",
      "Epoch 37/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2102 - acc: 0.9680 - binary_accuracy: 0.9680\n",
      "Epoch 38/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2078 - acc: 0.9688 - binary_accuracy: 0.9688\n",
      "Epoch 39/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.2034 - acc: 0.9692 - binary_accuracy: 0.9692\n",
      "Epoch 40/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1995 - acc: 0.9697 - binary_accuracy: 0.9697\n",
      "Epoch 41/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1981 - acc: 0.9703 - binary_accuracy: 0.9703\n",
      "Epoch 42/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1887 - acc: 0.9709 - binary_accuracy: 0.9709\n",
      "Epoch 43/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1930 - acc: 0.9709 - binary_accuracy: 0.9709\n",
      "Epoch 44/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1937 - acc: 0.9703 - binary_accuracy: 0.9703\n",
      "Epoch 45/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1912 - acc: 0.9707 - binary_accuracy: 0.9707\n",
      "Epoch 46/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1978 - acc: 0.9704 - binary_accuracy: 0.9704\n",
      "Epoch 47/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1908 - acc: 0.9710 - binary_accuracy: 0.9710\n",
      "Epoch 48/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1895 - acc: 0.9711 - binary_accuracy: 0.9711\n",
      "Epoch 49/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1965 - acc: 0.9710 - binary_accuracy: 0.9710\n",
      "Epoch 50/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1871 - acc: 0.9704 - binary_accuracy: 0.9704\n",
      "Epoch 51/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1850 - acc: 0.9718 - binary_accuracy: 0.9718\n",
      "Epoch 52/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1753 - acc: 0.9727 - binary_accuracy: 0.9727\n",
      "Epoch 53/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1805 - acc: 0.9720 - binary_accuracy: 0.9720\n",
      "Epoch 54/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1812 - acc: 0.9719 - binary_accuracy: 0.9719\n",
      "Epoch 55/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1862 - acc: 0.9716 - binary_accuracy: 0.9716\n",
      "Epoch 56/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1863 - acc: 0.9720 - binary_accuracy: 0.9720\n",
      "Epoch 57/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1770 - acc: 0.9722 - binary_accuracy: 0.9722\n",
      "Epoch 58/100\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1768 - acc: 0.9720 - binary_accuracy: 0.9720\n",
      "Epoch 59/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1817 - acc: 0.9718 - binary_accuracy: 0.9718\n",
      "Epoch 60/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1706 - acc: 0.9727 - binary_accuracy: 0.9727\n",
      "Epoch 61/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1785 - acc: 0.9726 - binary_accuracy: 0.9726\n",
      "Epoch 62/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1709 - acc: 0.9736 - binary_accuracy: 0.9736\n",
      "Epoch 63/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1711 - acc: 0.9726 - binary_accuracy: 0.9726\n",
      "Epoch 64/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1710 - acc: 0.9738 - binary_accuracy: 0.9738 3s - loss:\n",
      "Epoch 65/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1624 - acc: 0.9742 - binary_accuracy: 0.9742\n",
      "Epoch 66/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1628 - acc: 0.9738 - binary_accuracy: 0.9738\n",
      "Epoch 67/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1653 - acc: 0.9746 - binary_accuracy: 0.9746\n",
      "Epoch 68/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1651 - acc: 0.9739 - binary_accuracy: 0.9739\n",
      "Epoch 69/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1587 - acc: 0.9744 - binary_accuracy: 0.9744\n",
      "Epoch 70/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1598 - acc: 0.9747 - binary_accuracy: 0.9747\n",
      "Epoch 71/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1582 - acc: 0.9747 - binary_accuracy: 0.9747\n",
      "Epoch 72/100\n",
      "2450341/2450341 [==============================] - 37s 15us/step - loss: 0.1693 - acc: 0.9737 - binary_accuracy: 0.9737\n",
      "Epoch 73/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1667 - acc: 0.9733 - binary_accuracy: 0.9733\n",
      "Epoch 74/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1611 - acc: 0.9738 - binary_accuracy: 0.9738\n",
      "Epoch 75/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1576 - acc: 0.9747 - binary_accuracy: 0.9747\n",
      "Epoch 76/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1578 - acc: 0.9749 - binary_accuracy: 0.9749\n",
      "Epoch 77/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1598 - acc: 0.9743 - binary_accuracy: 0.9743\n",
      "Epoch 78/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1533 - acc: 0.9751 - binary_accuracy: 0.9751\n",
      "Epoch 79/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1508 - acc: 0.9750 - binary_accuracy: 0.9750\n",
      "Epoch 80/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1523 - acc: 0.9759 - binary_accuracy: 0.9759\n",
      "Epoch 81/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1511 - acc: 0.9753 - binary_accuracy: 0.9753\n",
      "Epoch 82/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1533 - acc: 0.9755 - binary_accuracy: 0.9755\n",
      "Epoch 83/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1551 - acc: 0.9751 - binary_accuracy: 0.9751\n",
      "Epoch 84/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1494 - acc: 0.9753 - binary_accuracy: 0.9753\n",
      "Epoch 85/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1550 - acc: 0.9753 - binary_accuracy: 0.9753\n",
      "Epoch 86/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1515 - acc: 0.9759 - binary_accuracy: 0.9759\n",
      "Epoch 87/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1501 - acc: 0.9761 - binary_accuracy: 0.9761\n",
      "Epoch 88/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1488 - acc: 0.9758 - binary_accuracy: 0.9758\n",
      "Epoch 89/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1474 - acc: 0.9757 - binary_accuracy: 0.9757\n",
      "Epoch 90/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1460 - acc: 0.9768 - binary_accuracy: 0.9768\n",
      "Epoch 91/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1467 - acc: 0.9760 - binary_accuracy: 0.9760\n",
      "Epoch 92/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1527 - acc: 0.9757 - binary_accuracy: 0.9757\n",
      "Epoch 93/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1505 - acc: 0.9754 - binary_accuracy: 0.9754\n",
      "Epoch 94/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1503 - acc: 0.9751 - binary_accuracy: 0.9751\n",
      "Epoch 95/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1477 - acc: 0.9760 - binary_accuracy: 0.9760\n",
      "Epoch 96/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1476 - acc: 0.9760 - binary_accuracy: 0.9760\n",
      "Epoch 97/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1485 - acc: 0.9761 - binary_accuracy: 0.9761\n",
      "Epoch 98/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1381 - acc: 0.9767 - binary_accuracy: 0.9767\n",
      "Epoch 99/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1429 - acc: 0.9765 - binary_accuracy: 0.9765\n",
      "Epoch 100/100\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1400 - acc: 0.9771 - binary_accuracy: 0.9771\n",
      "It took 3374 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "history = model.fit(timeseries_features, timeseries_features_label, epochs=100, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('model2.txt','wb')\n",
    "pickle.dump(model,file)\n",
    "pickle.dump(history,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test_features2.txt','rb')\n",
    "testseries_features = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 7s 3us/step\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.9467576404770455\n",
      "0.9412427892294105\n",
      "0.9439921604100708\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(model.predict(timeseries_features, batch_size=5000,verbose=1)>0.96).astype(int)\n",
    "print(train_data_check)\n",
    "print(precision_score(timeseries_features_label, train_data_check))\n",
    "print(recall_score(timeseries_features_label, train_data_check))\n",
    "print(f1_score(timeseries_features_label, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 7s 3us/step\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.10650830423971332\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(model.predict(testseries_features,batch_size=5000,verbose=1))>0.9999999).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25974\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "data_features_diff = len(test_data) - len(testseries_features)\n",
    "print(data_features_diff)\n",
    "data_features_diff_avg = int(data_features_diff / len(testseries_all))\n",
    "print(data_features_diff_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02624451045793218\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_data_check)/len(train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.3932252 , 0.35421474, 0.33600987, ..., 0.20766387, 0.20381349,\n",
      "       0.20573868]), array([0.3546042 , 0.34733441, 0.3546042 , ..., 0.22132473, 0.21970921,\n",
      "       0.21405494]), array([0.30256409, 0.30256409, 0.29230769, ..., 0.12307692, 0.12307692,\n",
      "       0.12307692]), array([0., 0., 0., ..., 0., 0., 0.]), array([0.00931793, 0.01024972, 0.00959747, ..., 0.00652255, 0.0071748 ,\n",
      "       0.00521804]), array([0.45261198, 0.45309328, 0.52687281, ..., 0.66877059, 0.66647539,\n",
      "       0.66527187]), array([0.4822847 , 0.46993302, 0.4612476 , ..., 0.33543289, 0.33277819,\n",
      "       0.33503722]), array([0.15237441, 0.14472776, 0.14727216, ..., 0.15764021, 0.16020598,\n",
      "       0.15416214]), array([0.52380952, 0.54761905, 0.54761905, ..., 0.4047619 , 0.38095238,\n",
      "       0.33333333]), array([0.45257122, 0.45066495, 0.45077709, ..., 0.36217659, 0.36294657,\n",
      "       0.36388102]), array([0.42934343, 0.40162322, 0.43447588, ..., 0.38412757, 0.38893099,\n",
      "       0.3898104 ]), array([0.23961237, 0.23312242, 0.23134891, ..., 0.18923813, 0.18908978,\n",
      "       0.19023867]), array([0.25766777, 0.25491776, 0.26299803, ..., 0.10650337, 0.10012064,\n",
      "       0.10100336]), array([0.52173071, 0.49510171, 0.53640322, ..., 0.39768219, 0.3829352 ,\n",
      "       0.38483691]), array([0.2938492 , 0.29527708, 0.30531423, ..., 0.20605701, 0.19494755,\n",
      "       0.17388772]), array([0.18975343, 0.19305297, 0.20277161, ..., 0.38634591, 0.38586598,\n",
      "       0.36432899]), array([0.1495006 , 0.13827545, 0.13117383, ..., 0.28626409, 0.28910474,\n",
      "       0.27755887]), array([0.46875   , 0.46556713, 0.46585648, ..., 0.54021991, 0.53501157,\n",
      "       0.52546296]), array([0.50235036, 0.51049828, 0.48950172, ..., 0.51331871, 0.51425885,\n",
      "       0.52679411]), array([0.45809414, 0.42795637, 0.40499426, ..., 0.49483352, 0.47531573,\n",
      "       0.46469575]), array([0.55269923, 0.56662382, 0.54134533, ..., 0.38560411, 0.37253642,\n",
      "       0.37317909]), array([0.41455467, 0.42541636, 0.42921796, ..., 0.40948588, 0.42143374,\n",
      "       0.39717596]), array([0.53071205, 0.53482003, 0.50097809, ..., 0.15062598, 0.13262911,\n",
      "       0.14162754]), array([0.5371179 , 0.51885669, 0.51429139, ..., 0.38785232, 0.38785232,\n",
      "       0.37177451]), array([0.21680269, 0.21757469, 0.21919684, ..., 0.18767348, 0.18562326,\n",
      "       0.18362713]), array([0.18075804, 0.17609999, 0.18156542, ..., 0.23770672, 0.23413307,\n",
      "       0.23274302])]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "testseries_scaled = []\n",
    "for i in range(len(testseries_all)):\n",
    "    testseries_scaled.append(minmax_scale(testseries_all[i]))\n",
    "print(testseries_scaled)\n",
    "print(len(testseries_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True), StandardScaler(copy=True, with_mean=True, with_std=True)]\n"
     ]
    }
   ],
   "source": [
    "print(scaler_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02e99bd4f6cfb33f' '046ec29ddf80d62e' '07927a9a18fa19ae'\n",
      " '09513ae3e75778a3' '18fbb1d5a5dc099d' '1c35dbf57f55f5e4'\n",
      " '40e25005ff8992bd' '54e8a140f6237526' '71595dd7171f4540'\n",
      " '769894baefea4e9e' '76f4550c43334374' '7c189dd36f048a6c'\n",
      " '88cf3a776ba00e7c' '8a20c229e9860d0c' '8bef9af9a922e0b3'\n",
      " '8c892e5525f3e491' '9bd90500bfd11edb' '9ee5879409dccef9'\n",
      " 'a40b1df87e3f1c87' 'a5bf5d65261d859a' 'affb01ca2b4f0b45'\n",
      " 'b3b2e6d1a791d63a' 'c58bfcbacb2822d1' 'cff6d3c01e6a6bfa'\n",
      " 'da403e4e3f87c9e0' 'e0770391decc44ce'] [      0 1030508 1039292  644017  514889  901655 1387132 1050252 1635015\n",
      " 1075531 1084315 1782683 2410879 1066747 2068297 1240123  128562 2345430\n",
      " 1930372  386210 2197750 1058500  772988 1487386  257175 1093099]\n"
     ]
    }
   ],
   "source": [
    "id_t1, id_t2 = np.unique(train_data['KPI ID'], return_index=True)\n",
    "print(id_t1,id_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02e99bd4f6cfb33f' '046ec29ddf80d62e' '07927a9a18fa19ae'\n",
      " '09513ae3e75778a3' '18fbb1d5a5dc099d' '1c35dbf57f55f5e4'\n",
      " '40e25005ff8992bd' '54e8a140f6237526' '71595dd7171f4540'\n",
      " '769894baefea4e9e' '76f4550c43334374' '7c189dd36f048a6c'\n",
      " '88cf3a776ba00e7c' '8a20c229e9860d0c' '8bef9af9a922e0b3'\n",
      " '8c892e5525f3e491' '9bd90500bfd11edb' '9ee5879409dccef9'\n",
      " 'a40b1df87e3f1c87' 'a5bf5d65261d859a' 'affb01ca2b4f0b45'\n",
      " 'b3b2e6d1a791d63a' 'c58bfcbacb2822d1' 'cff6d3c01e6a6bfa'\n",
      " 'da403e4e3f87c9e0' 'e0770391decc44ce'] [      0  890749  899533  554843  443672  778633 1247376  910493 1503906\n",
      "  935774  944558 1651575 2279775  926990 1937190 1100366  112627 2214325\n",
      " 1799265  334925 2066644  918742  665847 1356277  222812  953342]\n"
     ]
    }
   ],
   "source": [
    "id_t3, id_t4 = np.unique(test_data['KPI ID'], return_index=True)\n",
    "print(id_t3,id_t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_features(time_series, Windows):\n",
    "  \n",
    "    data = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    # features from tsa models\n",
    "    #time_series_SARIMA_residuals = get_feature_SARIMA_residuals(time_series)\n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    # features from tsa models for time series logarithm\n",
    "    time_series_logs = get_feature_logs(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        # the datum to put into the data pool\n",
    "        datum = []        \n",
    "        \n",
    "        # fill the datum with f01-f09\n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        # f01-f04: residuals\n",
    "        #datum.append(time_series_SARIMA_residuals[i])\n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "        # f05: logarithm\n",
    "        datum.append(time_series_logs[i])\n",
    "        \n",
    "        # f06: diff\n",
    "        datum.append(diff_plain)\n",
    "        # f07: diff percentage\n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  # to avoid 0, plus 1e-10\n",
    "        # f08: diff of diff - derivative\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    "        # f09: diff of accumulated mean and current value\n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "        # fill the datum with features related to windows\n",
    "        # loop over different windows size to fill the datum\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    "            #var_w = np.var(time_series[i-k:i+1])\n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "            #mean_w_and_1 = np.mean(time_series[i-k-1:i])\n",
    "            #var_w_and_1 = np.var(time_series[i-k-1:i])\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            #var_2w = np.var(time_series[i-2*k:i-k+1])\n",
    "            \n",
    "            # diff of sliding windows\n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            # diff of jumping windows\n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            # f1\n",
    "            datum.append(mean_w)  # [0:2] is [0,1]\n",
    "            # f2\n",
    "            datum.append(var_w)\n",
    "            # f3\n",
    "            datum.append(diff_mean_1)\n",
    "            # f4\n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            # f5\n",
    "            datum.append(diff_var_1)\n",
    "            # f6\n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            # f7\n",
    "            datum.append(diff_mean_w)\n",
    "            # f8\n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            # f9\n",
    "            datum.append(diff_var_w)\n",
    "            # f10\n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            # diff of sliding/jumping windows and current value\n",
    "            # f11\n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            # f12\n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 112627\n",
      "1 110185\n",
      "2 112113\n",
      "3 108747\n",
      "4 111171\n",
      "5 111004\n",
      "6 112786\n",
      "7 112116\n",
      "8 8784\n",
      "9 10960\n",
      "10 8249\n",
      "11 8248\n",
      "12 8784\n",
      "13 8784\n",
      "14 8784\n",
      "15 147024\n",
      "16 147010\n",
      "17 108901\n",
      "18 147629\n",
      "19 147669\n",
      "20 147690\n",
      "21 137925\n",
      "22 129454\n",
      "23 147681\n",
      "24 65450\n",
      "25 65436\n",
      "(2319237, 128)\n"
     ]
    }
   ],
   "source": [
    "testseries_features = None\n",
    "for i in range(len(testseries_scaled)):\n",
    "    print(i, len(testseries_scaled[i]))\n",
    "    features_temp = get_test_features(testseries_scaled[i], W)\n",
    "    features_temp = scaler_list[i].transform(features_temp)\n",
    "    if i==0:\n",
    "        testseries_features = features_temp\n",
    "    else:\n",
    "        testseries_features = np.concatenate((testseries_features, features_temp), axis = 0)\n",
    "    \n",
    "print(testseries_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.93206973 -0.93209252 -0.93206973 ... -0.398442    0.25718099\n",
      "   1.0760591 ]\n",
      " [ 0.65239108  0.65236829  0.65239108 ... -0.39651193  0.46121163\n",
      "   1.21466327]\n",
      " [ 0.279577    0.2795542   0.279577   ... -0.3966797   0.54603141\n",
      "   1.27566771]\n",
      " ...\n",
      " [-0.22850783 -0.22859523 -0.22850783 ... -0.29360525 -1.11937186\n",
      "  -0.53397675]\n",
      " [-0.48648383 -0.48657123 -0.48648383 ... -0.29232762 -1.16010143\n",
      "  -0.56824054]\n",
      " [-0.18911409 -0.18920148 -0.18911409 ... -0.29098218 -1.1744739\n",
      "  -0.58297141]]\n"
     ]
    }
   ],
   "source": [
    "print(testseries_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 7s 3us/step\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.056859648237760956\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(model.predict(testseries_features,batch_size=5000,verbose=1))>0.97).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictn2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_timeseries_features(time_series, time_series_label, Windows, delay):\n",
    "  \n",
    "    data = []\n",
    "    data_label = []\n",
    "    data_label_vital = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        datum = []\n",
    "        datum_label = time_series_label[i]        \n",
    "        \n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "\n",
    "        datum.append(time_series[i])\n",
    "        \n",
    "\n",
    "        datum.append(diff_plain)\n",
    "        \n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  \n",
    "\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    " \n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    " \n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            \n",
    "            \n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            datum.append(mean_w)  \n",
    "            \n",
    "            datum.append(var_w)\n",
    "            \n",
    "            datum.append(diff_mean_1)\n",
    "            \n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_1)\n",
    "            \n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            \n",
    "            datum.append(diff_mean_w)\n",
    "            \n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            \n",
    "            datum.append(diff_var_w)\n",
    "            \n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            \n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            \n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "        data_label.append(np.asarray(datum_label))\n",
    "\n",
    "        if datum_label == 1 and sum(time_series_label[i-delay:i]) < delay:\n",
    "            data_label_vital.append(np.asarray(1))\n",
    "        else:\n",
    "            data_label_vital.append(np.asarray(0))\n",
    "            \n",
    "    return data, data_label, data_label_vital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 128562 0 0 0 0\n",
      "1 128613 1 127563 127563 127563\n",
      "2 129035 2 255177 255177 255177\n",
      "3 128679 3 383213 383213 383213\n",
      "4 129128 4 510893 510893 510893\n",
      "5 128971 5 639022 639022 639022\n",
      "6 128667 6 766994 766994 766994\n",
      "7 128853 7 894662 894662 894662\n",
      "8 8784 8 1022516 1022516 1022516\n",
      "9 10960 9 1030301 1030301 1030301\n",
      "10 8248 10 1040262 1040262 1040262\n",
      "11 8247 11 1047511 1047511 1047511\n",
      "12 8784 12 1054759 1054759 1054759\n",
      "13 8784 13 1062544 1062544 1062544\n",
      "14 8784 14 1070329 1070329 1070329\n",
      "15 147024 15 1078114 1078114 1078114\n",
      "16 147009 16 1224139 1224139 1224139\n",
      "17 100254 17 1370149 1370149 1370149\n",
      "18 147629 18 1469404 1469404 1469404\n",
      "19 147668 19 1616034 1616034 1616034\n",
      "20 147689 20 1762703 1762703 1762703\n",
      "21 137925 21 1909393 1909393 1909393\n",
      "22 129453 22 2046319 2046319 2046319\n",
      "23 147680 23 2174773 2174773 2174773\n",
      "24 65449 24 2321454 2321454 2321454\n",
      "25 65436 25 2385904 2385904 2385904\n"
     ]
    }
   ],
   "source": [
    "scaler_list_new = [] \n",
    "timeseries_features_new = []\n",
    "timeseries_features_label_new = []\n",
    "timeseries_features_label_vital_new = []\n",
    "\n",
    "for i in range(len(timeseries_all)):\n",
    "    print(i,len(timeseries_all[i]),len(scaler_list_new),len(timeseries_features_new),len(timeseries_features_label_new),\n",
    "                len(timeseries_features_label_vital_new))\n",
    "    features_temp,label_temp,label_vital_temp = new_get_timeseries_features(timeseries_all[i], timeseries_label[i], W, delay) \n",
    "    assert(len(features_temp)==len(label_temp))\n",
    "    assert(len(label_temp) == len(label_vital_temp))\n",
    "    scaler_temp = StandardScaler()\n",
    "    features_temp = scaler_temp.fit_transform(features_temp)\n",
    "    scaler_list_new.append(scaler_temp)\n",
    "    if i==0:\n",
    "        timeseries_features_new = features_temp\n",
    "    else:\n",
    "        timeseries_features_new = np.concatenate((timeseries_features_new, features_temp), axis = 0)\n",
    "        \n",
    "    timeseries_features_label_new = timeseries_features_label_new + label_temp\n",
    "    timeseries_features_label_vital_new = timeseries_features_label_vital_new + label_vital_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('train_feature.txt','wb')\n",
    "pickle.dump(timeseries_features_new,file)\n",
    "pickle.dump(timeseries_features_label_new,file)\n",
    "pickle.dump(timeseries_features_label_vital_new,file)\n",
    "pickle.dump(scaler_list_new,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_test_features(time_series, Windows):\n",
    "  \n",
    "    data = []\n",
    "    \n",
    "    start_point = 2*max(Windows) - 1\n",
    "    start_accum = sum(time_series[0:start_point])\n",
    "    \n",
    "    # features from tsa models\n",
    "    #time_series_SARIMA_residuals = get_feature_SARIMA_residuals(time_series)\n",
    "    time_series_AddES_residuals = get_feature_AddES_residuals(time_series)\n",
    "    time_series_SimpleES_residuals = get_feature_SimpleES_residuals(time_series)\n",
    "    time_Series_Holt_residuals = get_feature_Holt_residuals(time_series)\n",
    "    \n",
    "    for i in np.arange(start_point, len(time_series)):        \n",
    "        # the datum to put into the data pool\n",
    "        datum = []        \n",
    "        \n",
    "        # fill the datum with f01-f09\n",
    "        diff_plain = time_series[i] - time_series[i-1]\n",
    "        start_accum = start_accum + time_series[i]\n",
    "        mean_accum = (start_accum)/(i+1)\n",
    "        \n",
    "        # f01-f04: residuals\n",
    "        #datum.append(time_series_SARIMA_residuals[i])\n",
    "        datum.append(time_series_AddES_residuals[i])\n",
    "        datum.append(time_series_SimpleES_residuals[i])\n",
    "        datum.append(time_Series_Holt_residuals[i])\n",
    "        # f05: logarithm\n",
    "        datum.append(time_series[i])\n",
    "        \n",
    "        # f06: diff\n",
    "        datum.append(diff_plain)\n",
    "        # f07: diff percentage\n",
    "        datum.append(diff_plain/(time_series[i-1] + 1e-8))  # to avoid 0, plus 1e-10\n",
    "        # f08: diff of diff - derivative\n",
    "        datum.append(diff_plain - (time_series[i-1] - time_series[i-2]))\n",
    "        # f09: diff of accumulated mean and current value\n",
    "        datum.append(time_series[i] - mean_accum)\n",
    "\n",
    "        # fill the datum with features related to windows\n",
    "        # loop over different windows size to fill the datum\n",
    "        for k in Windows:\n",
    "            mean_w = np.mean(time_series[i-k+1:i+1])\n",
    "            var_w = np.mean((np.asarray(time_series[i-k+1:i+1]) - mean_w)**2)\n",
    "            #var_w = np.var(time_series[i-k:i+1])\n",
    "            \n",
    "            mean_w_and_1 = mean_w + (time_series[i-k]-time_series[i])/k\n",
    "            var_w_and_1 = np.mean((np.asarray(time_series[i-k:i]) - mean_w_and_1)**2)\n",
    "            #mean_w_and_1 = np.mean(time_series[i-k-1:i])\n",
    "            #var_w_and_1 = np.var(time_series[i-k-1:i])\n",
    "            \n",
    "            mean_2w = np.mean(time_series[i-2*k+1:i-k+1])\n",
    "            var_2w = np.mean((np.asarray(time_series[i-2*k+1:i-k+1]) - mean_2w)**2)\n",
    "            #var_2w = np.var(time_series[i-2*k:i-k+1])\n",
    "            \n",
    "            # diff of sliding windows\n",
    "            diff_mean_1 = mean_w - mean_w_and_1\n",
    "            diff_var_1 = var_w - var_w_and_1\n",
    "            \n",
    "            # diff of jumping windows\n",
    "            diff_mean_w = mean_w - mean_2w\n",
    "            diff_var_w = var_w - var_2w\n",
    "            \n",
    "            # f1\n",
    "            datum.append(mean_w)  # [0:2] is [0,1]\n",
    "            # f2\n",
    "            datum.append(var_w)\n",
    "            # f3\n",
    "            datum.append(diff_mean_1)\n",
    "            # f4\n",
    "            datum.append(diff_mean_1/(mean_w_and_1 + 1e-8))\n",
    "            # f5\n",
    "            datum.append(diff_var_1)\n",
    "            # f6\n",
    "            datum.append(diff_var_1/(var_w_and_1 + 1e-8))\n",
    "            # f7\n",
    "            datum.append(diff_mean_w)\n",
    "            # f8\n",
    "            datum.append(diff_mean_w/(mean_2w + 1e-8))\n",
    "            # f9\n",
    "            datum.append(diff_var_w)\n",
    "            # f10\n",
    "            datum.append(diff_var_w/(var_2w + 1e-8))\n",
    "            \n",
    "            # diff of sliding/jumping windows and current value\n",
    "            # f11\n",
    "            datum.append(time_series[i] - mean_w_and_1)\n",
    "            # f12\n",
    "            datum.append(time_series[i] - mean_2w)\n",
    "\n",
    "        data.append(np.asarray(datum))\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 112627 0\n",
      "1 110185 111628\n",
      "2 112113 220814\n",
      "3 108747 331928\n",
      "4 111171 439676\n",
      "5 111004 549848\n",
      "6 112786 659853\n",
      "7 112116 771640\n",
      "8 8784 882757\n",
      "9 10960 890542\n",
      "10 8249 900503\n",
      "11 8248 907753\n",
      "12 8784 915002\n",
      "13 8784 922787\n",
      "14 8784 930572\n",
      "15 147024 938357\n",
      "16 147010 1084382\n",
      "17 108901 1230393\n",
      "18 147629 1338295\n",
      "19 147669 1484925\n",
      "20 147690 1631595\n",
      "21 137925 1778286\n",
      "22 129454 1915212\n",
      "23 147681 2043667\n",
      "24 65450 2190349\n",
      "25 65436 2254800\n",
      "(2319237, 128)\n"
     ]
    }
   ],
   "source": [
    "testseries_features_new = []\n",
    "for i in range(len(testseries_all)):\n",
    "    print(i, len(testseries_all[i]), len(testseries_features_new))\n",
    "    features_temp = new_get_test_features(testseries_all[i], W)\n",
    "    features_temp = scaler_list_new[i].transform(features_temp)\n",
    "    if i==0:\n",
    "        testseries_features_new = features_temp\n",
    "    else:\n",
    "        testseries_features_new = np.concatenate((testseries_features_new, features_temp), axis = 0)\n",
    "    \n",
    "print(testseries_features_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test_feature.txt','wb')\n",
    "pickle.dump(testseries_features_new,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_features_label_new = np.array(timeseries_features_label_new)\n",
    "timeseries_features_label_vital_new = np.array(timeseries_features_label_vital_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 2843598.0\n",
      "[ 1.  1.  1. ... 23. 23. 23.] 4067635.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.5 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (4*ratio) * timeseries_features_label_vital_new + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label_new\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1081 - acc: 0.9837 - binary_accuracy: 0.9837\n",
      "Epoch 2/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1091 - acc: 0.9840 - binary_accuracy: 0.9840\n",
      "Epoch 3/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1076 - acc: 0.9841 - binary_accuracy: 0.9841\n",
      "Epoch 4/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.1024 - acc: 0.9843 - binary_accuracy: 0.9843\n",
      "Epoch 5/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1055 - acc: 0.9841 - binary_accuracy: 0.9841\n",
      "Epoch 6/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1048 - acc: 0.9844 - binary_accuracy: 0.9844\n",
      "Epoch 7/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1016 - acc: 0.9847 - binary_accuracy: 0.9847\n",
      "Epoch 8/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1045 - acc: 0.9847 - binary_accuracy: 0.9847\n",
      "Epoch 9/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1035 - acc: 0.9846 - binary_accuracy: 0.9846\n",
      "Epoch 10/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1014 - acc: 0.9850 - binary_accuracy: 0.9850\n",
      "Epoch 11/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0997 - acc: 0.9850 - binary_accuracy: 0.9850\n",
      "Epoch 12/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0957 - acc: 0.9851 - binary_accuracy: 0.9851\n",
      "Epoch 13/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1010 - acc: 0.9847 - binary_accuracy: 0.9847\n",
      "Epoch 14/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0980 - acc: 0.9854 - binary_accuracy: 0.9854\n",
      "Epoch 15/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0969 - acc: 0.9853 - binary_accuracy: 0.9853\n",
      "Epoch 16/20\n",
      "2450341/2450341 [==============================] - 33s 13us/step - loss: 0.1004 - acc: 0.9853 - binary_accuracy: 0.9853\n",
      "Epoch 17/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0952 - acc: 0.9853 - binary_accuracy: 0.9853\n",
      "Epoch 18/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0930 - acc: 0.9858 - binary_accuracy: 0.9858\n",
      "Epoch 19/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0964 - acc: 0.9855 - binary_accuracy: 0.9855 0s - loss: 0.0963 - acc: 0.9855 - binary_accuracy: \n",
      "Epoch 20/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.1028 - acc: 0.9846 - binary_accuracy: 0.9846\n",
      "It took 666 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "history = model.fit(timeseries_features_new, timeseries_features_label_new, epochs=20, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 7s 3us/step\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.9505989166094453\n",
      "0.9364700576861648\n",
      "0.9434815943661438\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(model.predict(timeseries_features, batch_size=5000,verbose=1)>0.95).astype(int)\n",
    "print(train_data_check)\n",
    "print(precision_score(timeseries_features_label, train_data_check))\n",
    "print(recall_score(timeseries_features_label, train_data_check))\n",
    "print(f1_score(timeseries_features_label, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 6s 3us/step\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.014157242230957854\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(model.predict(testseries_features_new,batch_size=5000,verbose=1))>0.999).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictn4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 2571830.0\n",
      "[1. 1. 1. ... 9. 9. 9.] 3050801.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.2 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (4*ratio) * timeseries_features_label_vital_new + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label_new\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0567 - acc: 0.9913 - binary_accuracy: 0.9913\n",
      "Epoch 2/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0566 - acc: 0.9922 - binary_accuracy: 0.9922\n",
      "Epoch 3/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0539 - acc: 0.9923 - binary_accuracy: 0.9923\n",
      "Epoch 4/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0559 - acc: 0.9921 - binary_accuracy: 0.9921\n",
      "Epoch 5/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0540 - acc: 0.9926 - binary_accuracy: 0.9926\n",
      "Epoch 6/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0532 - acc: 0.9926 - binary_accuracy: 0.9926\n",
      "Epoch 7/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0540 - acc: 0.9925 - binary_accuracy: 0.9925\n",
      "Epoch 8/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0540 - acc: 0.9926 - binary_accuracy: 0.9926\n",
      "Epoch 9/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0534 - acc: 0.9927 - binary_accuracy: 0.9927\n",
      "Epoch 10/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0547 - acc: 0.9926 - binary_accuracy: 0.9926\n",
      "Epoch 11/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0554 - acc: 0.9924 - binary_accuracy: 0.9924\n",
      "Epoch 12/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0545 - acc: 0.9925 - binary_accuracy: 0.9925\n",
      "Epoch 13/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0551 - acc: 0.9924 - binary_accuracy: 0.9924\n",
      "Epoch 14/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0538 - acc: 0.9927 - binary_accuracy: 0.9927\n",
      "Epoch 15/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0532 - acc: 0.9927 - binary_accuracy: 0.9927\n",
      "Epoch 16/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0542 - acc: 0.9927 - binary_accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0542 - acc: 0.9925 - binary_accuracy: 0.9925\n",
      "Epoch 18/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0516 - acc: 0.9929 - binary_accuracy: 0.9929\n",
      "Epoch 19/20\n",
      "2450341/2450341 [==============================] - 33s 14us/step - loss: 0.0515 - acc: 0.9930 - binary_accuracy: 0.9930\n",
      "Epoch 20/20\n",
      "2450341/2450341 [==============================] - 34s 14us/step - loss: 0.0523 - acc: 0.9929 - binary_accuracy: 0.9929\n",
      "It took 673 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "history = model.fit(timeseries_features_new, timeseries_features_label_new, epochs=20, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 7s 3us/step\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.9635409659625992\n",
      "0.9420319810593961\n",
      "0.952665083135392\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(model.predict(timeseries_features, batch_size=5000,verbose=1)>0.9).astype(int)\n",
    "print(train_data_check)\n",
    "print(precision_score(timeseries_features_label, train_data_check))\n",
    "print(recall_score(timeseries_features_label, train_data_check))\n",
    "print(f1_score(timeseries_features_label, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 6s 3us/step\n",
      "[0 0 0 ... 0 0 0]\n",
      "0.018293085182756224\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(model.predict(testseries_features_new,batch_size=5000,verbose=1))>0.99).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictn5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function l1 in module keras.regularizers:\n",
      "\n",
      "l1(l=0.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l1\n",
    "help(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(128, input_dim = 128))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(64))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.5))\n",
    "\n",
    "m.add(Dense(1))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 3095954.0\n",
      "[ 1.  1.  1. ... 36. 36. 36.] 5011838.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.8 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (4*ratio) * timeseries_features_label_vital_new + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label_new\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/50\n",
      "2450341/2450341 [==============================] - 27s 11us/step - loss: 0.9429 - binary_accuracy: 0.7647\n",
      "Epoch 2/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.7383 - binary_accuracy: 0.8635\n",
      "Epoch 3/50\n",
      "2450341/2450341 [==============================] - 21s 9us/step - loss: 0.6438 - binary_accuracy: 0.8969\n",
      "Epoch 4/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.5818 - binary_accuracy: 0.9128\n",
      "Epoch 5/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.5387 - binary_accuracy: 0.9228\n",
      "Epoch 6/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.5068 - binary_accuracy: 0.9301\n",
      "Epoch 7/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.4881 - binary_accuracy: 0.9345\n",
      "Epoch 8/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.4717 - binary_accuracy: 0.9371\n",
      "Epoch 9/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4559 - binary_accuracy: 0.9394\n",
      "Epoch 10/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4420 - binary_accuracy: 0.9402\n",
      "Epoch 11/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4410 - binary_accuracy: 0.9416\n",
      "Epoch 12/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4339 - binary_accuracy: 0.9429\n",
      "Epoch 13/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4192 - binary_accuracy: 0.9452\n",
      "Epoch 14/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4138 - binary_accuracy: 0.9467\n",
      "Epoch 15/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4159 - binary_accuracy: 0.9471\n",
      "Epoch 16/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4032 - binary_accuracy: 0.9470\n",
      "Epoch 17/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4066 - binary_accuracy: 0.9491\n",
      "Epoch 18/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.4013 - binary_accuracy: 0.9481\n",
      "Epoch 19/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3918 - binary_accuracy: 0.9494\n",
      "Epoch 20/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3910 - binary_accuracy: 0.9492\n",
      "Epoch 21/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3901 - binary_accuracy: 0.9508\n",
      "Epoch 22/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3819 - binary_accuracy: 0.9508\n",
      "Epoch 23/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3789 - binary_accuracy: 0.9519\n",
      "Epoch 24/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3821 - binary_accuracy: 0.9511\n",
      "Epoch 25/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3745 - binary_accuracy: 0.9529\n",
      "Epoch 26/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3716 - binary_accuracy: 0.9524\n",
      "Epoch 27/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3740 - binary_accuracy: 0.9527\n",
      "Epoch 28/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3776 - binary_accuracy: 0.9529\n",
      "Epoch 29/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3714 - binary_accuracy: 0.9528\n",
      "Epoch 30/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3774 - binary_accuracy: 0.9521\n",
      "Epoch 31/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3648 - binary_accuracy: 0.9538\n",
      "Epoch 32/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3596 - binary_accuracy: 0.9552\n",
      "Epoch 33/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3616 - binary_accuracy: 0.9550\n",
      "Epoch 34/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3594 - binary_accuracy: 0.9558\n",
      "Epoch 35/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3557 - binary_accuracy: 0.9554\n",
      "Epoch 36/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3586 - binary_accuracy: 0.9554\n",
      "Epoch 37/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3564 - binary_accuracy: 0.9555\n",
      "Epoch 38/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3535 - binary_accuracy: 0.9558\n",
      "Epoch 39/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3568 - binary_accuracy: 0.9556\n",
      "Epoch 40/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3523 - binary_accuracy: 0.9573\n",
      "Epoch 41/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3508 - binary_accuracy: 0.9570\n",
      "Epoch 42/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3540 - binary_accuracy: 0.9568\n",
      "Epoch 43/50\n",
      "2450341/2450341 [==============================] - 21s 8us/step - loss: 0.3496 - binary_accuracy: 0.9565\n",
      "Epoch 44/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3500 - binary_accuracy: 0.9566\n",
      "Epoch 45/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3450 - binary_accuracy: 0.9568\n",
      "Epoch 46/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3440 - binary_accuracy: 0.9568\n",
      "Epoch 47/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3475 - binary_accuracy: 0.9569\n",
      "Epoch 48/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3456 - binary_accuracy: 0.9576\n",
      "Epoch 49/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3446 - binary_accuracy: 0.9568\n",
      "Epoch 50/50\n",
      "2450341/2450341 [==============================] - 20s 8us/step - loss: 0.3429 - binary_accuracy: 0.9566\n",
      "It took 1027 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "h = m.fit(timeseries_features_new, timeseries_features_label_new, epochs=50, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341/2450341 [==============================] - 8s 3us/step\n"
     ]
    }
   ],
   "source": [
    "p = m.predict(timeseries_features_new, batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.8598601499945654\n",
      "0.44594975478682425\n",
      "0.587305122494432\n"
     ]
    }
   ],
   "source": [
    "train_data_check = np.ravel(p>0.95).astype(int)\n",
    "print(train_data_check)\n",
    "print(precision_score(timeseries_features_label, train_data_check))\n",
    "print(recall_score(timeseries_features_label, train_data_check))\n",
    "print(f1_score(timeseries_features_label, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341\n",
      "2476315\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data_check))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 127563\n",
      "257175 255177\n",
      "386210 383213\n",
      "514889 510893\n",
      "644017 639022\n",
      "772988 766994\n",
      "901655 894662\n",
      "1030508 1022516\n",
      "1039292 1030301\n",
      "1050252 1040262\n",
      "1058500 1047511\n",
      "1066747 1054759\n",
      "1075531 1062544\n",
      "1084315 1070329\n",
      "1093099 1078114\n",
      "1240123 1224139\n",
      "1387132 1370149\n",
      "1487386 1469404\n",
      "1635015 1616034\n",
      "1782683 1762703\n",
      "1930372 1909393\n",
      "2068297 2046319\n",
      "2197750 2174773\n",
      "2345430 2321454\n",
      "2410879 2385904\n",
      "2476315 2450341\n",
      "2476315\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "evaluation_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(timeseries_all)):\n",
    "    next_index += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    evaluation_new = np.concatenate((evaluation_new, train_data_check[last_index : next_index]))\n",
    "    print(len(evaluation_new),next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(timeseries_all)-1:\n",
    "        evaluation_new = np.concatenate((evaluation_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(evaluation_new))\n",
    "assert(len(evaluation_new) == len(train_data))\n",
    "evaluation_new = evaluation_new.astype(int)\n",
    "evaluation_df = pd.DataFrame({'KPI ID': train_data['KPI ID'], \n",
    "                         'timestamp': train_data['timestamp'], \n",
    "                         'predict': evaluation_new})\n",
    "evaluation_df.to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0\n",
      "OMP: Info #156: KMP_AFFINITY: 1 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 1 threads/core (1 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 15953 tid 15953 thread 0 bound to OS proc set 0\n",
      "{\"result\": true, \"data\": 0.5535412621971205, \"message\": \"\"}\n"
     ]
    }
   ],
   "source": [
    "!python evaluation.py \"../../input/train.csv\" \"evaluation.csv\" 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319237/2319237 [==============================] - 7s 3us/step\n"
     ]
    }
   ],
   "source": [
    "pm_t = m.predict(testseries_features_new,batch_size=5000,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.008406212905364998\n"
     ]
    }
   ],
   "source": [
    "predict_flagm = (np.ravel(pm_t)>0.9985).astype(int)\n",
    "print(predict_flagm)\n",
    "print(sum(predict_flagm)/len(predict_flagm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flagm[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictw2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Sequential()\n",
    "test.add(Dense(128, input_dim = 128, kernel_regularizer=l1(0.05)))\n",
    "test.add(BatchNormalization())\n",
    "test.add(Activation('relu'))\n",
    "test.add(Dropout(0.2))\n",
    "\n",
    "test.add(Dense(128, kernel_regularizer=l1(0.05)))\n",
    "test.add(BatchNormalization())\n",
    "test.add(Activation('relu'))\n",
    "test.add(Dropout(0.2))\n",
    "\n",
    "test.add(Dense(1))\n",
    "test.add(BatchNormalization())\n",
    "test.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "test.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/100\n",
      "2450341/2450341 [==============================] - 28s 12us/step - loss: 16.9901 - binary_accuracy: 0.7321\n",
      "Epoch 2/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7455 - binary_accuracy: 0.8211\n",
      "Epoch 3/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5959 - binary_accuracy: 0.8328\n",
      "Epoch 4/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5670 - binary_accuracy: 0.8379\n",
      "Epoch 5/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5445 - binary_accuracy: 0.8421\n",
      "Epoch 6/100\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 1.5064 - binary_accuracy: 0.8388\n",
      "Epoch 7/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5098 - binary_accuracy: 0.8362\n",
      "Epoch 8/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4633 - binary_accuracy: 0.8396\n",
      "Epoch 9/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3951 - binary_accuracy: 0.8399\n",
      "Epoch 10/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3987 - binary_accuracy: 0.8397\n",
      "Epoch 11/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4183 - binary_accuracy: 0.8386\n",
      "Epoch 12/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4494 - binary_accuracy: 0.8367\n",
      "Epoch 13/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4068 - binary_accuracy: 0.8361\n",
      "Epoch 14/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4650 - binary_accuracy: 0.8381\n",
      "Epoch 15/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3724 - binary_accuracy: 0.8367\n",
      "Epoch 16/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3816 - binary_accuracy: 0.8412\n",
      "Epoch 17/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4673 - binary_accuracy: 0.8465\n",
      "Epoch 18/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4579 - binary_accuracy: 0.8404\n",
      "Epoch 19/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3300 - binary_accuracy: 0.8401\n",
      "Epoch 20/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3207 - binary_accuracy: 0.8437\n",
      "Epoch 21/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3930 - binary_accuracy: 0.8394\n",
      "Epoch 22/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4473 - binary_accuracy: 0.8404\n",
      "Epoch 23/100\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 1.3866 - binary_accuracy: 0.8426\n",
      "Epoch 24/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3165 - binary_accuracy: 0.8424\n",
      "Epoch 25/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3066 - binary_accuracy: 0.8419\n",
      "Epoch 26/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3520 - binary_accuracy: 0.8399\n",
      "Epoch 27/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4500 - binary_accuracy: 0.8379\n",
      "Epoch 28/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3436 - binary_accuracy: 0.8418\n",
      "Epoch 29/100\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 1.3271 - binary_accuracy: 0.8380\n",
      "Epoch 30/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3003 - binary_accuracy: 0.8420\n",
      "Epoch 31/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4360 - binary_accuracy: 0.8354\n",
      "Epoch 32/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3933 - binary_accuracy: 0.8406\n",
      "Epoch 33/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3798 - binary_accuracy: 0.8385\n",
      "Epoch 34/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3655 - binary_accuracy: 0.8348\n",
      "Epoch 35/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5048 - binary_accuracy: 0.8399\n",
      "Epoch 36/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3625 - binary_accuracy: 0.8417\n",
      "Epoch 37/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3333 - binary_accuracy: 0.8394\n",
      "Epoch 38/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3273 - binary_accuracy: 0.8449\n",
      "Epoch 39/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4282 - binary_accuracy: 0.8377\n",
      "Epoch 40/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3299 - binary_accuracy: 0.8422\n",
      "Epoch 41/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3764 - binary_accuracy: 0.8398\n",
      "Epoch 42/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4505 - binary_accuracy: 0.8425\n",
      "Epoch 43/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3646 - binary_accuracy: 0.8388\n",
      "Epoch 44/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.2980 - binary_accuracy: 0.8381\n",
      "Epoch 45/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3081 - binary_accuracy: 0.8408\n",
      "Epoch 46/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4407 - binary_accuracy: 0.8380\n",
      "Epoch 47/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3365 - binary_accuracy: 0.8372\n",
      "Epoch 48/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4062 - binary_accuracy: 0.8395\n",
      "Epoch 49/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4059 - binary_accuracy: 0.8365\n",
      "Epoch 50/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3694 - binary_accuracy: 0.8355\n",
      "Epoch 51/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3395 - binary_accuracy: 0.8386\n",
      "Epoch 52/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3516 - binary_accuracy: 0.8392\n",
      "Epoch 53/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4234 - binary_accuracy: 0.8404\n",
      "Epoch 54/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3522 - binary_accuracy: 0.8399\n",
      "Epoch 55/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3765 - binary_accuracy: 0.8387\n",
      "Epoch 56/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3386 - binary_accuracy: 0.8388\n",
      "Epoch 57/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3695 - binary_accuracy: 0.8412\n",
      "Epoch 58/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3203 - binary_accuracy: 0.8421\n",
      "Epoch 59/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3220 - binary_accuracy: 0.8420\n",
      "Epoch 60/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3427 - binary_accuracy: 0.8422\n",
      "Epoch 61/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3089 - binary_accuracy: 0.8361\n",
      "Epoch 62/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3832 - binary_accuracy: 0.8422\n",
      "Epoch 63/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3966 - binary_accuracy: 0.8396\n",
      "Epoch 64/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4521 - binary_accuracy: 0.8454\n",
      "Epoch 65/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4012 - binary_accuracy: 0.8408\n",
      "Epoch 66/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3775 - binary_accuracy: 0.8387\n",
      "Epoch 67/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4016 - binary_accuracy: 0.8388\n",
      "Epoch 68/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3494 - binary_accuracy: 0.8368\n",
      "Epoch 69/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3399 - binary_accuracy: 0.8423\n",
      "Epoch 70/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3045 - binary_accuracy: 0.8426\n",
      "Epoch 71/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3259 - binary_accuracy: 0.8440\n",
      "Epoch 72/100\n",
      "2450341/2450341 [==============================] - 24s 10us/step - loss: 1.3525 - binary_accuracy: 0.8370\n",
      "Epoch 73/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3730 - binary_accuracy: 0.8417\n",
      "Epoch 74/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3805 - binary_accuracy: 0.8405\n",
      "Epoch 75/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3450 - binary_accuracy: 0.8427\n",
      "Epoch 76/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3461 - binary_accuracy: 0.8429\n",
      "Epoch 77/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.2980 - binary_accuracy: 0.8429\n",
      "Epoch 78/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3397 - binary_accuracy: 0.8454\n",
      "Epoch 79/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3756 - binary_accuracy: 0.8424\n",
      "Epoch 80/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4044 - binary_accuracy: 0.8380\n",
      "Epoch 81/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3269 - binary_accuracy: 0.8398\n",
      "Epoch 82/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3042 - binary_accuracy: 0.8452\n",
      "Epoch 83/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3519 - binary_accuracy: 0.8398\n",
      "Epoch 84/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3194 - binary_accuracy: 0.8399\n",
      "Epoch 85/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3377 - binary_accuracy: 0.8381\n",
      "Epoch 86/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3841 - binary_accuracy: 0.8419\n",
      "Epoch 87/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4183 - binary_accuracy: 0.8429\n",
      "Epoch 88/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3560 - binary_accuracy: 0.8470\n",
      "Epoch 89/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3688 - binary_accuracy: 0.8510\n",
      "Epoch 90/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3408 - binary_accuracy: 0.8485\n",
      "Epoch 91/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3805 - binary_accuracy: 0.8436\n",
      "Epoch 92/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3289 - binary_accuracy: 0.8435\n",
      "Epoch 93/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3001 - binary_accuracy: 0.8413\n",
      "Epoch 94/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3030 - binary_accuracy: 0.8400\n",
      "Epoch 95/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3152 - binary_accuracy: 0.8390\n",
      "Epoch 96/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3020 - binary_accuracy: 0.8397\n",
      "Epoch 97/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3790 - binary_accuracy: 0.8423\n",
      "Epoch 98/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.4258 - binary_accuracy: 0.8406\n",
      "Epoch 99/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3411 - binary_accuracy: 0.8394\n",
      "Epoch 100/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.3670 - binary_accuracy: 0.8418\n",
      "It took 2482 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "h_t = test.fit(timeseries_features_new, timeseries_features_label_new, epochs=100, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = Sequential()\n",
    "t1.add(Dense(128, input_dim = 128, kernel_regularizer=l1(0.1)))\n",
    "t1.add(BatchNormalization())\n",
    "t1.add(Activation('relu'))\n",
    "t1.add(Dropout(0.2))\n",
    "\n",
    "t1.add(Dense(128, kernel_regularizer=l1(0.1)))\n",
    "t1.add(BatchNormalization())\n",
    "t1.add(Activation('relu'))\n",
    "t1.add(Dropout(0.2))\n",
    "\n",
    "t1.add(Dense(1))\n",
    "t1.add(BatchNormalization())\n",
    "t1.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "t1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/100\n",
      "2450341/2450341 [==============================] - 28s 12us/step - loss: 29.8404 - binary_accuracy: 0.7187\n",
      "Epoch 2/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.9888 - binary_accuracy: 0.8087\n",
      "Epoch 3/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.9249 - binary_accuracy: 0.8315\n",
      "Epoch 4/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8623 - binary_accuracy: 0.8372\n",
      "Epoch 5/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7749 - binary_accuracy: 0.8334\n",
      "Epoch 6/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8153 - binary_accuracy: 0.8325\n",
      "Epoch 7/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8095 - binary_accuracy: 0.8360\n",
      "Epoch 8/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6953 - binary_accuracy: 0.8345\n",
      "Epoch 9/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8146 - binary_accuracy: 0.8314\n",
      "Epoch 10/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7855 - binary_accuracy: 0.8300\n",
      "Epoch 11/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7276 - binary_accuracy: 0.8357\n",
      "Epoch 12/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8731 - binary_accuracy: 0.8445\n",
      "Epoch 13/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8737 - binary_accuracy: 0.8263\n",
      "Epoch 14/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7194 - binary_accuracy: 0.8335\n",
      "Epoch 15/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6798 - binary_accuracy: 0.8368\n",
      "Epoch 16/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6671 - binary_accuracy: 0.8306\n",
      "Epoch 17/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7309 - binary_accuracy: 0.8361\n",
      "Epoch 18/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7269 - binary_accuracy: 0.8275\n",
      "Epoch 19/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6538 - binary_accuracy: 0.8331\n",
      "Epoch 20/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6826 - binary_accuracy: 0.8308\n",
      "Epoch 21/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6278 - binary_accuracy: 0.8331\n",
      "Epoch 22/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6798 - binary_accuracy: 0.8305\n",
      "Epoch 23/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7139 - binary_accuracy: 0.8338\n",
      "Epoch 24/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6387 - binary_accuracy: 0.8332\n",
      "Epoch 25/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5907 - binary_accuracy: 0.8354\n",
      "Epoch 26/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7843 - binary_accuracy: 0.8396\n",
      "Epoch 27/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6284 - binary_accuracy: 0.8320\n",
      "Epoch 28/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 2.0326 - binary_accuracy: 0.8370\n",
      "Epoch 29/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6893 - binary_accuracy: 0.8304\n",
      "Epoch 30/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8725 - binary_accuracy: 0.8451\n",
      "Epoch 31/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6633 - binary_accuracy: 0.8320\n",
      "Epoch 32/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6516 - binary_accuracy: 0.8333\n",
      "Epoch 33/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6601 - binary_accuracy: 0.8371\n",
      "Epoch 34/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6692 - binary_accuracy: 0.8356\n",
      "Epoch 35/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6642 - binary_accuracy: 0.8349\n",
      "Epoch 36/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6363 - binary_accuracy: 0.8369\n",
      "Epoch 37/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7194 - binary_accuracy: 0.8377\n",
      "Epoch 38/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6335 - binary_accuracy: 0.8341\n",
      "Epoch 39/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7317 - binary_accuracy: 0.8367\n",
      "Epoch 40/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7015 - binary_accuracy: 0.8391\n",
      "Epoch 41/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7349 - binary_accuracy: 0.8379\n",
      "Epoch 42/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6079 - binary_accuracy: 0.8357\n",
      "Epoch 43/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6447 - binary_accuracy: 0.8385\n",
      "Epoch 44/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6410 - binary_accuracy: 0.8404\n",
      "Epoch 45/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6273 - binary_accuracy: 0.8343\n",
      "Epoch 46/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6786 - binary_accuracy: 0.8350\n",
      "Epoch 47/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7245 - binary_accuracy: 0.8351\n",
      "Epoch 48/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6598 - binary_accuracy: 0.8317\n",
      "Epoch 49/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6944 - binary_accuracy: 0.8342\n",
      "Epoch 50/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6671 - binary_accuracy: 0.8356\n",
      "Epoch 51/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7202 - binary_accuracy: 0.8310\n",
      "Epoch 52/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7688 - binary_accuracy: 0.8443\n",
      "Epoch 53/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6327 - binary_accuracy: 0.8353\n",
      "Epoch 54/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6930 - binary_accuracy: 0.8344\n",
      "Epoch 55/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6374 - binary_accuracy: 0.8366\n",
      "Epoch 56/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6180 - binary_accuracy: 0.8336\n",
      "Epoch 57/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6553 - binary_accuracy: 0.8297\n",
      "Epoch 58/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6872 - binary_accuracy: 0.8364\n",
      "Epoch 59/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5748 - binary_accuracy: 0.8353\n",
      "Epoch 60/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5600 - binary_accuracy: 0.8376\n",
      "Epoch 61/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6797 - binary_accuracy: 0.8404\n",
      "Epoch 62/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6992 - binary_accuracy: 0.8270\n",
      "Epoch 63/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7367 - binary_accuracy: 0.8359\n",
      "Epoch 64/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6572 - binary_accuracy: 0.8425\n",
      "Epoch 65/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6107 - binary_accuracy: 0.8404\n",
      "Epoch 66/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6514 - binary_accuracy: 0.8361\n",
      "Epoch 67/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6860 - binary_accuracy: 0.8378\n",
      "Epoch 68/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6482 - binary_accuracy: 0.8365\n",
      "Epoch 69/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6225 - binary_accuracy: 0.8416\n",
      "Epoch 70/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7020 - binary_accuracy: 0.8380\n",
      "Epoch 71/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6588 - binary_accuracy: 0.8307\n",
      "Epoch 72/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6916 - binary_accuracy: 0.8328\n",
      "Epoch 73/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7315 - binary_accuracy: 0.8356\n",
      "Epoch 74/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6992 - binary_accuracy: 0.8353\n",
      "Epoch 75/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6116 - binary_accuracy: 0.8360\n",
      "Epoch 76/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6413 - binary_accuracy: 0.8350\n",
      "Epoch 77/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7062 - binary_accuracy: 0.8332\n",
      "Epoch 78/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6933 - binary_accuracy: 0.8376\n",
      "Epoch 79/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.8743 - binary_accuracy: 0.8332\n",
      "Epoch 80/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6647 - binary_accuracy: 0.8378\n",
      "Epoch 81/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6520 - binary_accuracy: 0.8380\n",
      "Epoch 82/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6239 - binary_accuracy: 0.8295\n",
      "Epoch 83/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6122 - binary_accuracy: 0.8343\n",
      "Epoch 84/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7525 - binary_accuracy: 0.8418\n",
      "Epoch 85/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6866 - binary_accuracy: 0.8286\n",
      "Epoch 86/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6281 - binary_accuracy: 0.8303\n",
      "Epoch 87/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6640 - binary_accuracy: 0.8341\n",
      "Epoch 88/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5960 - binary_accuracy: 0.8380\n",
      "Epoch 89/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6116 - binary_accuracy: 0.8356\n",
      "Epoch 90/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7074 - binary_accuracy: 0.8312\n",
      "Epoch 91/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6255 - binary_accuracy: 0.8335\n",
      "Epoch 92/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6621 - binary_accuracy: 0.8326\n",
      "Epoch 93/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6525 - binary_accuracy: 0.8336\n",
      "Epoch 94/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6717 - binary_accuracy: 0.8304\n",
      "Epoch 95/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6364 - binary_accuracy: 0.8334\n",
      "Epoch 96/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6026 - binary_accuracy: 0.8377\n",
      "Epoch 97/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.7037 - binary_accuracy: 0.8325\n",
      "Epoch 98/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.6766 - binary_accuracy: 0.8334\n",
      "Epoch 99/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5801 - binary_accuracy: 0.8324\n",
      "Epoch 100/100\n",
      "2450341/2450341 [==============================] - 25s 10us/step - loss: 1.5921 - binary_accuracy: 0.8386\n",
      "It took 2492 seconds to train the model!\n"
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "h_t1 = t1.fit(timeseries_features_new, timeseries_features_label_new, epochs=100, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450341, 128)\n",
      "(2319237, 128)\n"
     ]
    }
   ],
   "source": [
    "print(timeseries_features_new.shape)\n",
    "print(testseries_features_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.31490319e-06  1.15616180e-05  7.31490319e-06  1.94693803e+00\n",
      "  1.15616180e-05  4.80487943e-03 -1.92543333e-06 -1.07310891e-02\n",
      "  1.94693225e+00  1.04023104e-02  1.25243347e-05  2.03071919e-03\n",
      "  1.18228370e-07  3.26021319e+04  2.52730025e-05  5.12386077e-03\n",
      "  1.39358402e-07  5.03469111e+04  1.83051437e-05  3.10538115e-05\n",
      "  1.94691372e+00  2.42435890e-02  1.22141007e-05  4.34735801e-04\n",
      "  4.30472062e-08  6.65788249e+02  5.85065655e-05  4.15036881e-03\n",
      "  5.24113795e-07  9.88633839e+03  3.65214554e-05  8.28139202e-05\n",
      "  1.94688447e+00  3.25345479e-02  1.19198745e-05  1.29097877e-04\n",
      "  8.62630058e-08  3.26284234e+01  1.16814951e-04  3.53093725e-03\n",
      "  2.80210169e-06  1.52484960e+03  6.54805120e-05  1.70375588e-04\n",
      "  1.94679561e+00  4.14839491e-02  1.20202948e-05  3.22326955e-05\n",
      "  2.47354161e-07  8.73884756e-01  2.92743303e-04  4.34386068e-03\n",
      "  3.08897323e-06  1.04461288e+02  1.54436068e-04  4.35159076e-04\n",
      "  1.94664924e+00  4.86315837e-02  1.12977645e-05  1.50448778e-05\n",
      "  4.40078534e-08  8.97180574e-02  4.94057445e-04  8.14740071e-03\n",
      "  1.52505080e-06  7.20835359e+01  3.00085189e-04  7.82844870e-04\n",
      "  1.94640221e+00  6.04792949e-02  9.70374325e-06  8.96951084e-06\n",
      " -2.66093598e-07  3.76660153e-03  6.16169954e-04  2.14896596e-02\n",
      " -2.33529684e-05  5.39366526e+00  5.45519890e-04  1.15198610e-03\n",
      "  1.94609413e+00  9.04662197e-02  6.25837388e-06  5.09423168e-06\n",
      " -1.26553361e-06  1.40690365e-03  3.14154088e-04  5.69365085e-02\n",
      " -1.35734555e-04  1.49594894e+01  8.50159498e-04  1.15805521e-03\n",
      "  1.94596309e+00  1.27851610e-01  3.97531603e-06  3.26140158e-06\n",
      " -2.17193597e-06  2.96077421e-04  3.50625693e-06  8.51097682e-02\n",
      " -2.03283123e-04  6.10526195e+00  9.78909259e-04  9.78440200e-04\n",
      "  1.94593705e+00  1.68685917e-01  2.38696967e-06  2.10209754e-06\n",
      " -2.63603149e-06  1.24791136e-04  1.01509430e-04  9.36625710e-02\n",
      " -2.16596310e-04  1.94315436e+00  1.00336514e-03  1.10248760e-03\n",
      "  1.94595836e+00  2.09699689e-01  1.76498517e-06  1.47951804e-06\n",
      " -2.36411882e-06  5.45968836e-05 -1.38319522e-04  8.81412882e-02\n",
      "  1.49727799e-04  1.87399410e+00  9.81432290e-04  8.41347783e-04] [4.16092414e-02 4.16092414e-02 4.16092414e-02 4.27837950e-01\n",
      " 4.16092414e-02 1.06576586e-02 9.86465864e-02 4.28985288e-01\n",
      " 4.17437872e-01 4.87670658e-03 1.69473583e-02 4.21959836e-03\n",
      " 6.25736869e-03 4.85589656e+11 4.27123286e-02 1.11379878e-02\n",
      " 7.57405435e-03 3.23171392e+12 4.42971452e-02 6.19323903e-02\n",
      " 4.03600497e-01 1.04802264e-02 3.37329305e-03 8.61834422e-04\n",
      " 3.01808930e-03 9.26810317e+09 3.31648806e-02 8.62651413e-03\n",
      " 1.37628485e-02 4.80443629e+11 4.63839812e-02 6.40184688e-02\n",
      " 3.95317459e-01 1.16334846e-02 9.31973142e-04 2.44167658e-04\n",
      " 1.02020462e-03 7.15711660e+06 2.72410393e-02 7.17867913e-03\n",
      " 1.28451491e-02 9.89524373e+09 4.77965684e-02 6.75131133e-02\n",
      " 3.86396212e-01 1.27612979e-02 1.79229200e-04 5.00948579e-05\n",
      " 2.47831672e-04 8.16195812e+03 2.85966306e-02 8.60002505e-03\n",
      " 1.47103765e-02 4.93006294e+07 5.33634138e-02 8.38613689e-02\n",
      " 3.79268198e-01 1.32103230e-02 5.68333997e-05 1.73090120e-05\n",
      " 8.17611685e-05 2.96436696e+02 4.73936508e-02 1.60917793e-02\n",
      " 1.67849186e-02 1.77113598e+07 6.20644587e-02 1.33579859e-01\n",
      " 3.67416631e-01 1.32249149e-02 2.31522612e-05 7.66590577e-06\n",
      " 2.63030047e-05 5.85456995e-01 1.19900613e-01 4.54390793e-02\n",
      " 1.83978709e-02 7.47269258e+03 8.68678481e-02 2.91017830e-01\n",
      " 3.37344741e-01 1.69903886e-02 1.21124748e-05 4.21618522e-06\n",
      " 9.36482373e-06 7.69725801e-02 3.12607223e-01 1.28828176e-01\n",
      " 2.85742621e-02 2.30263942e+04 1.60231433e-01 6.62993154e-01\n",
      " 2.99914290e-01 2.38213269e-02 8.43393772e-06 2.99313955e-06\n",
      " 5.80512125e-06 6.95786322e-04 4.79657898e-01 1.97666616e-01\n",
      " 4.24561176e-02 1.76638884e+03 2.47523979e-01 9.18520108e-01\n",
      " 2.59115492e-01 3.17222738e-02 6.25872690e-06 2.18158074e-06\n",
      " 4.36526681e-06 3.24596137e-04 5.66631802e-01 2.17396803e-01\n",
      " 5.29122401e-02 2.69988367e+01 3.34750727e-01 9.95559385e-01\n",
      " 2.18176771e-01 4.01699109e-02 4.57586571e-06 1.49936630e-06\n",
      " 3.52528595e-06 1.79786464e-04 5.83754235e-01 1.99946752e-01\n",
      " 6.64852899e-02 1.84010022e+01 4.09805282e-01 9.66175239e-01]\n",
      "[-1.05010032e-04  6.02781594e-07 -1.05010032e-04  3.15013447e+01\n",
      "  6.02781594e-07  1.12844033e-03  1.40647046e-06 -5.77927431e+00\n",
      "  3.15013444e+01  5.74042054e-01 -1.00453635e-07  5.03597464e-04\n",
      "  1.71300648e-07  1.08027540e+06  3.49111131e-06  1.34956141e-03\n",
      "  2.69913187e-07  1.26913574e+06  2.00937162e-07  3.79250211e-06\n",
      "  3.15013380e+01  1.55480048e+00  1.12418334e-05  1.39825104e-04\n",
      "  2.68906208e-06  4.60304546e-01  8.90003601e-05  1.78847157e-03\n",
      "  2.67295316e-06  2.83599002e+00  1.79728756e-05  9.57314023e-05\n",
      "  3.15012935e+01  2.54240696e+00  1.97560928e-05  4.58226462e-05\n",
      "  1.61274495e-05  8.27270386e-02  1.51995820e-04  2.01790707e-03\n",
      " -2.84138438e-04  1.60458354e+00  7.09873150e-05  2.03227042e-04\n",
      "  3.15012326e+01  4.09484038e+00  5.00304239e-07  1.16445285e-05\n",
      " -9.95644491e-05  1.48623272e-02 -5.97374446e-04  2.89614635e-03\n",
      " -1.88039506e-03  1.34247719e+00  1.12567017e-04 -4.85307734e-04\n",
      "  3.15015313e+01  5.59148747e+00 -1.42065049e-05  3.15519010e-06\n",
      " -8.21145498e-05  3.35292886e-03 -1.20974493e-03  3.92264711e-03\n",
      " -2.18049237e-03  5.19378570e-01 -2.00827016e-04 -1.39636544e-03\n",
      "  3.15021362e+01  7.35514523e+00 -2.47968340e-05  1.15708579e-06\n",
      " -7.15094105e-05  6.57816228e-04 -6.24312384e-03  9.65250399e-03\n",
      " -1.23633503e-02  3.83419726e-01 -8.16289808e-04 -7.03461682e-03\n",
      "  3.15052577e+01  1.19127080e+01 -6.56951578e-05 -7.00597778e-07\n",
      " -3.15246825e-04  1.31762544e-04 -2.46369824e-02  1.90521751e-02\n",
      " -4.21026314e-02  4.90175563e-01 -3.97875005e-03 -2.85500373e-02\n",
      "  3.15107918e+01  1.68750956e+01 -9.39654220e-05 -1.61079619e-06\n",
      " -5.02682698e-04  4.96868896e-05 -4.41601219e-02  2.91258364e-02\n",
      " -4.27593282e-02  6.09092175e-01 -9.54106152e-03 -5.36072180e-02\n",
      "  3.15175762e+01  2.18167872e+01 -1.13877315e-04 -2.11240208e-06\n",
      " -4.71359507e-04  1.64678949e-05 -5.63762663e-02  3.96017109e-02\n",
      " -9.53941053e-02  5.12011431e-01 -1.63454234e-02 -7.26078124e-02\n",
      "  3.15251418e+01  2.69999492e+01 -1.26444108e-04 -2.46835911e-06\n",
      " -3.51677146e-04  8.76965836e-06 -6.35705998e-02  4.58653245e-02\n",
      " -1.59837293e-01  4.53973575e-01 -2.39235730e-02 -8.73677287e-02] [2.29616822e+00 2.29616822e+00 2.29616822e+00 1.39202324e+02\n",
      " 2.29616822e+00 2.38775596e-03 5.00186911e+00 1.07342904e+02\n",
      " 1.38628279e+02 7.45061809e+00 1.04570060e+00 1.02818882e-03\n",
      " 9.97521627e+00 4.84032274e+14 2.81374357e+00 2.80970407e-03\n",
      " 1.22042160e+01 6.43688236e+14 2.66544342e+00 4.11012644e+00\n",
      " 1.37647450e+02 2.22006139e+01 3.00187441e-01 2.81059962e-04\n",
      " 7.39621201e+00 3.30479040e+01 3.95043127e+00 3.70191203e-03\n",
      " 3.49934731e+01 3.90950296e+03 3.63221944e+00 6.94792633e+00\n",
      " 1.36659331e+02 3.61208110e+01 9.77235530e-02 9.07164941e-05\n",
      " 3.47002202e+00 1.57628191e+00 4.50201216e+00 4.10800477e-03\n",
      " 5.63277078e+01 1.07749480e+02 4.52924678e+00 9.04105824e+00\n",
      " 1.35106613e+02 4.95752035e+01 2.39505927e-02 2.32590215e-05\n",
      " 1.00136842e+00 5.02702617e-02 5.98282720e+00 5.82529458e-03\n",
      " 8.04207683e+01 2.22054975e+01 6.38850494e+00 1.23816166e+01\n",
      " 1.33616316e+02 5.83542216e+01 6.65629747e-03 7.02586995e-06\n",
      " 3.73973151e-01 8.07655979e-03 7.05026861e+00 7.92190796e-03\n",
      " 6.55538689e+01 4.74781555e+00 7.92275411e+00 1.86915266e+01\n",
      " 1.31864087e+02 9.07197696e+01 3.33366630e-03 3.54465196e-06\n",
      " 1.42537454e-01 1.40518116e-03 1.82054852e+01 1.94065574e-02\n",
      " 1.14113211e+02 2.87393502e+00 1.17115020e+01 4.06195404e+01\n",
      " 1.27383750e+02 2.38096195e+02 1.63740725e-03 1.68569436e-06\n",
      " 7.98275511e-02 3.06382924e-04 3.95315049e+01 3.90516129e-02\n",
      " 3.93665495e+02 2.71180119e+00 2.17746688e+01 8.14761185e+01\n",
      " 1.22599335e+02 4.47741529e+02 9.86979942e-04 9.77484280e-07\n",
      " 7.68894665e-02 1.46448303e-04 6.23165667e+01 5.96021741e-02\n",
      " 7.98412402e+02 3.71717604e+00 3.16411391e+01 1.30326179e+02\n",
      " 1.17890741e+02 6.39728026e+02 8.03328018e-04 7.92719861e-07\n",
      " 6.26305564e-02 7.43493920e-05 8.43684669e+01 8.18924890e-02\n",
      " 1.09564935e+03 2.55214613e+00 4.18997955e+01 1.62418754e+02\n",
      " 1.12949959e+02 8.16600273e+02 6.81323501e-04 6.76121321e-07\n",
      " 4.99917108e-02 4.56833847e-05 9.72149487e+01 9.72310456e-02\n",
      " 1.33871536e+03 1.66925691e+00 5.37782840e+01 1.67629726e+02]\n",
      "[-1.18233820e-05  1.23071441e-05 -1.18233820e-05  2.45426276e+00\n",
      "  1.23071441e-05  1.37934725e+04 -2.66260274e-06 -2.20024497e-01\n",
      "  2.45425661e+00  1.68476740e-02  1.36384455e-05  5.82281932e+03\n",
      "  1.05899030e-07  1.87077003e+05  2.89336223e-05  2.52840862e+04\n",
      "  1.19458636e-07  4.00069704e+05  1.97920175e-05  3.50871943e-05\n",
      "  2.45423482e+00  5.68578463e-02  1.41768839e-05  1.70312005e+03\n",
      "  9.81488227e-08  1.74662605e+04  7.04110687e-05  5.57764621e+04\n",
      "  2.63033038e-07  5.53052925e+05  4.21164681e-05  9.83506530e-05\n",
      "  2.45419961e+00  1.07088842e-01  1.38514547e-05  5.75122840e+02\n",
      "  6.83178133e-08  3.30482654e+03  1.42354614e-04  9.17684831e+04\n",
      "  7.28921704e-07  9.78473869e+05  7.69965733e-05  2.05499733e-04\n",
      "  2.45409278e+00  1.60027708e-01  1.41532163e-05  1.84844061e+02\n",
      "  8.56121782e-08  3.91358942e+02  3.39405013e-04  1.11117466e+05\n",
      "  1.47814063e-06  1.16928854e+06  1.84129085e-04  5.09380882e-04\n",
      "  2.45392308e+00  1.87796502e-01  1.31071091e-05  7.68015274e+01\n",
      "  2.58988337e-08  8.72792617e+01  4.69414622e-04  1.19694619e+05\n",
      " -1.49785323e-05  4.39177518e+05  3.52785484e-04  8.09092997e-04\n",
      "  2.45368837e+00  2.18568127e-01  8.87238637e-06  2.21294479e+01\n",
      " -7.58273967e-07  1.62226036e+01  9.23514143e-05  1.05945897e+05\n",
      " -1.12618089e-04  4.14405182e+05  5.83258072e-04  6.66737100e-04\n",
      "  2.45364220e+00  2.82269660e-01  1.48999305e-06  8.78674340e+00\n",
      " -3.57413720e-06  7.15274785e+00 -1.46096863e-03  8.44406325e+04\n",
      " -2.43874544e-04  3.51329113e+05  6.22051386e-04 -8.40407237e-04\n",
      "  2.45392022e+00  3.54855563e-01 -2.72366564e-06  2.82054736e+00\n",
      " -4.63574622e-06  1.24522611e+00 -3.04303693e-03  9.55226360e+04\n",
      " -2.27511908e-04  3.95054898e+05  3.39819260e-04 -2.70049400e-03\n",
      "  2.45437268e+00  4.31173420e-01 -5.37115143e-06  2.11542195e+00\n",
      " -2.94415230e-06  9.34553923e-01 -4.08541256e-03  1.51316899e+05\n",
      " -4.47079917e-04  5.46569396e+05 -1.15294073e-04 -4.19533548e-03\n",
      "  2.45490880e+00  5.09686962e-01 -6.80142511e-06  1.56217534e+00\n",
      " -1.33995011e-06  7.37194953e-01 -4.83028102e-03  2.40453363e+05\n",
      " -6.94148256e-04  7.56845184e+05 -6.52839774e-04 -5.47631937e-03] [6.73906958e-02 6.73906958e-02 6.73906958e-02 1.31868115e+00\n",
      " 6.73906958e-02 7.52691099e+11 1.26793249e-01 1.28593339e+00\n",
      " 1.30181594e+00 9.76144139e-02 3.56921716e-02 1.50355275e+11\n",
      " 1.33337033e-01 9.68900837e+13 1.08782550e-01 3.79499206e+12\n",
      " 1.74092358e-01 3.75862792e+14 8.82321231e-02 1.63028925e-01\n",
      " 1.26174054e+00 6.80941943e-01 1.39909823e-02 1.82842976e+10\n",
      " 2.33034779e-01 1.12360617e+12 2.00924503e-01 1.46725345e+13\n",
      " 7.91596572e-01 1.04980456e+15 1.56448329e-01 3.32441775e-01\n",
      " 1.21140723e+00 1.61276158e+00 4.10728421e-03 2.99101904e+09\n",
      " 1.03279447e-01 5.10684867e+10 1.71219251e-01 2.18039894e+13\n",
      " 2.27616421e+00 2.52178867e+15 2.00309332e-01 3.02406553e-01\n",
      " 1.15815282e+00 1.89127500e+00 7.02567713e-04 4.59038111e+08\n",
      " 2.85814411e-02 1.31245426e+09 1.11078017e-01 1.50158838e+13\n",
      " 3.02444637e+00 2.33849038e+15 2.12288417e-01 3.00579425e-01\n",
      " 1.12988818e+00 1.48775437e+00 1.91095455e-04 1.11724831e+08\n",
      " 8.84573506e-03 1.24340902e+08 1.23056323e-01 1.14729810e+13\n",
      " 2.42849075e+00 1.37688338e+14 2.29437076e-01 3.89239584e-01\n",
      " 1.09840411e+00 1.03151125e+00 6.75563706e-05 1.63794515e+07\n",
      " 2.77150426e-03 1.61740621e+07 2.54580885e-01 7.98298866e+12\n",
      " 1.75198170e+00 1.16573156e+14 2.80551726e-01 6.87922552e-01\n",
      " 1.03465035e+00 6.72980195e-01 2.87061376e-05 3.59320459e+06\n",
      " 7.45839275e-04 3.96356559e+06 5.95125159e-01 4.39418069e+12\n",
      " 1.26261762e+00 7.97753420e+13 4.22952131e-01 1.35557918e+00\n",
      " 9.63273926e-01 5.27401713e-01 1.81713654e-05 4.15818406e+05\n",
      " 3.42519908e-04 1.70485078e+05 9.33923755e-01 3.33645027e+12\n",
      " 1.07154771e+00 6.37403427e+13 5.80996992e-01 1.99569086e+00\n",
      " 8.88799544e-01 4.50287954e-01 1.33836313e-05 2.33897853e+05\n",
      " 1.94750808e-04 9.60582692e+04 1.20700000e+00 9.50877222e+12\n",
      " 9.92779249e-01 1.11692328e+14 7.42966058e-01 2.36990119e+00\n",
      " 8.12217330e-01 4.03710103e-01 1.04564577e-05 1.47525521e+05\n",
      " 1.24851400e-04 6.15239473e+04 1.33406002e+00 2.87012695e+13\n",
      " 9.37314458e-01 2.45001951e+14 9.07348788e-01 2.33793340e+00]\n",
      "[-5.93907172e-14  0.00000000e+00 -5.93907172e-14  1.05739611e-02\n",
      "  0.00000000e+00  6.35873525e+05  7.78234341e-20 -2.51944069e-04\n",
      "  1.05739611e-02  6.84795238e-03  1.39125692e-20  2.87822405e+05\n",
      " -6.52151683e-21  2.05155143e+05  3.14119727e-20  6.13826200e+05\n",
      " -1.21734981e-20  3.87640856e+05  5.83675756e-20  9.09751597e-20\n",
      "  1.05739611e-02  1.12194896e-02  2.91294418e-20  1.04247582e+05\n",
      " -6.32858862e-20  1.15200545e+05  2.98359395e-20  6.61703662e+05\n",
      "  3.64151989e-19  7.00598780e+05  6.85302727e-20 -5.97805709e-22\n",
      "  1.05739611e-02  1.38007149e-02  3.36945036e-21  4.63358786e+04\n",
      " -4.98624307e-21  6.33480809e+04 -2.94419312e-19  6.98425634e+05\n",
      "  7.71427508e-19  9.29058021e+05 -2.48632829e-20  1.24696836e-19\n",
      "  1.05739611e-02  1.65123524e-02  1.03257350e-20  1.26347065e+04\n",
      " -1.13501566e-19  1.44645893e+04  3.13283213e-07  5.30362386e+05\n",
      "  5.81453651e-08  6.09065495e+05  6.30983926e-19  3.13283213e-07\n",
      "  1.05738044e-02  1.76665498e-02  6.26566425e-08  3.36492417e+03\n",
      "  1.20300755e-08  1.96394709e+03 -2.76733513e-06  3.00273033e+05\n",
      " -6.16485712e-07  2.37174448e+05  2.19298249e-07 -2.61069352e-06\n",
      "  1.05751881e-02  1.83078878e-02 -8.61528852e-08  5.61433254e+02\n",
      " -2.03799443e-08  1.71696499e+02 -1.37740188e-05  1.01614157e+05\n",
      " -3.02696691e-06  1.08810440e+05 -1.31317884e-06 -1.50010448e-05\n",
      "  1.05820751e-02  1.86477186e-02 -8.09314982e-08  2.68923990e+01\n",
      " -1.79102282e-08  8.49935957e+00 -4.95509636e-06  7.04336887e+03\n",
      " -1.32107321e-06  4.41213228e+03 -8.19496686e-06 -1.30691317e-05\n",
      "  1.05841367e-02  1.87662350e-02 -4.61222520e-08  1.95911987e+00\n",
      " -1.00562951e-08  4.97957545e-01 -1.01381953e-06  7.38098227e+02\n",
      " -1.96848788e-06  4.33633302e+02 -1.02217353e-05 -1.11894325e-05\n",
      "  1.05845527e-02  1.88277552e-02 -2.74122823e-08  6.50937122e-04\n",
      " -1.21821491e-08  7.75064409e-03 -4.35333380e-07  8.76491766e-01\n",
      " -2.26441229e-06  1.39599483e+01 -1.06189958e-05 -1.10269169e-05\n",
      "  1.05845897e-02  1.88672504e-02 -1.87969937e-08  4.15136614e-04\n",
      " -9.23224809e-09  6.18996191e-03 -7.72765528e-07  8.30127342e-01\n",
      " -1.73917335e-06  1.58225986e+01 -1.06474524e-05 -1.14014210e-05] [2.73918095e-02 2.73918095e-02 2.73918095e-02 1.90293543e-02\n",
      " 2.73918095e-02 1.28227688e+14 8.01978847e-02 1.90289186e-02\n",
      " 1.21814019e-02 3.09079307e-01 7.34233833e-03 1.99788094e+13\n",
      " 3.14756227e-01 7.71319681e+14 1.43976395e-02 5.39948035e+13\n",
      " 4.85652733e-01 1.45447725e+15 2.15326290e-02 2.09585548e-02\n",
      " 7.80986472e-03 3.39359332e-01 1.34938583e-03 2.86897189e+12\n",
      " 8.89021315e-02 3.01737528e+14 1.03249012e-02 3.88118700e+13\n",
      " 4.91617646e-01 2.02480209e+15 1.81933282e-02 2.48678322e-02\n",
      " 5.22863941e-03 2.87466794e-01 3.75044523e-04 7.01720534e+11\n",
      " 3.37805879e-02 9.54682591e+13 8.72532709e-03 3.52749883e+13\n",
      " 4.92957927e-01 2.08848092e+15 1.93263356e-02 2.39801225e-02\n",
      " 2.51700184e-03 1.80928055e-01 6.06016935e-05 6.01093014e+10\n",
      " 7.38529863e-03 6.56796519e+12 4.61690581e-03 1.03593313e+13\n",
      " 3.45114238e-01 5.49312546e+14 1.94571188e-02 2.14188928e-02\n",
      " 1.36277644e-03 1.06544636e-01 1.52515390e-05 3.99657049e+09\n",
      " 2.04701236e-03 1.34403886e+11 2.56411900e-03 2.45883870e+12\n",
      " 2.08728472e-01 1.31328510e+13 1.92888472e-02 2.03152769e-02\n",
      " 7.21764237e-04 5.79319012e-02 3.80819290e-06 1.73063120e+08\n",
      " 5.39345889e-04 5.80340067e+07 1.35326909e-03 6.23257116e+11\n",
      " 1.14901960e-01 2.24798059e+13 1.91605367e-02 1.97047307e-02\n",
      " 3.83453606e-04 3.02004640e-02 9.51530649e-07 4.26014603e+06\n",
      " 1.38392367e-04 9.63846312e+05 7.17504233e-04 1.57036299e+10\n",
      " 6.03042657e-02 1.52153035e+10 1.90942052e-02 1.93914078e-02\n",
      " 2.65373161e-04 2.03924166e-02 4.22753654e-07 1.63164503e+05\n",
      " 6.20328701e-05 1.01319705e+04 4.96661141e-04 1.01395134e+09\n",
      " 4.04800955e-02 5.46975341e+08 1.90643579e-02 1.92739576e-02\n",
      " 2.04068059e-04 1.53832808e-02 2.38229641e-07 1.90046666e-03\n",
      " 3.50564847e-05 1.36030009e+00 3.83492433e-04 9.12978470e+00\n",
      " 3.02523298e-02 2.20726484e+04 1.90635660e-02 1.92351768e-02\n",
      " 1.65038894e-04 1.23375430e-02 1.50020733e-07 1.15525299e-03\n",
      " 2.24346084e-05 9.24581040e-01 3.17921289e-04 8.45877988e+00\n",
      " 2.41844558e-02 2.60616740e+04 1.90449526e-02 1.92221813e-02]\n",
      "[-5.17165512e-05  3.12185078e-06 -5.17165512e-05  4.28362258e+00\n",
      "  3.12185078e-06  3.12217835e+02 -3.12185823e-06 -6.70478537e-01\n",
      "  4.28362102e+00  3.20131235e+00  4.68277989e-06  5.85439692e+01\n",
      "  1.40483635e-06  3.04952864e+05  1.03411391e-05  3.70742362e+02\n",
      "  2.15115742e-06  4.34464838e+05  6.24370528e-06  1.19020645e-05\n",
      "  4.28361415e+00  1.38820612e+01  1.63897277e-06  2.34159155e+01\n",
      " -7.66414798e-07  3.46777088e+00 -6.47784893e-06  1.02243397e+03\n",
      " -1.03629958e-05  3.07377927e+02  1.00679758e-05  1.95115413e-06\n",
      "  4.28361739e+00  2.93674583e+01 -1.24874217e-06  1.17077338e+01\n",
      " -1.99018118e-07  1.82752171e+00 -3.54330417e-05  1.68994902e+03\n",
      " -8.44070171e-07  5.41593390e+02  3.94133642e-06 -3.02429631e-05\n",
      "  4.28364269e+00  4.82408226e+01 -2.24773479e-06  4.68305592e+00\n",
      "  5.80715293e-09  7.45882032e-01 -9.41863248e-05  4.15326693e+03\n",
      " -1.23187557e-05  1.08741516e+03 -2.23524764e-05 -1.14291066e-04\n",
      "  4.28368978e+00  5.70772924e+01 -3.30916503e-06  2.34153318e+00\n",
      " -7.06630984e-07  3.77537179e-01 -2.11849000e-04  5.72109550e+03\n",
      " -7.09276379e-05  1.05403600e+03 -7.05070691e-05 -2.79046904e-04\n",
      "  4.28379571e+00  6.17349403e+01 -5.19583280e-06  5.22379913e-05\n",
      " -2.12630828e-06  1.42119086e-02 -1.02265824e-03  4.32087888e-02\n",
      " -2.33275891e-04  1.06973244e+02 -1.78318237e-04 -1.19578065e-03\n",
      "  4.28430703e+00  6.37473850e+01 -1.05758160e-05  1.79987163e-05\n",
      " -3.92785300e-06  6.04582431e-03 -4.02062471e-03  6.00470887e-02\n",
      " -8.86328692e-04  9.67800268e+01 -6.95027341e-04 -4.70507624e-03\n",
      "  4.28519864e+00  6.47955100e+01 -1.56581487e-05  9.68860556e-06\n",
      " -6.88725548e-06  3.53700072e-03 -7.10509364e-03  7.85525297e-02\n",
      " -1.42564107e-03  8.09830868e+01 -1.59171663e-03 -8.68115212e-03\n",
      "  4.28631735e+00  6.54885891e+01 -1.87954239e-05  5.33553778e-06\n",
      " -5.43148228e-06  2.58504892e-03 -9.19988109e-03  9.36009064e-02\n",
      " -2.52454450e-03  7.60355367e+01 -2.71355930e-03 -1.18946450e-02\n",
      "  4.28754095e+00  6.59994362e+01 -2.00064534e-05  2.73191107e-06\n",
      " -3.55764115e-06  2.09015476e-03 -1.02318401e-02  1.01110144e-01\n",
      " -4.17086066e-03  7.01574218e+01 -3.93837029e-03 -1.41502039e-02] [1.28052494e+01 1.28052494e+01 1.28052494e+01 7.00726987e+01\n",
      " 1.28052494e+01 6.63384189e+09 1.68981247e+01 6.96430633e+01\n",
      " 6.68713826e+01 1.87557661e+04 8.58071540e+00 4.39007292e+08\n",
      " 2.08416559e+04 5.14032785e+13 2.81945781e+01 1.29262877e+10\n",
      " 3.04639068e+04 2.82519695e+14 2.03627445e+01 4.24290377e+01\n",
      " 5.61906121e+01 2.33700225e+05 3.94897908e+00 7.02411668e+07\n",
      " 7.80988751e+04 1.01309906e+06 6.19415676e+01 3.56427194e+10\n",
      " 2.78645634e+05 2.06087658e+09 4.24732886e+01 1.01844602e+02\n",
      " 4.07052277e+01 6.22307901e+05 1.25962526e+00 1.75602917e+07\n",
      " 4.04496899e+04 3.20776655e+05 5.98132824e+01 4.53492257e+10\n",
      " 9.59236816e+05 3.08448197e+09 5.93973583e+01 1.02536252e+02\n",
      " 2.18319354e+01 8.09459341e+05 2.15362410e-01 2.80964668e+06\n",
      " 1.35174851e+04 5.83653431e+04 3.53458544e+01 1.19229154e+11\n",
      " 1.50461763e+06 5.41399855e+09 6.72854348e+01 8.61935688e+01\n",
      " 1.29956297e+01 6.08968096e+05 5.36281868e-02 7.02411669e+05\n",
      " 4.48082644e+03 1.52174851e+04 1.86304499e+01 1.88055916e+11\n",
      " 1.12170444e+06 5.11710215e+09 6.78924167e+01 7.61545888e+01\n",
      " 8.33836250e+00 3.84959606e+05 1.34362625e-02 1.09925840e-04\n",
      " 1.26672188e+03 6.62022699e+00 8.04931109e+00 2.38617126e-01\n",
      " 5.73884165e+05 2.55759019e+06 6.73688101e+01 7.02741996e+01\n",
      " 6.32795090e+00 2.55569744e+05 3.39784230e-03 3.93240281e-05\n",
      " 3.30445808e+02 9.95054783e-01 6.96302744e+00 2.02409693e-01\n",
      " 4.26066661e+05 1.03472547e+06 6.68169296e+01 7.26727482e+01\n",
      " 5.28377887e+00 1.94310255e+05 1.51854093e-03 2.40494518e-05\n",
      " 1.51908600e+02 2.19277418e-01 6.40626812e+00 2.15664823e-01\n",
      " 3.49467254e+05 4.55858568e+05 6.74825911e+01 7.30161501e+01\n",
      " 4.59545961e+00 1.56471819e+05 8.59647806e-04 1.64643458e-05\n",
      " 8.71758713e+01 1.04472147e-01 6.00062757e+00 2.47468373e-01\n",
      " 2.94157038e+05 3.13214384e+05 6.80081535e+01 7.30462508e+01\n",
      " 4.08916946e+00 1.30355175e+05 5.52550677e-04 1.19598553e-05\n",
      " 5.64298404e+01 6.85249837e-02 5.58344111e+00 2.75474462e-01\n",
      " 2.51621017e+05 2.18763055e+05 6.83657094e+01 7.26547630e+01]\n",
      "[-9.13128247e+02 -9.13128247e+02 -9.13128247e+02  1.02579314e+09\n",
      " -9.13128247e+02  1.29711389e-03  9.99158566e+01  2.98098737e+07\n",
      "  1.02579359e+09  6.46004213e+14 -9.63086175e+02  4.04308749e-04\n",
      " -2.93487242e+08  4.27279936e+04 -1.95198700e+03  7.53259792e-04\n",
      " -4.00239947e+08  4.76877328e+04 -1.41965030e+03 -2.40855112e+03\n",
      "  1.02579507e+09  1.06612232e+15 -9.79673604e+02  4.89750931e-05\n",
      " -3.70354445e+08  6.96454488e-01 -4.75375748e+03  1.89581967e-04\n",
      " -3.10189479e+09  5.10370537e+00 -2.91327768e+03 -6.68736156e+03\n",
      "  1.02579745e+09  1.15857416e+15 -9.51022036e+02  1.24137894e-05\n",
      " -6.09532059e+08  1.74345920e-01 -9.20976114e+03  2.64273940e-04\n",
      " -4.31667899e+09  4.86044051e+00 -5.26150485e+03 -1.35202440e+04\n",
      "  1.02580429e+09  1.34617441e+15 -9.07814584e+02  2.25598427e-06\n",
      "  3.73904227e+07  2.94257085e-02 -2.14588646e+04  6.28992360e-04\n",
      "  7.22317892e+08  4.25615386e+00 -1.20634094e+04 -3.26144594e+04\n",
      "  1.02581502e+09  1.64778140e+15 -8.66951526e+02  3.26169733e-07\n",
      "  5.52712964e+08  5.31151573e-03 -4.07696196e+04  1.25965735e-03\n",
      "  1.02380371e+10  2.66261628e+00 -2.27519786e+04 -6.26546467e+04\n",
      "  1.02583541e+09  2.25860576e+15 -8.17406353e+02 -2.99361318e-07\n",
      "  2.28285802e+09  6.59282650e-04 -4.80335468e+04  2.48908352e-03\n",
      " -1.54353603e+11  1.30510956e+00 -4.30872433e+04 -9.03033837e+04\n",
      "  1.02585942e+09  3.47802769e+15 -6.11034616e+02 -3.81272537e-07\n",
      "  2.00619206e+10  1.23228265e-04 -1.93961903e+05  4.43090234e-03\n",
      "  4.47927017e+12  7.92825116e-01 -6.68976449e+04 -2.60248514e+05\n",
      "  1.02590409e+09  4.63625225e+15 -9.38995972e+02 -7.76062165e-07\n",
      "  1.87083224e+10  5.81755097e-05 -2.49553843e+05  6.04961463e-03\n",
      " -8.72169262e+12  7.66126329e-01 -1.11888185e+05 -3.60503031e+05\n",
      "  1.02595640e+09  5.72671909e+15 -8.89189196e+02 -7.60292233e-07\n",
      "  1.47893817e+10  3.64435839e-05 -3.93703217e+04  7.50477732e-03\n",
      " -4.87471531e+13  6.75964735e-01 -1.64156751e+05 -2.02637884e+05\n",
      "  1.02600321e+09  6.74982131e+15 -8.39726854e+02 -7.36897058e-07\n",
      "  2.07520277e+09  2.11812347e-05  3.53452069e+05  8.73172216e-03\n",
      " -7.07178581e+13  6.07014977e-01 -2.10917866e+05  1.43373929e+05] [2.58401685e+15 2.58401685e+15 2.58401685e+15 2.39720879e+16\n",
      " 2.58401685e+15 2.66741959e-03 7.07054076e+15 2.31392754e+16\n",
      " 2.33260791e+16 2.69099061e+30 8.16382247e+14 8.15467384e-04\n",
      " 3.57659073e+30 2.57535534e+13 1.50975363e+15 1.53978792e-03\n",
      " 3.36329528e+30 6.95632674e+13 2.27876841e+15 2.03274672e+15\n",
      " 2.29059600e+16 4.06748065e+30 1.00338022e+14 1.00241987e-04\n",
      " 6.06056800e+29 1.02208834e+03 3.69801127e+14 4.16724492e-04\n",
      " 3.04937485e+30 1.03668314e+04 1.49116046e+15 1.54372603e+15\n",
      " 2.28135156e+16 4.49644089e+30 2.65520372e+13 2.67738911e-05\n",
      " 2.49292608e+29 1.32867017e+02 5.08312352e+14 6.02447745e-04\n",
      " 4.82958870e+30 8.69100996e+03 1.43966786e+15 1.96371356e+15\n",
      " 2.26259901e+16 6.02121791e+30 6.21598729e+12 6.28871382e-06\n",
      " 8.13784880e+28 2.91843700e+00 1.20642894e+15 1.44393444e-03\n",
      " 9.21620098e+30 1.55050790e+03 1.72541056e+15 3.42558977e+15\n",
      " 2.23246838e+16 9.20804838e+30 2.29405755e+12 2.32397403e-06\n",
      " 3.73815308e+28 7.77630814e-02 2.44331627e+15 2.88402683e-03\n",
      " 1.63845073e+31 3.08430444e+02 2.31089650e+15 5.90697504e+15\n",
      " 2.17151892e+16 1.62877687e+31 9.45461569e+11 9.46652344e-07\n",
      " 1.75199515e+28 1.67908853e-03 4.87737670e+15 5.65176498e-03\n",
      " 3.18475073e+31 5.82962536e+01 3.53530343e+15 1.06469019e+16\n",
      " 2.04979389e+16 3.45376487e+31 4.05269181e+11 4.00205143e-07\n",
      " 9.87202998e+27 2.36267552e-04 9.00368654e+15 1.03127214e-02\n",
      " 7.06959294e+31 1.73271940e+01 5.90555817e+15 1.85054514e+16\n",
      " 1.93389334e+16 5.41038567e+31 2.46400468e+11 2.43347350e-07\n",
      " 6.99391688e+27 1.04230436e-04 1.23129567e+16 1.39116781e-02\n",
      " 1.09851932e+32 1.27926774e+01 8.05996310e+15 2.47155215e+16\n",
      " 1.82501094e+16 7.27483833e+31 1.70011881e+11 1.68309484e-07\n",
      " 5.47739957e+27 6.19771041e-05 1.49294226e+16 1.63738387e-02\n",
      " 1.43474109e+32 6.86439535e+00 9.98206578e+15 2.92398748e+16\n",
      " 1.72299334e+16 8.84688109e+31 1.27752441e+11 1.25798394e-07\n",
      " 4.36956999e+27 4.12052194e-05 1.67744114e+16 1.78988284e-02\n",
      " 1.68981214e+32 4.82344351e+00 1.17555001e+16 3.16682830e+16]\n",
      "[ 1.30606260e+02  1.30606260e+02  1.30606260e+02  1.05145944e+09\n",
      "  1.30606260e+02  1.42637513e-04  4.23904502e+01 -9.75562302e+06\n",
      "  1.05145937e+09  7.55152158e+13  1.09411035e+02  7.01844740e-05\n",
      " -1.69066002e+08  1.58197535e+04 -1.42713413e+02  2.14061271e-04\n",
      "  1.51154897e+10  2.87599747e+04  1.74714165e+02 -7.74102829e+01\n",
      "  1.05145962e+09  2.43036489e+14 -1.86633481e+02  2.85446482e-05\n",
      "  1.23081700e+10  6.48052012e-01 -1.75107425e+03  4.91975511e-04\n",
      "  2.66849939e+10  3.38367503e+00 -3.65070401e+02 -1.92951117e+03\n",
      "  1.05146049e+09  5.03848295e+14 -3.66755923e+02  1.36661777e-05\n",
      "  1.42892265e+10  6.52965506e-02 -4.52060387e+03  9.37531676e-04\n",
      "  4.40662009e+10  1.18619955e+00 -1.42072997e+03 -5.57457791e+03\n",
      "  1.05146420e+09  1.24120935e+15 -5.08805434e+02  4.85334144e-06\n",
      "  1.41435616e+10  7.83936388e-03 -2.64127467e+03  2.19216177e-03\n",
      " -1.82780052e+11  7.38293556e-01 -5.27331685e+03 -7.40578608e+03\n",
      "  1.05146552e+09  2.39059323e+15  7.68277620e+01  2.49732332e-06\n",
      " -2.49913470e+10  1.93354843e-03  8.05273059e+03  3.74038944e-03\n",
      " -1.07574883e+12  6.26716589e-01 -6.00832098e+03  1.96758185e+03\n",
      "  1.05146150e+09  4.36633477e+15 -6.85702190e+01  8.69686056e-07\n",
      " -2.83507183e+10  4.50881955e-04 -3.73743029e+04  4.23659191e-03\n",
      " -1.99445801e+12  4.45174613e-01 -2.12735367e+03 -3.94330864e+04\n",
      "  1.05148018e+09  6.71635414e+15 -3.26367258e+02 -1.42886644e-07\n",
      " -3.50489402e+10  6.45829291e-05 -5.88834364e+04  1.31977552e-03\n",
      " -8.57110757e+12  1.56042083e-01 -2.10723022e+04 -7.96293713e+04\n",
      "  1.05149233e+09  7.34322544e+15 -3.09546862e+02 -2.29734567e-07\n",
      " -4.14466031e+10  1.19702640e-05 -9.26915534e+04  5.95635617e-04\n",
      " -1.20342694e+13  1.02546228e-01 -3.32018013e+04 -1.25583808e+05\n",
      "  1.05150963e+09  7.53518185e+15 -2.57172074e+02 -2.05613733e-07\n",
      " -3.74599911e+10  2.69469652e-06 -1.09682039e+05  5.02601474e-04\n",
      " -1.70450081e+13  1.21790753e-01 -5.04448252e+04 -1.59869692e+05\n",
      "  1.05152502e+09  7.67026813e+15 -2.34460001e+02 -2.00103721e-07\n",
      " -4.21745104e+10 -2.57414440e-06 -1.28840805e+05  4.09418352e-04\n",
      " -2.46257880e+13  1.42791832e-01 -6.58201697e+04 -1.94426515e+05] [3.02060863e+14 3.02060863e+14 3.02060863e+14 8.63938445e+15\n",
      " 3.02060863e+14 2.93276295e-04 6.10058217e+14 8.54457228e+15\n",
      " 8.56386559e+15 8.61956228e+28 1.49546647e+14 1.43028604e-04\n",
      " 1.47648112e+29 3.04677211e+12 4.53777046e+14 4.44052350e-04\n",
      " 1.63482306e+29 1.96324721e+13 3.74608341e+14 6.83987266e+14\n",
      " 8.39632618e+15 3.36161003e+29 6.15708588e+13 5.79670151e-05\n",
      " 1.33076799e+29 2.83348030e+01 1.04330059e+15 1.02594808e-03\n",
      " 6.59223959e+29 7.03427479e+02 6.75422567e+14 1.89019667e+15\n",
      " 8.13544114e+15 8.49661969e+29 3.00646979e+13 2.81821240e-05\n",
      " 8.90888620e+28 4.00434928e-01 1.98235794e+15 1.96687787e-03\n",
      " 1.56590432e+30 2.43442269e+01 1.16871244e+15 3.78258962e+15\n",
      " 7.39766044e+15 3.76260160e+30 1.15034608e+13 1.07482939e-05\n",
      " 6.03709723e+28 1.70726167e-02 4.59716994e+15 4.66522839e-03\n",
      " 6.84387740e+30 8.44886004e+00 2.58724049e+15 8.88765897e+15\n",
      " 6.24828606e+15 1.46412907e+31 5.22378987e+12 4.84298701e-06\n",
      " 6.13221527e+28 3.93265005e-03 7.90081462e+15 8.01269762e-03\n",
      " 2.59197208e+31 5.46812859e+00 4.70552142e+15 1.50282450e+16\n",
      " 4.27380120e+15 4.57813551e+31 2.06876400e+12 1.87749152e-06\n",
      " 4.93139419e+28 9.20146042e-04 9.39608717e+15 8.95053614e-03\n",
      " 5.95214787e+31 2.45180121e+00 7.96794536e+15 1.52485653e+16\n",
      " 1.92458436e+15 9.06992764e+31 4.01678853e+11 3.56021682e-07\n",
      " 1.37098505e+28 1.47591305e-04 3.25816513e+15 2.77123750e-03\n",
      " 3.49456758e+31 6.98039723e-01 9.23611026e+15 8.72621836e+15\n",
      " 1.29918266e+15 1.03745155e+32 1.67037173e+11 1.45921037e-07\n",
      " 4.43569650e+27 4.45247638e-05 1.68741259e+15 1.38530132e-03\n",
      " 2.70281527e+31 4.53294647e-01 8.15981739e+15 9.32555359e+15\n",
      " 1.10860560e+15 1.08443600e+32 1.05582436e+11 9.27073778e-08\n",
      " 2.70741696e+27 2.45375749e-05 1.46595996e+15 1.23376818e-03\n",
      " 2.97365458e+31 7.03791994e-01 8.16866817e+15 9.12279312e+15\n",
      " 9.74653906e+14 1.10543731e+32 6.64284267e+10 5.86025781e-08\n",
      " 1.61338120e+27 1.64205439e-05 1.26967988e+15 1.08194412e-03\n",
      " 3.60901019e+31 1.14725784e+00 8.22853294e+15 8.97293817e+15]\n",
      "[ 1.29824693e-03  1.49095844e-03  1.29824693e-03  1.25904933e+03\n",
      "  1.49095844e-03  9.32689192e-04  6.50155646e-05  2.97140616e+01\n",
      "  1.25904859e+03  2.30320550e+03  1.45845065e-03  3.05869217e-04\n",
      " -2.69205072e-04  1.88046636e+07  2.77024966e-03  7.19037382e-04\n",
      " -8.29223421e-04  2.28571407e+07  2.20392987e-03  3.51572888e-03\n",
      "  1.25904662e+03  5.36801020e+03  1.33912510e-03  7.42086349e-05\n",
      "  2.27340365e-03  1.01926423e+00  6.87053201e-03  6.46002894e-04\n",
      "  9.89642238e-03  7.20548110e+00  4.05345159e-03  9.58485851e-03\n",
      "  1.25904318e+03  7.05074181e+03  1.37617908e-03  2.68278281e-05\n",
      "  6.12719146e-04  5.63389915e-01  1.33064374e-02  6.20911623e-04\n",
      " -1.57784823e-02  5.88174713e+00  7.52577158e-03  1.94560299e-02\n",
      "  1.25903328e+03  8.39663585e+03  1.33111727e-03  7.09230964e-06\n",
      " -1.75017523e-03  1.87133339e-01  3.03855805e-02  5.47764066e-04\n",
      " -3.08141505e-02  3.01605471e+00  1.73848452e-02  4.64393085e-02\n",
      "  1.25901809e+03  9.02361347e+03  1.17653665e-03  2.80527310e-06\n",
      " -7.46425323e-04  9.21286897e-03  5.46098656e-02  4.73464779e-04\n",
      " -2.58560052e-02  1.12993771e+00  3.24230549e-02  8.58563838e-02\n",
      "  1.25899078e+03  9.42834259e+03  1.10984958e-03  1.36913368e-06\n",
      " -1.26582802e-03  4.03084968e-03  8.79622157e-02  5.41270697e-04\n",
      " -2.59516214e-01  2.58748263e+00  5.96613006e-02  1.46513667e-01\n",
      "  1.25894680e+03  9.84916606e+03  8.63983366e-04  8.10077224e-07\n",
      " -6.68904663e-03  2.49502092e-03  9.79413669e-02  6.25583001e-04\n",
      " -7.07290706e-01  3.44341448e+00  1.03396542e-01  2.00473926e-01\n",
      "  1.25891817e+03  1.01116178e+04  6.66702297e-04  5.77639517e-07\n",
      " -1.17469730e-02  1.84440551e-03  1.06105558e-01  7.21862672e-04\n",
      " -6.30673045e-01  3.67468669e+00  1.31824729e-01  2.37263585e-01\n",
      "  1.25889783e+03  1.03107899e+04  5.58134994e-04  4.66441252e-07\n",
      " -9.79117879e-03  1.57128372e-03  1.62300762e-01  8.70165596e-04\n",
      " -3.83356226e-01  3.56298764e+00  1.52061377e-01  3.13804005e-01\n",
      "  1.25888155e+03  1.04799277e+04  4.95184544e-04  4.05694582e-07\n",
      " -5.60269954e-03  1.39801822e-03  2.75071412e-01  1.02710199e-03\n",
      " -1.10465877e+00  3.05320020e+00  1.68276002e-01  4.42852229e-01] [9.21282200e+03 9.21282200e+03 9.21282200e+03 1.56108138e+04\n",
      " 9.21282200e+03 3.10298387e-03 2.20464419e+04 1.47293811e+04\n",
      " 1.33075746e+04 5.01490464e+10 3.70121206e+03 8.82287952e-04\n",
      " 6.17227513e+10 1.25270598e+18 9.47969910e+03 2.46283755e-03\n",
      " 8.72563546e+10 2.20652872e+18 9.70562936e+03 1.38601760e+04\n",
      " 1.02426547e+04 1.20452259e+11 7.18271268e+02 1.75695102e-04\n",
      " 3.90512363e+10 5.50389824e+04 6.73094620e+03 2.04764137e-03\n",
      " 2.19968914e+11 1.07688426e+06 1.01093827e+04 1.29894154e+04\n",
      " 8.55975097e+03 1.15476247e+11 1.87326808e+02 5.62179957e-05\n",
      " 1.53217894e+10 3.07425585e+04 4.36022083e+03 1.66870222e-03\n",
      " 2.22840343e+11 2.92524042e+05 9.94425185e+03 1.18854999e+04\n",
      " 7.21328485e+03 6.74130152e+10 3.10765842e+01 1.24477625e-05\n",
      " 3.20124560e+09 3.67480005e+03 2.50784793e+03 1.16149848e-03\n",
      " 1.33010594e+11 3.59297027e+04 9.85693222e+03 1.13259940e+04\n",
      " 6.58524548e+03 3.81974657e+10 7.99741765e+00 3.81951569e-06\n",
      " 8.74747603e+08 2.32929540e+00 1.61886177e+03 8.86170379e-04\n",
      " 5.01795663e+10 3.16284947e+02 9.97591481e+03 1.10567117e+04\n",
      " 6.17871033e+03 2.63338753e+10 2.06350006e+00 1.02506765e-06\n",
      " 2.20081182e+08 5.16618889e-01 1.68276710e+03 9.60847095e-04\n",
      " 4.31930193e+10 4.92209269e+03 1.01180115e+04 1.17889518e+04\n",
      " 5.75433608e+03 1.60230442e+10 5.39493724e-01 2.94050811e-07\n",
      " 5.69958080e+07 2.46340585e-01 1.84507118e+03 1.10593242e-03\n",
      " 2.95338995e+10 3.73454094e+03 1.05375075e+04 1.25001377e+04\n",
      " 5.48890802e+03 1.13788471e+10 2.48056198e-01 1.39264067e-07\n",
      " 2.55866424e+07 1.77122546e-01 2.09497464e+03 1.29060826e-03\n",
      " 2.10795172e+10 2.40985743e+03 1.08106239e+04 1.33554677e+04\n",
      " 5.28743274e+03 8.81246891e+09 1.45016663e-01 8.31783865e-08\n",
      " 1.44843343e+07 1.53964448e-01 2.42053156e+03 1.50135015e-03\n",
      " 1.62627147e+10 1.55831993e+03 1.10616056e+04 1.40392769e+04\n",
      " 5.11652118e+03 7.25417445e+09 9.60845566e-02 5.56108269e-08\n",
      " 9.27996947e+06 1.38500150e-01 2.65166204e+03 1.63682050e-03\n",
      " 1.34945117e+10 9.21278930e+02 1.12963420e+04 1.42777437e+04]\n",
      "[-1.40671965e-04 -1.40671965e-04 -1.40671965e-04 -7.79903828e-02\n",
      " -1.40671965e-04  1.76948801e-03 -2.81343931e-05 -2.30989145e-01\n",
      " -7.79200469e-02  5.02139878e-03 -1.26604769e-04 -7.31790720e-03\n",
      " -2.54025018e-20  1.35606361e+05 -2.39142341e-04 -3.20964816e-02\n",
      "  1.02501323e-20  1.72425404e+05 -1.96940751e-04 -3.09478324e-04\n",
      " -7.77399867e-02  9.81017561e-03 -1.20977890e-04  1.00859523e-01\n",
      " -2.46486824e-07  3.99329886e+03 -5.48620665e-04  6.01153851e-01\n",
      "  3.69730237e-07  1.64044667e+04 -3.71373988e-04 -7.99016763e-04\n",
      " -7.74656764e-02  1.59582811e-02 -1.09724133e-04  2.34968358e-03\n",
      " -1.23243412e-07  1.38697986e+02 -8.45438512e-04  2.41218715e-01\n",
      " -1.13537993e-05  1.16519044e+03 -6.34430564e-04 -1.37014494e-03\n",
      " -7.68897654e-02  4.55658403e-02 -7.14613584e-05 -3.73674327e-02\n",
      " -1.02587816e-05  8.85587490e-03 -4.36645780e-04 -1.83434717e+00\n",
      " -4.53846330e-04  1.64089437e+00 -1.17207882e-03 -1.53726324e-03\n",
      " -7.66714425e-02  1.27221838e-01 -1.15351012e-05 -6.49155653e-03\n",
      " -9.08605894e-05  2.71536002e-03  2.94088811e-03 -7.81808713e-01\n",
      " -2.37393494e-03  2.63804387e+00 -1.33047545e-03  1.62194776e-03\n",
      " -7.81418865e-02  3.43527626e-01  2.64463295e-05 -6.67713960e-03\n",
      " -1.03104206e-04  7.32159048e-04 -1.30276307e-03 -2.68682030e+00\n",
      " -7.35484286e-03  2.07355801e+00  1.77950036e-04 -1.15125936e-03\n",
      " -7.74905050e-02  6.34749372e-01 -2.07491149e-05  2.11257877e-02\n",
      " -1.01288561e-04  8.65907339e-05 -1.93416215e-02  3.94147321e+00\n",
      " -1.40103893e-02  3.02574503e-01 -5.20626944e-04 -1.98414994e-02\n",
      " -7.33472233e-02  7.25152209e-01 -7.80729408e-05  1.09451408e-03\n",
      " -1.26561998e-04 -1.15486756e-04 -3.34338342e-02 -6.66824074e-01\n",
      " -3.01521474e-02  2.45301053e-02 -4.72123250e-03 -3.80769938e-02\n",
      " -6.78196942e-02  7.16537069e-01 -7.14613584e-05 -2.55039910e-02\n",
      " -1.18799255e-04 -1.00706466e-04 -4.67212392e-02 -5.20017751e-01\n",
      " -4.09205661e-02  4.92267529e-02 -1.02421500e-02 -5.68919278e-02\n",
      " -6.28891067e-02  7.43649492e-01 -8.24900405e-05  2.70433882e-03\n",
      " -1.08422159e-04 -9.73751811e-05 -6.11767185e-02 -6.68058428e-01\n",
      " -5.08424043e-02  4.62087866e-02 -1.51837662e-02 -7.62779946e-02] [2.00855753e-02 2.00855753e-02 2.00855753e-02 8.34972929e-01\n",
      " 2.00855753e-02 7.58942899e-01 5.65718065e-02 8.09238929e-01\n",
      " 8.30110462e-01 1.55985829e-04 5.94262725e-03 9.21521206e-01\n",
      " 1.94193466e-04 3.65090180e+11 1.42037461e-02 2.17103484e+00\n",
      " 3.05744141e-04 7.92136134e+11 1.69066466e-02 2.15405419e-02\n",
      " 8.25715871e-01 1.72810525e-04 1.67073468e-03 7.66324263e+01\n",
      " 7.76525472e-05 1.60165731e+09 2.45928603e-02 1.04702449e+03\n",
      " 3.05889458e-04 2.03210498e+10 2.01610054e-02 5.10249688e-02\n",
      " 8.20169072e-01 2.46022179e-04 9.52690348e-04 2.26116446e+01\n",
      " 3.83777928e-05 3.29048523e+07 7.14167320e-02 1.59233250e+03\n",
      " 3.49899856e-04 6.72052664e+08 3.47948330e-02 1.51707928e-01\n",
      " 7.91766093e-01 2.07283759e-03 6.15243261e-04 7.62553600e+00\n",
      " 3.22592339e-05 2.05511992e-02 3.25716106e-01 3.20062466e+03\n",
      " 3.73091109e-03 3.05185098e+01 1.16225291e-01 6.77494916e-01\n",
      " 7.10781608e-01 1.86134176e-02 4.49140147e-04 1.81632991e+00\n",
      " 7.38040389e-05 5.94248400e-03 8.60466636e-01 2.92512115e+03\n",
      " 4.70978637e-02 1.01636315e+03 3.30111806e-01 1.60277462e+00\n",
      " 4.97940399e-01 1.09731607e-01 2.12770117e-04 6.93433092e-01\n",
      " 1.83952924e-04 1.65591100e-03 1.15017560e+00 2.87240977e+03\n",
      " 2.01856478e-01 2.61923578e+01 7.61317462e-01 1.79909738e+00\n",
      " 2.11163266e-01 1.37010109e-01 5.12856594e-05 9.82267401e+00\n",
      " 7.00257274e-05 3.34335049e-04 2.98755912e-01 8.39385105e+04\n",
      " 2.03550742e-01 1.11583470e+00 9.92232444e-01 5.97723818e-01\n",
      " 1.28875705e-01 1.03432140e-01 3.53453379e-06 5.94268893e-02\n",
      " 6.04634251e-06 1.03477729e-05 1.07597085e-01 1.32322762e+03\n",
      " 7.85003910e-02 1.28078600e-01 6.81735875e-01 8.47820364e-01\n",
      " 1.45849502e-01 1.01063674e-01 1.44833118e-05 5.16265980e+00\n",
      " 7.89562351e-06 1.40160841e-05 2.21270405e-01 1.11759066e+03\n",
      " 1.12631565e-01 2.36847091e-01 7.20886308e-01 1.13553002e+00\n",
      " 1.24090181e-01 1.03997209e-01 8.32202216e-06 1.45245433e-01\n",
      " 8.51816410e-06 1.44244374e-05 1.81301130e-01 2.03453546e+03\n",
      " 1.27347132e-01 2.78319058e-01 8.67947403e-01 8.58287532e-01]\n",
      "[ 5.87883055e-06  7.73746060e-05  5.87883055e-06  1.25064094e-01\n",
      "  7.73746060e-05  1.12094828e+00  1.50619721e-07 -1.72647208e-01\n",
      "  1.25025407e-01  5.54095495e-04  7.72992961e-05  1.83738037e-03\n",
      " -8.99205074e-10  1.98290687e+04  1.53752925e-04  2.62351143e-03\n",
      " -5.14912112e-09  2.01229469e+04  1.15986599e-04  1.92440228e-04\n",
      "  1.24910719e-01  2.93030940e-03  7.55960381e-05  3.49025266e-03\n",
      " -3.36615458e-08  1.26656443e+04  3.66459665e-04  1.24431410e-03\n",
      " -1.72415652e-07  6.55807488e+04  2.28971473e-04  5.19835099e-04\n",
      "  1.24727489e-01  9.86301343e-03  7.24192171e-05 -1.60614530e-01\n",
      " -1.55704236e-07  7.12445183e+03  6.46285062e-04 -1.52508739e+00\n",
      " -2.21061113e-06  1.43689009e+05  4.09024484e-04  9.82890329e-04\n",
      "  1.24271135e-01  5.05451462e-02  5.66872383e-05  3.18023214e-03\n",
      " -1.60004442e-06  3.03589471e+03  6.15478371e-04  7.15162051e-02\n",
      " -1.00849155e-04  8.12326786e+02  8.49645973e-04  1.40843711e-03\n",
      "  1.23963396e-01  1.59049116e-01  1.52897217e-05 -3.74146837e-03\n",
      " -1.85538125e-05  2.88221155e-03 -6.94459863e-03 -4.03829442e-01\n",
      " -2.64491063e-03  4.11523635e+00  1.11598764e-03 -5.84390071e-03\n",
      "  1.27435695e-01  4.19844736e-01 -1.51405799e-04 -4.81617263e-01\n",
      " -1.51475600e-04  6.84442101e-04 -3.10351853e-02 -2.76843764e+01\n",
      " -4.82126650e-03  1.20722469e+00 -2.52300720e-03 -3.34067867e-02\n",
      "  1.42953288e-01  8.89781492e-01 -2.49756429e-04  6.40035379e-03\n",
      " -1.24686800e-04 -1.07253238e-04 -2.62226561e-02 -7.51808323e-01\n",
      " -2.03252182e-02  1.08949503e-01 -1.81389505e-02 -4.41118502e-02\n",
      "  1.52412510e-01  1.00734197e+00 -1.32995310e-04 -1.43092845e-01\n",
      " -1.50477930e-04 -1.11030030e-04 -2.42718815e-02 -6.62281305e+00\n",
      " -1.92076527e-02  2.43877578e-02 -2.74814113e-02 -5.16202976e-02\n",
      "  1.56064616e-01  9.86205621e-01 -1.26654594e-04 -1.08067695e-03\n",
      " -1.19440024e-04 -1.05852194e-04 -1.99693900e-02 -1.89888914e+00\n",
      " -1.00725976e-02  3.62500842e-02 -3.11271767e-02 -5.09699121e-02\n",
      "  1.61821485e-01  1.02066082e+00 -1.28407279e-04 -5.60990203e-03\n",
      " -9.37096474e-05 -9.27410633e-05 -9.76403678e-03 -2.26697199e+00\n",
      " -6.71499154e-03  4.58529002e-02 -3.68857987e-02 -4.65214282e-02] [1.49533136e-03 2.21637599e-03 1.49533136e-03 1.06238968e+00\n",
      " 2.21637599e-03 1.20638583e+04 2.47843679e-03 1.04218856e+00\n",
      " 1.06186895e+00 4.18797362e-04 1.59676861e-03 9.71255718e+00\n",
      " 8.37685181e-04 3.89087001e+12 5.75025106e-03 3.05021381e+01\n",
      " 8.37640977e-04 3.89115057e+12 3.74763031e-03 8.86103252e-03\n",
      " 1.05958866e+00 1.14892856e-03 1.20214117e-03 4.49543447e+00\n",
      " 4.40852727e-04 1.59369761e+12 2.77303370e-02 1.01147573e+02\n",
      " 2.29097875e-03 1.06783498e+13 1.13584824e-02 5.34920790e-02\n",
      " 1.05280211e+00 2.48884221e-03 1.04213007e-03 4.51653770e+02\n",
      " 2.40589357e-04 5.04255902e+11 9.67858354e-02 7.26113119e+04\n",
      " 4.89135308e-03 2.27701655e+13 3.29085039e-02 1.98101658e-01\n",
      " 1.01244621e+00 9.75025757e-03 8.21161678e-04 3.56145302e+00\n",
      " 1.39819940e-04 9.17968606e+10 4.33813802e-01 1.10140578e+03\n",
      " 1.92921925e-02 6.47608343e+09 1.49586965e-01 8.62380418e-01\n",
      " 9.04176191e-01 3.82776409e-02 5.36035097e-04 1.54776612e+00\n",
      " 1.63457568e-04 7.11153972e-03 1.03784443e+00 1.03765502e+03\n",
      " 8.57190994e-02 2.81673522e+02 4.14049706e-01 2.01164757e+00\n",
      " 6.47229366e-01 1.27678284e-01 3.07222458e-04 2.24039247e+03\n",
      " 2.28771126e-04 1.76339567e-03 1.86914131e+00 6.55915130e+06\n",
      " 2.70062762e-01 6.93806326e+00 9.70568262e-01 2.92151902e+00\n",
      " 1.94284387e-01 1.26705354e-01 6.06512648e-05 1.58605206e+00\n",
      " 8.62935789e-05 1.30282245e-04 3.44358452e-01 1.73126413e+03\n",
      " 1.91790923e-01 3.03277844e-01 1.47548120e+00 6.39080050e-01\n",
      " 8.99563570e-02 1.04734749e-01 6.07169791e-06 2.12370299e+02\n",
      " 1.15763264e-05 1.15769209e-05 8.03000036e-02 3.56475812e+05\n",
      " 9.68324769e-02 1.08292811e-01 9.15048764e-01 1.04646594e+00\n",
      " 1.18645254e-01 9.73329217e-02 2.18324663e-05 4.02908369e-01\n",
      " 6.91262315e-06 7.23640495e-06 2.39186048e-01 5.05309175e+03\n",
      " 1.09677695e-01 1.15635899e-01 9.73798583e-01 1.38176732e+00\n",
      " 8.71955121e-02 9.79892207e-02 7.80583976e-06 3.07802223e-01\n",
      " 1.02309084e-05 9.48096439e-06 1.25443109e-01 1.96276112e+04\n",
      " 1.28300987e-01 1.29099515e-01 1.16930848e+00 1.04067642e+00]\n",
      "[ 6.60121154e-05  1.88408984e-04  6.60121154e-05 -4.71193629e-02\n",
      "  1.88408984e-04  7.84166935e-01  3.79778006e-05  3.91762265e-01\n",
      " -4.72135673e-02  2.27880161e-02  1.69420084e-04 -3.66833226e-01\n",
      "  1.90137111e-06  1.33947762e+03  3.53185759e-04 -4.24384691e-01\n",
      "  2.60255477e-06  1.00962530e+03  2.63624575e-04  4.47390251e-04\n",
      " -4.74781931e-02  4.03933001e-02  1.81274043e-04  1.37564182e-02\n",
      "  1.52312112e-06  2.88239781e-01  9.09193944e-04  3.81303469e-01\n",
      "  8.12979098e-06  1.13015339e+00  5.40104278e-04  1.26802418e-03\n",
      " -4.79327901e-02  5.63398627e-02  1.82526386e-04  6.44130486e-02\n",
      "  1.56494815e-06  4.59191594e-02  1.84885758e-03  4.91173755e-01\n",
      "  2.99302866e-05  4.49052658e-01  9.95953593e-04  2.66228479e-03\n",
      " -4.93081109e-02  1.07062173e-01  1.84197449e-04 -2.86362038e-02\n",
      "  3.09427657e-06  8.02536969e-03  5.28338681e-03 -5.44788050e-01\n",
      "  1.66669429e-04  5.62616189e-01  2.37294554e-03  7.47213490e-03\n",
      " -5.19498043e-02  1.99120540e-01  2.26866104e-04  6.54321181e-02\n",
      "  1.35140096e-05  2.74277708e-03  1.52574775e-02  5.12959937e-01\n",
      "  1.79552731e-04  1.08165378e+00  5.05730760e-03  2.00879190e-02\n",
      " -5.95785431e-02  3.78560093e-01  3.10937050e-04  1.02285488e-02\n",
      " -1.41313982e-07  6.98624345e-04  3.24606560e-02 -1.92669759e+00\n",
      "  5.61337874e-03  1.11027587e+00  1.27701173e-02  4.49198362e-02\n",
      " -7.58088711e-02  5.80287398e-01  2.21544325e-04 -3.93793773e-02\n",
      "  1.45568319e-04  3.40523066e-04  3.02604035e-02 -1.72512167e+00\n",
      "  3.05901490e-02  4.44346288e-01  2.89110526e-02  5.89499117e-02\n",
      " -8.18399118e-02  6.31857540e-01  1.81765961e-04  1.01262283e-02\n",
      "  1.33077148e-04  2.28015564e-04  5.52980807e-02  1.35928660e+00\n",
      "  4.02836928e-02  1.89250219e-01  3.49023149e-02  9.00186296e-02\n",
      " -9.09390728e-02  6.24410553e-01  2.32375002e-04 -9.17995204e-03\n",
      "  1.06701616e-04  1.85027373e-04  7.20454055e-02  4.78966290e-01\n",
      "  5.00804520e-02  2.64556282e-01  4.40520850e-02  1.15865115e-01\n",
      " -1.01984072e-01  6.37526233e-01  1.87625608e-04 -1.07319467e-03\n",
      "  1.52211590e-04  2.11910297e-04  7.89630727e-02 -7.53935888e-01\n",
      "  5.86036402e-02  2.54059261e-01  5.50523351e-02  1.33827782e-01] [9.11520289e-02 9.11520289e-02 9.11520289e-02 8.92331266e-01\n",
      " 9.11520289e-02 5.12769251e+03 2.67604818e-01 7.97985733e-01\n",
      " 8.69472806e-01 3.23702072e-03 2.42470282e-02 8.74663812e+02\n",
      " 3.70327550e-03 1.93265353e+09 5.44260837e-02 8.01117827e+02\n",
      " 5.19356944e-03 8.38209321e+08 7.12839617e-02 8.31450500e-02\n",
      " 8.51712379e-01 3.67035256e-03 5.74360726e-03 1.35541008e+01\n",
      " 1.04997979e-03 2.65956139e+00 6.38016833e-02 2.92084696e+02\n",
      " 4.15418541e-03 2.32502896e+01 7.38400316e-02 1.37717622e-01\n",
      " 8.35544713e-01 5.01961350e-03 2.37595951e-03 9.55471526e+00\n",
      " 4.60349940e-04 1.26649587e-01 1.31324142e-01 4.98286263e+02\n",
      " 4.93915787e-03 2.57174846e+00 9.92297886e-02 2.85444319e-01\n",
      " 7.84227950e-01 2.79075250e-02 9.43061857e-04 1.07307038e+01\n",
      " 4.73817364e-04 1.69879546e-02 3.68538893e-01 2.35209433e+03\n",
      " 4.74477978e-02 2.51001080e+00 2.10202260e-01 7.43174026e-01\n",
      " 6.90745480e-01 9.10605472e-02 4.56768889e-04 1.77439333e+01\n",
      " 4.49791808e-04 5.19578397e-03 7.17884526e-01 1.75596824e+04\n",
      " 1.79077848e-01 8.65697567e+00 3.95615884e-01 1.36547859e+00\n",
      " 5.06819168e-01 1.83385158e-01 1.75760617e-04 8.76293778e-01\n",
      " 2.51116426e-04 1.38282478e-03 8.17082283e-01 8.50073323e+03\n",
      " 3.75168638e-01 7.33727452e+00 7.07227707e-01 1.37894173e+00\n",
      " 2.94004660e-01 1.71487236e-01 4.37467022e-05 4.86580507e+00\n",
      " 7.07013493e-05 2.96729702e-04 2.36757227e-01 3.76755016e+04\n",
      " 2.02327150e-01 2.15396371e+00 8.38917241e-01 6.80269568e-01\n",
      " 2.33661979e-01 1.23995913e-01 6.58688693e-06 4.39576671e-01\n",
      " 1.19383872e-05 2.96346795e-05 1.31022319e-01 6.42555104e+03\n",
      " 7.76639285e-02 3.65325033e-01 6.45658769e-01 8.45743835e-01\n",
      " 2.34580513e-01 1.20700917e-01 1.15379572e-05 5.31970575e-01\n",
      " 1.26015314e-05 2.87143603e-05 2.13441145e-01 1.34458376e+04\n",
      " 1.18913224e-01 5.83753504e-01 7.00681180e-01 1.02703255e+00\n",
      " 2.13152630e-01 1.12621010e-01 6.84193345e-06 1.80467549e-02\n",
      " 9.17095908e-06 2.23374536e-05 1.99451264e-01 3.26525441e+02\n",
      " 9.92328027e-02 5.24600494e-01 7.82391515e-01 9.18445177e-01]\n",
      "[ 3.64662179e-05  1.98387497e-04  3.64662179e-05 -8.00056104e-02\n",
      "  1.98387497e-04  2.94076337e-01  4.39974266e-05  2.01907411e-01\n",
      " -8.01048041e-02  3.95415435e-02  1.76388784e-04  8.74169242e-02\n",
      "  2.21091382e-06  1.21622550e+03  3.30239035e-04  4.23240336e-02\n",
      "  2.18882225e-06  7.68859904e+02  2.75582532e-04  4.29432783e-04\n",
      " -8.03445748e-02  6.78045641e-02  1.65813082e-04 -4.96759882e-02\n",
      "  2.75517371e-06  3.03700614e-01  8.06896614e-04 -2.07040959e-02\n",
      "  1.44462852e-05  1.24709647e+00  5.04777468e-04  1.14586100e-03\n",
      " -8.07480231e-02  8.64236483e-02  1.53438887e-04 -1.55625814e-02\n",
      "  5.96952190e-06  5.54290007e-02  1.21233125e-03  6.80225858e-01\n",
      "  5.93345684e-05  5.50766860e-01  8.95851580e-04  1.95474395e-03\n",
      " -8.16024013e-02  1.33475225e-01  1.10978346e-04  4.58561199e-02\n",
      "  8.30793356e-06  1.08482156e-02  2.11768862e-03 -1.42980228e+00\n",
      "  1.29709628e-04  5.80275210e-01  1.70776932e-03  3.71447959e-03\n",
      " -8.26612457e-02  2.32098317e-01  9.67036600e-05  6.48996265e-02\n",
      "  7.13822183e-06  3.19330306e-03  7.43422194e-03  6.89088563e-01\n",
      " -2.38062579e-03  1.01542161e+00  2.75233894e-03  1.00898572e-02\n",
      " -8.63783566e-02  4.48602448e-01  1.91719762e-04  1.62651353e-02\n",
      " -1.08473977e-04  9.40756227e-04  2.46505414e-02 -2.65279815e+00\n",
      " -4.25687086e-03  2.81287746e+00  6.56446601e-03  3.10232877e-02\n",
      " -9.87036273e-02  6.71622695e-01  1.42913377e-04 -4.54037156e-03\n",
      " -9.03338819e-07  3.66031579e-04  1.46734018e-02 -8.26444762e-01\n",
      "  1.41555964e-02  6.83780132e-01  1.88409303e-02  3.33714188e-02\n",
      " -1.01361413e-01  7.29380090e-01  1.06449203e-04  1.40210626e-02\n",
      "  3.29698636e-05  5.63015504e-05  2.97327789e-02 -3.03740410e-01\n",
      "  1.26252692e-02  4.30835962e-02  2.14622515e-02  5.10885812e-02\n",
      " -1.06040328e-01  7.12342892e-01  1.35848536e-04  6.63396759e-03\n",
      "  8.72115670e-07  1.63291294e-05  3.83381249e-02 -8.10504786e-01\n",
      "  1.28144673e-02  7.62168786e-02  2.61705664e-02  6.43728427e-02\n",
      " -1.12789327e-01  7.24000635e-01  1.03518058e-04  8.18746254e-03\n",
      "  4.00241742e-05  6.18075014e-05  3.93032863e-02  1.54547183e+00\n",
      "  1.75037179e-02  5.94670041e-02  3.28872351e-02  7.20870033e-02] [1.58166135e-01 1.58166135e-01 1.58166135e-01 7.83902911e-01\n",
      " 1.58166135e-01 1.92576090e+03 4.64408555e-01 7.59337811e-01\n",
      " 7.44223203e-01 1.98359775e-02 4.20595820e-02 3.70739834e+01\n",
      " 2.06674704e-02 1.45500478e+09 8.98383943e-02 7.89732971e+01\n",
      " 3.38776374e-02 2.68968725e+08 1.23662905e-01 1.35104058e-01\n",
      " 7.15664158e-01 2.38029344e-02 8.44437106e-03 5.98172672e+01\n",
      " 7.35019711e-03 4.44369544e+00 7.45045779e-02 1.03730213e+03\n",
      " 3.30910196e-02 5.15334699e+01 1.15054170e-01 1.70620271e-01\n",
      " 6.96540031e-01 2.16949670e-02 2.86831143e-03 8.33684971e+01\n",
      " 2.52823071e-03 5.33384169e-01 1.21828460e-01 3.39634998e+03\n",
      " 2.72224592e-02 1.21746356e+01 1.31787840e-01 2.94891282e-01\n",
      " 6.48464442e-01 2.52275822e-02 1.02201820e-03 1.57830006e+01\n",
      " 6.40939970e-04 1.18275046e-01 3.94747305e-01 1.33896290e+04\n",
      " 2.85963892e-02 3.71802565e+00 2.32283721e-01 8.63953100e-01\n",
      " 5.48738013e-01 6.11194151e-02 5.54814101e-04 3.10413852e+01\n",
      " 3.88385666e-04 9.66647306e-03 8.61200003e-01 1.42786956e+04\n",
      " 1.10358082e-01 7.15362128e+00 4.64400020e-01 1.61753289e+00\n",
      " 3.32395138e-01 1.50561565e-01 2.03775978e-04 3.56319027e+00\n",
      " 2.93324697e-04 4.46994088e-03 8.82959598e-01 1.89578966e+03\n",
      " 3.77651515e-01 3.43102353e+01 8.32062878e-01 1.47001973e+00\n",
      " 1.12809176e-01 8.76921181e-02 5.09760490e-05 2.35169944e+00\n",
      " 9.59516453e-05 7.44254760e-04 1.90976672e-01 4.24338037e+03\n",
      " 2.19032938e-01 3.81161401e+00 9.30359272e-01 6.31626548e-01\n",
      " 5.17229482e-02 1.65973282e-02 4.62609002e-06 1.53622846e+00\n",
      " 1.53706586e-05 2.64817552e-05 2.60235831e-02 3.44362659e+03\n",
      " 2.81845846e-02 5.61081266e-02 6.84335883e-01 7.33001204e-01\n",
      " 6.46174511e-02 2.38821435e-02 1.22863911e-05 1.99990269e-01\n",
      " 1.62037455e-05 3.01819264e-05 9.50983488e-02 1.86833333e+03\n",
      " 5.77226947e-02 1.31571129e-01 7.33067947e-01 9.19509574e-01\n",
      " 5.11414368e-02 1.65101562e-02 7.60885283e-06 2.91101869e-01\n",
      " 1.20118989e-05 2.81843601e-05 5.44243141e-02 1.44290738e+04\n",
      " 3.09064996e-02 7.94099833e-02 8.19632154e-01 7.29201833e-01]\n",
      "[-2.02960637e-06 -3.16994879e-06 -2.02960637e-06  1.02468753e-01\n",
      " -3.16994879e-06  4.75338513e-03  8.38300522e-07 -1.04293519e-02\n",
      "  1.02470338e-01  7.24823474e-06 -3.58909906e-06  1.73579855e-03\n",
      "  6.38198176e-10  3.33751478e+01 -7.37369606e-06  4.25122998e-03\n",
      "  2.96649949e-10  4.39858535e+01 -5.17407345e-06 -8.95867045e-06\n",
      "  1.02475921e-01  1.89651959e-05 -3.86481197e-06  5.76717265e-04\n",
      " -1.16897849e-10  1.35952588e+00 -1.99004081e-05  9.53649327e-03\n",
      " -4.18177677e-11  6.14760260e+00 -1.10329811e-05 -2.70685773e-05\n",
      "  1.02485872e-01  4.23589082e-05 -3.80782019e-06  3.38312253e-04\n",
      "  2.54271075e-10  1.17983840e-01 -3.31832399e-05  2.89527979e-02\n",
      " -1.04196318e-08  1.79454695e+00 -2.09261934e-05 -5.03016131e-05\n",
      "  1.02508048e-01  1.61108442e-04 -2.47645836e-06  2.33178705e-04\n",
      " -1.56465957e-08  1.73606285e-02 -2.63651512e-07  1.34523409e-01\n",
      " -1.09986075e-06  3.19216879e+00 -4.17711023e-05 -3.95582954e-05\n",
      "  1.02508180e-01  4.80301819e-04  1.00079622e-07  1.71231591e-04\n",
      " -1.57472292e-07  3.50930100e-03  1.08097909e-04  2.95658482e-01\n",
      " -1.95333562e-06  1.89566545e+00 -3.93263901e-05  6.86714397e-05\n",
      "  1.02454131e-01  1.25607018e-03  7.53505891e-07  5.90113662e-05\n",
      " -1.12339259e-07  6.68480448e-04 -9.75501846e-05  2.08898829e-01\n",
      " -3.53578351e-06  1.83299373e+00  1.53759909e-05 -8.29276996e-05\n",
      "  1.02502906e-01  2.08795866e-03 -8.38453788e-07 -2.54138903e-07\n",
      " -8.46796687e-08  1.50261106e-04 -7.31272665e-04  2.87273183e-02\n",
      " -1.34373543e-05  3.36617441e-01 -3.49910611e-05 -7.65425273e-04\n",
      "  1.02651017e-01  2.33801840e-03 -3.07712767e-06 -2.60051390e-05\n",
      " -1.37920622e-07 -4.45435384e-05 -1.29941046e-03 -3.33028744e-03\n",
      " -5.47213587e-05  1.36219240e-02 -1.85340613e-04 -1.48167395e-03\n",
      "  1.02868542e-01  2.29758264e-03 -2.71746684e-06 -2.45876813e-05\n",
      " -1.80248786e-07 -4.93128043e-05 -1.77755122e-03  5.85269859e-03\n",
      " -8.06523360e-05  4.06813702e-02 -4.02506407e-04 -2.17734016e-03\n",
      "  1.03061530e-01  2.35964821e-03 -3.18225542e-06 -2.82749001e-05\n",
      " -1.62810064e-07 -4.73845050e-05 -2.42038673e-03 -4.79223976e-03\n",
      " -1.07958363e-04  2.28087843e-02 -5.95958799e-04 -3.01316327e-03] [2.87907272e-05 2.89929289e-05 2.87907272e-05 2.56512066e-03\n",
      " 2.89929289e-05 1.10263564e-02 7.10071148e-05 2.50082367e-03\n",
      " 2.55807114e-03 8.88490042e-10 1.12398708e-05 3.72428711e-03\n",
      " 1.00863313e-09 7.71392456e+04 3.16209726e-05 9.23525623e-03\n",
      " 1.73373867e-09 9.78379717e+04 2.97286135e-05 4.80126132e-05\n",
      " 2.54703764e-03 1.39272928e-09 4.92886159e-06 1.21748851e-03\n",
      " 5.61989239e-10 6.74698450e+02 9.35743698e-05 2.06825816e-02\n",
      " 2.60915272e-09 3.25145202e+03 5.20145339e-05 1.81536076e-04\n",
      " 2.52488081e-03 3.23096028e-09 3.36531776e-06 7.29099568e-04\n",
      " 3.95515372e-10 2.51026108e+00 2.86201340e-04 6.55903121e-02\n",
      " 4.06135821e-09 1.16914308e+02 1.14713047e-04 5.89648190e-04\n",
      " 2.40886934e-03 3.04552240e-08 2.37394158e-06 4.96710565e-04\n",
      " 3.99411561e-10 1.16782736e-01 1.27457379e-03 3.42061952e-01\n",
      " 6.17762419e-08 5.15022325e+01 4.43003396e-04 2.60921969e-03\n",
      " 2.09078399e-03 2.56133812e-07 1.69281300e-06 3.39438121e-04\n",
      " 1.06705511e-09 7.56336098e-03 3.09915510e-03 8.68886207e-01\n",
      " 6.44224320e-07 1.88961032e+01 1.26228486e-03 5.57474704e-03\n",
      " 1.31873222e-03 1.23138005e-06 6.81829188e-07 1.00328470e-04\n",
      " 2.28178577e-09 1.40559957e-03 3.32047281e-03 5.12690747e-01\n",
      " 2.46800608e-06 2.06852996e+01 2.66955619e-03 5.15299194e-03\n",
      " 4.88190333e-04 1.00504181e-06 1.70686661e-07 1.52361638e-05\n",
      " 6.20315558e-10 3.48787874e-04 8.11086454e-04 6.70834374e-02\n",
      " 1.87933201e-06 1.33245682e+00 3.08117329e-03 2.03986790e-03\n",
      " 2.46992713e-04 4.71294875e-07 9.19683625e-09 8.58960366e-07\n",
      " 4.65989638e-11 8.41952150e-06 1.92524005e-04 1.72796584e-02\n",
      " 3.72128741e-07 7.52831292e-02 2.20431080e-03 2.52315904e-03\n",
      " 3.00836274e-04 5.04685104e-07 4.26025147e-08 4.18571824e-06\n",
      " 9.18090986e-11 1.64930749e-05 4.88857955e-04 4.64921913e-02\n",
      " 7.45441414e-07 1.64517818e-01 2.35758414e-03 3.29798247e-03\n",
      " 2.47866019e-04 4.81046288e-07 2.75633195e-08 2.54984391e-06\n",
      " 7.66494218e-11 1.41322481e-05 3.77899270e-04 3.51828959e-02\n",
      " 6.81008208e-07 1.51817081e-01 2.71291968e-03 2.55476746e-03]\n",
      "[-2.88282155e-06 -2.94779446e-06 -2.88282155e-06  8.71741410e-02\n",
      " -2.94779446e-06  2.56953364e-03 -7.71872284e-07 -6.66310947e-03\n",
      "  8.71756149e-02  4.79009037e-06 -2.56185832e-06  9.12870573e-04\n",
      "  1.13820869e-09  2.23055069e+01 -4.27726160e-06  2.43954145e-03\n",
      "  9.25711790e-10  2.83384418e+01 -4.03575554e-06 -5.75115882e-06\n",
      "  8.71789444e-02  1.10245186e-05 -2.37458852e-06  3.85452676e-04\n",
      "  1.61757936e-09  4.78311635e-01 -1.13576981e-05  7.61401947e-03\n",
      "  3.59925381e-09  1.87023224e+00 -7.17797027e-06 -1.61610799e-05\n",
      "  8.71846232e-02  2.46911919e-05 -2.22859575e-06  2.84972887e-04\n",
      "  3.86331532e-10  7.77376195e-02 -1.74648969e-05  2.64883901e-02\n",
      " -1.77403498e-08  1.37406913e+00 -1.27108266e-05 -2.79471277e-05\n",
      "  8.71956381e-02  1.04658787e-04 -1.07015909e-06  2.23844177e-04\n",
      " -2.08927100e-08  7.84560676e-03  2.72186413e-05  1.30812097e-01\n",
      " -8.43799269e-07  1.87289467e+00 -2.25672410e-05  5.72155934e-06\n",
      "  8.71820288e-02  3.34685340e-04  9.24071607e-07  1.83259572e-04\n",
      " -1.17118893e-07  2.84633932e-03  1.01125807e-04  3.34153171e-01\n",
      " -7.35202281e-07  2.89892646e+00 -6.96368968e-06  9.32380457e-05\n",
      "  8.71314659e-02  9.26995080e-04  9.59181536e-07  8.06043087e-05\n",
      " -4.53854651e-08  9.65967349e-04 -5.63125738e-05  2.95917350e-01\n",
      " -9.39687066e-07  3.27455710e+00  4.36343238e-05 -1.36374316e-05\n",
      "  8.71596222e-02  1.70342271e-03 -3.13889993e-07  4.85418711e-06\n",
      " -3.85758945e-08  1.40754055e-04 -4.72243873e-04  3.61268969e-02\n",
      " -7.07302218e-06  2.63871749e-01  1.42049653e-05 -4.57725018e-04\n",
      "  8.72540596e-02  1.92672400e-03 -1.88918767e-06 -1.88128223e-05\n",
      " -9.38019645e-08 -3.85490296e-05 -9.12442752e-04 -4.94468020e-03\n",
      " -3.49255199e-05 -3.61866516e-04 -8.18077442e-05 -9.92361308e-04\n",
      "  8.73957441e-02  1.87485558e-03 -1.69761273e-06 -1.74693751e-05\n",
      " -1.07447739e-07 -3.68129009e-05 -1.30815023e-03  7.10765466e-03\n",
      " -5.42795749e-05  1.53711902e-02 -2.23300694e-04 -1.52975331e-03\n",
      "  8.75336280e-02  1.92421942e-03 -2.19565843e-06 -2.28740771e-05\n",
      " -1.08828314e-07 -3.81622991e-05 -1.75627328e-03 -6.90371612e-03\n",
      " -7.20921703e-05 -1.77800584e-03 -3.61682675e-04 -2.11576029e-03] [1.89216055e-05 1.91603528e-05 1.89216055e-05 2.02587704e-03\n",
      " 1.91603528e-05 5.50595049e-03 5.13841368e-05 1.99678097e-03\n",
      " 2.02123717e-03 6.94430196e-10 6.31204416e-06 1.88390386e-03\n",
      " 7.29339145e-10 1.82685122e+04 1.68610646e-05 4.99720935e-03\n",
      " 1.38010881e-09 3.05662336e+04 1.74153137e-05 2.58936840e-05\n",
      " 2.01535140e-03 7.37206654e-10 2.74821807e-06 8.05402774e-04\n",
      " 1.70628010e-10 2.46291517e+01 5.46737626e-05 1.58596391e-02\n",
      " 8.81253513e-10 9.95152008e+01 2.90314917e-05 1.09825933e-04\n",
      " 2.00226507e-03 1.00271118e-09 2.15049469e-06 6.02811744e-04\n",
      " 1.00437138e-10 1.23681261e+00 1.89752602e-04 5.70725697e-02\n",
      " 1.34525996e-09 3.60203414e+01 7.03942493e-05 3.93953537e-04\n",
      " 1.92336000e-03 1.27603939e-08 1.65949321e-06 4.63238705e-04\n",
      " 1.16672393e-10 1.76269400e-02 9.18417874e-04 3.14399895e-01\n",
      " 2.30112914e-08 2.53274683e+01 3.01142521e-04 1.89233884e-03\n",
      " 1.69362934e-03 1.31836931e-07 1.24321798e-06 3.47484516e-04\n",
      " 4.72205324e-10 5.97972131e-03 2.36775833e-03 1.00027975e+00\n",
      " 3.56075983e-07 4.50230880e+01 9.08446840e-04 4.37989970e-03\n",
      " 1.10444563e-03 6.75386124e-07 5.72835779e-07 1.31389870e-04\n",
      " 1.31433921e-09 1.97037868e-03 3.10382797e-03 7.85892344e-01\n",
      " 1.37634213e-06 7.22588437e+01 2.07844997e-03 4.85053307e-03\n",
      " 3.28573831e-04 4.85656302e-07 1.36571067e-07 1.69266898e-05\n",
      " 3.81384801e-10 3.09193639e-04 6.71362422e-04 7.84432226e-02\n",
      " 1.02030445e-06 8.32452927e-01 2.68719109e-03 1.37615940e-03\n",
      " 1.10517052e-04 1.76755625e-07 4.99544710e-09 6.50357100e-07\n",
      " 1.97660536e-11 5.34564205e-06 7.75105014e-05 9.68061863e-03\n",
      " 1.11751835e-07 3.23412173e-02 1.79603159e-03 1.92600096e-03\n",
      " 1.70171857e-04 1.99582165e-07 3.69565667e-08 5.15185576e-06\n",
      " 4.20546603e-11 1.17477746e-05 3.32176117e-04 4.39345153e-02\n",
      " 2.82963678e-07 8.84527163e-02 1.86487238e-03 2.65166025e-03\n",
      " 1.26305253e-04 1.92851477e-07 2.06953460e-08 2.64509498e-06\n",
      " 4.18604343e-11 1.20849962e-05 1.92882074e-04 2.41583087e-02\n",
      " 2.34824891e-07 6.85266799e-02 2.23892293e-03 1.86023377e-03]\n",
      "[-1.00577043e-05 -6.78408874e-06 -1.00577043e-05  9.10116797e-02\n",
      " -6.78408874e-06  3.18711766e-03 -3.01419729e-07 -1.09864686e-02\n",
      "  9.10150718e-02  1.19834040e-05 -6.63337887e-06  1.21157696e-03\n",
      "  3.09459322e-09  3.66209975e+01 -1.27836711e-05  3.18228986e-03\n",
      " -3.65740796e-10  3.86827017e+01 -1.00254232e-05 -1.61757155e-05\n",
      "  9.10240665e-02  2.73265363e-05 -5.83175682e-06  3.45132771e-04\n",
      " -6.51465385e-09  3.31100599e-01 -2.69867032e-05  6.23636173e-03\n",
      " -2.22810533e-08  1.29144550e+00 -1.82185348e-05 -3.93734812e-05\n",
      "  9.10375599e-02  4.66448124e-05 -5.53352574e-06  2.07364681e-04\n",
      " -5.79920246e-09  5.53617147e-02 -4.64667254e-05  1.94250133e-02\n",
      " -2.80488267e-08  7.82213335e-01 -3.14136553e-05 -7.23468550e-05\n",
      "  9.10673684e-02  1.34275594e-04 -3.35000861e-06  1.52008340e-04\n",
      " -3.18778971e-08  1.05516294e-02 -4.23589092e-06  9.53026656e-02\n",
      " -1.33031873e-06  2.03005896e+00 -5.90386935e-05 -5.99245758e-05\n",
      "  9.10694864e-02  3.64755339e-04 -2.61777601e-07  1.28678741e-04\n",
      " -2.43782915e-07  3.21030183e-03  1.27231333e-04  2.57545127e-01\n",
      " -4.79015825e-06  1.01069211e+02 -5.80684079e-05  6.94247022e-05\n",
      "  9.10058707e-02  9.49207443e-04  1.06312369e-06  6.88547178e-05\n",
      " -2.28880811e-07  8.77041054e-04 -6.17902881e-05  2.45248660e-01\n",
      " -1.29329971e-05  1.60465456e+00  6.87215964e-06 -5.59812522e-05\n",
      "  9.10367658e-02  1.72639369e-03 -8.06378565e-07 -7.70591560e-07\n",
      " -1.89789144e-07  7.15516561e-05 -8.58249946e-04  3.39262387e-02\n",
      " -2.76695768e-05  2.35474660e-01 -2.58924867e-05 -8.83336054e-04\n",
      "  9.12111322e-02  1.96168773e-03 -3.53833827e-06 -3.22968820e-05\n",
      " -2.66076260e-07 -8.92654598e-05 -1.56148095e-03 -1.77817101e-03\n",
      " -7.60944818e-05  2.44295176e-02 -2.02990851e-04 -1.76093346e-03\n",
      "  9.14658908e-02  1.93408667e-03 -3.18969850e-06 -3.18205246e-05\n",
      " -2.70465781e-07 -8.38101958e-05 -2.22207468e-03  8.36296618e-03\n",
      " -1.10606162e-04  4.37386009e-02 -4.57400780e-04 -2.67628576e-03\n",
      "  9.16910460e-02  2.00340337e-03 -3.78573571e-06 -3.68186207e-05\n",
      " -2.55533456e-07 -8.31600553e-05 -2.92584688e-03 -4.40699301e-03\n",
      " -1.39409472e-04  3.81445707e-02 -6.83152021e-04 -3.60521317e-03] [4.79335701e-05 4.79335701e-05 4.79335701e-05 2.22998307e-03\n",
      " 4.79335701e-05 6.53092700e-03 1.20115668e-04 2.16956488e-03\n",
      " 2.21840553e-03 9.05108930e-10 1.78984658e-05 2.51911511e-03\n",
      " 1.15559762e-09 8.23972572e+04 4.53965754e-05 6.58772760e-03\n",
      " 1.52995313e-09 1.12993032e+05 4.77834178e-05 6.69959355e-05\n",
      " 2.20434347e-03 1.34467424e-09 4.98397911e-06 7.65604838e-04\n",
      " 5.16850952e-10 3.44441399e+00 7.72278139e-05 1.29560779e-02\n",
      " 2.12290050e-09 3.82495755e+01 5.95227791e-05 1.54996057e-04\n",
      " 2.18669396e-03 2.19773043e-09 2.84545683e-06 4.87527166e-04\n",
      " 2.83105923e-10 1.83395947e-01 2.13552071e-04 4.04766011e-02\n",
      " 2.92407106e-09 9.41851710e+00 1.03944537e-04 4.47680392e-04\n",
      " 2.10258376e-03 1.96255838e-08 1.76459599e-06 3.48997923e-04\n",
      " 2.82914444e-10 2.89405616e-02 9.19258328e-04 2.17859867e-01\n",
      " 3.81308381e-08 3.80763807e+01 3.38354192e-04 1.89679764e-03\n",
      " 1.87361181e-03 1.62745828e-07 1.24296361e-06 2.61143073e-04\n",
      " 7.05352722e-10 6.82221024e-03 2.32821191e-03 7.46053298e-01\n",
      " 4.38941863e-07 1.88880998e+07 9.27624248e-04 4.32317157e-03\n",
      " 1.29614072e-03 7.85641433e-07 5.74469328e-07 1.07341819e-04\n",
      " 1.41651239e-09 1.90501936e-03 3.08287517e-03 6.30567081e-01\n",
      " 1.40713767e-06 1.63537887e+01 2.06961090e-03 4.85018270e-03\n",
      " 5.26840734e-04 8.63245258e-07 1.37032244e-07 1.54270989e-05\n",
      " 4.39368198e-10 2.55213831e-04 7.74696198e-04 8.02196926e-02\n",
      " 1.25282344e-06 7.92864207e-01 2.68778568e-03 1.59444225e-03\n",
      " 3.08015998e-04 6.38913624e-07 9.40844153e-09 1.06173514e-06\n",
      " 3.99894125e-11 1.01430569e-05 2.55267109e-04 2.77544844e-02\n",
      " 4.79860142e-07 1.18424099e-01 1.84984640e-03 2.25258873e-03\n",
      " 3.55501424e-04 6.23627900e-07 3.87449129e-08 4.70533440e-06\n",
      " 5.06741530e-11 1.29634128e-05 5.52936185e-04 6.49234658e-02\n",
      " 7.09963136e-07 2.14050833e-01 1.94906020e-03 3.01537978e-03\n",
      " 3.00005057e-04 6.40237092e-07 2.21340957e-08 2.57934146e-06\n",
      " 5.21142171e-11 1.22943237e-05 4.45182525e-04 5.16380308e-02\n",
      " 7.98534855e-07 2.45990763e-01 2.33737578e-03 2.27713066e-03]\n",
      "[-2.81732580e-02 -2.81732580e-02 -2.81732580e-02  6.59632803e+03\n",
      " -2.81732580e-02  1.48721109e+07 -2.30097586e-03 -4.28726431e+01\n",
      "  6.59634212e+03  2.05578309e+04 -2.70227701e-02  3.42408056e+02\n",
      " -4.25680534e-02  2.22292863e+09 -5.33470296e-02  6.84817070e+02\n",
      " -1.47878788e-01  2.85562809e+09 -4.11093991e-02 -6.74336586e-02\n",
      "  6.59638280e+03  5.52094842e+04 -2.80335559e-02  2.24222132e-04\n",
      "  1.41069543e-01  1.97273996e+03 -1.41264852e-01  3.43882488e-03\n",
      "  8.31418730e-01  3.12600307e+03 -8.27981510e-02 -1.96029447e-01\n",
      "  6.59645343e+03  8.90338171e+04 -2.80821777e-02  8.83492032e-05\n",
      "  1.75075021e-01  1.12300105e-01 -3.01252525e-01  6.78571061e-03\n",
      " -1.21445470e+01  2.45990976e+00 -1.53479199e-01 -4.26649546e-01\n",
      "  6.59667974e+03  1.34261291e+05 -2.96108201e-02  3.53207721e-05\n",
      " -1.81861342e+00  1.99175376e-02 -7.25211162e-01  1.00489421e-02\n",
      " -4.25874070e+01  6.30183258e-01 -3.81322376e-01 -1.07692272e+00\n",
      "  6.59704235e+03  1.77708904e+05 -2.92927923e-02  7.29738828e-06\n",
      " -2.16136942e+00  2.72536588e-03 -1.32512118e+00  2.04084162e-02\n",
      " -1.14008661e+02  1.50328803e+00 -7.43609930e-01 -2.03943832e+00\n",
      "  6.59770491e+03  3.08113852e+05 -2.61927067e-02  4.17289925e-07\n",
      " -1.75954492e+00  5.44731423e-04 -1.82876959e+00  3.50438064e-02\n",
      " -3.64756983e+01  5.01620317e+00 -1.40307043e+00 -3.20564732e+00\n",
      "  6.59861930e+03  7.25292502e+05 -1.62061976e-02 -4.73807662e-08\n",
      "  4.36129336e+00  2.07540227e-04  1.97297103e+00  9.20110685e-02\n",
      "  2.34110935e+03  2.17807872e+01 -2.30746872e+00 -3.18291491e-01\n",
      "  6.59857465e+03  1.25535136e+06 -1.68790732e-03  1.85022965e-06\n",
      "  2.70738476e+01  1.25753255e-04  7.46303416e+00  1.39211104e-01\n",
      "  2.87907602e+03  4.74942244e+01 -2.24830607e+00  5.21641600e+00\n",
      "  6.59763281e+03  1.81510550e+06  8.80761856e-03  2.99013394e-06\n",
      "  2.57380853e+01  8.73186142e-05  1.09825831e+01  1.60560889e-01\n",
      " -4.52874396e+02  7.89190200e+01 -1.29596939e+00  9.67780604e+00\n",
      "  6.59630999e+03  2.35881608e+06  1.42118404e-02  3.44073040e-06\n",
      "  1.96732035e+01  6.14209595e-05  1.16722483e+01  1.67699130e-01\n",
      " -7.69009272e+03  1.06774861e+02  3.22527375e-02  1.16902892e+01] [8.22313229e+04 8.22313229e+04 8.22313229e+04 5.66325059e+06\n",
      " 8.22313229e+04 8.71381594e+18 1.84879009e+05 5.66241291e+06\n",
      " 5.64265530e+06 3.59270243e+10 3.60116559e+04 1.71202389e+10\n",
      " 6.66447388e+10 2.02182194e+22 9.97132307e+04 3.42402432e+10\n",
      " 4.31373149e+10 4.06258475e+22 9.25811000e+04 1.47961317e+05\n",
      " 5.60789782e+06 9.57965374e+10 1.03332464e+04 6.85010179e-04\n",
      " 2.55005330e+10 4.52302220e+11 1.35298975e+05 5.56028878e-02\n",
      " 1.68646952e+11 8.75237619e+11 1.26948824e+05 2.39326239e+05\n",
      " 5.57388398e+06 1.02220683e+11 3.24222150e+03 2.27076034e-04\n",
      " 1.08274536e+10 1.43966916e+00 1.34917267e+05 2.00856564e-01\n",
      " 1.93387118e+11 3.71443107e+02 1.55901991e+05 2.68024514e+05\n",
      " 5.52816050e+06 6.81406878e+10 6.83273343e+02 9.06906068e-05\n",
      " 2.39187274e+09 1.14771737e-01 1.73704752e+05 2.79850479e-01\n",
      " 1.00284560e+11 1.48082217e+02 1.95704110e+05 4.55571693e+05\n",
      " 5.48391027e+06 5.47285841e+10 3.37972978e+02 2.63733627e-05\n",
      " 6.61119347e+08 1.11385915e-02 5.21390020e+05 4.02089950e-01\n",
      " 6.23373637e+10 3.86148015e+03 2.90568194e+05 1.22711466e+06\n",
      " 5.35209065e+06 1.37469824e+11 2.17182136e+02 1.08247845e-05\n",
      " 2.09503105e+08 1.30017441e-03 1.66863831e+06 1.90576583e-01\n",
      " 2.01454954e+11 4.07325541e+04 6.41762130e+05 3.60787247e+06\n",
      " 4.93307978e+06 8.91927079e+11 1.46497231e+02 5.70598605e-06\n",
      " 2.33119479e+08 4.48133632e-04 4.36393030e+06 5.85072857e-01\n",
      " 2.01476837e+12 4.13367293e+05 1.72610329e+06 8.56759138e+06\n",
      " 4.40216809e+06 2.34013545e+12 1.04341542e+02 4.25397718e-06\n",
      " 3.29666377e+08 2.39811985e-04 6.40360043e+06 1.15267220e+00\n",
      " 5.54123149e+12 1.33037567e+06 2.94131222e+06 1.15654293e+07\n",
      " 3.84178932e+06 3.96373835e+12 7.68793456e+01 3.09999036e-06\n",
      " 3.97276121e+08 1.66265105e-04 7.30173774e+06 1.68359531e+00\n",
      " 7.69508106e+12 2.75825325e+06 4.07199039e+06 1.21414700e+07\n",
      " 3.29835554e+06 5.16931637e+12 5.60944266e+01 2.18829334e-06\n",
      " 3.82935707e+08 1.13520726e-04 7.34557133e+06 2.14772780e+00\n",
      " 1.08205117e+13 4.29480793e+06 5.00526005e+06 1.16796161e+07]\n",
      "[-2.46216028e-02 -2.46216013e-02 -2.46216028e-02  6.50175165e+03\n",
      " -2.46216013e-02  1.33929183e+07  1.30128073e-04 -2.44138068e+01\n",
      "  6.50176396e+03  1.97074004e+04 -2.46866653e-02  8.88447094e-04\n",
      "  2.57002945e-03  1.94089360e+09 -5.00376687e-02  2.66636013e-03\n",
      "  1.11937539e-01  2.73374075e+09 -3.69974659e-02 -6.23484693e-02\n",
      "  6.50180221e+03  5.18868557e+04 -2.64351757e-02  2.10106479e-04\n",
      "  4.70409424e-01  1.10049240e+02 -1.39542497e-01  4.33898225e-03\n",
      "  7.90853229e-01  9.90042553e+02 -7.70029450e-02 -1.90110266e-01\n",
      "  6.50187199e+03  8.31045633e+04 -2.76227656e-02  9.68928020e-05\n",
      "  2.68386823e-01  1.11666086e-01 -3.01454695e-01  7.55011444e-03\n",
      " -1.03849024e+01  2.89649440e+00 -1.47961783e-01 -4.21793713e-01\n",
      "  6.50209797e+03  1.25503748e+05 -2.92095062e-02  2.48435493e-05\n",
      " -1.46670186e+00  1.88517505e-02 -7.19384700e-01  9.87215280e-03\n",
      " -3.58648847e+01  7.04007552e-01 -3.75529621e-01 -1.06570481e+00\n",
      "  6.50245766e+03  1.68176296e+05 -2.89682898e-02  7.17105779e-06\n",
      " -1.92328607e+00  2.55510709e-03 -1.32665543e+00  1.84112893e-02\n",
      " -1.04021459e+02  1.33814341e+00 -7.34980755e-01 -2.03266790e+00\n",
      "  6.50312099e+03  2.97604979e+05 -2.61798507e-02 -9.87958812e-07\n",
      " -1.54657554e+00  5.62154865e-04 -1.93318060e+00  3.39454026e-02\n",
      " -8.70155226e+01  5.40383007e+00 -1.39552003e+00 -3.30252079e+00\n",
      "  6.50408758e+03  7.12155492e+05 -1.73487775e-02 -3.60403624e-07\n",
      "  2.10043054e+00  2.06381027e-04  1.54503883e+00  9.42266411e-02\n",
      "  2.34619441e+03  2.42252553e+01 -2.35327926e+00 -7.90891651e-01\n",
      "  6.50416738e+03  1.23883193e+06 -3.59258498e-03  1.55253706e-06\n",
      "  2.28623773e+01  1.24303759e-04  7.13927854e+00  1.42808940e-01\n",
      "  3.31967902e+03  5.16298719e+01 -2.41932945e+00  4.72354168e+00\n",
      "  6.50331506e+03  1.79486118e+06  7.62237176e-03  2.87735730e-06\n",
      "  2.76332904e+01  8.71367377e-05  1.07728854e+01  1.64673860e-01\n",
      "  2.45348821e+02  8.73335695e+01 -1.55578870e+00  9.20947433e+00\n",
      "  6.50203141e+03  2.33549216e+06  1.32934594e-02  3.38710166e-06\n",
      "  2.19166365e+01  6.36204010e-05  1.15820244e+01  1.71925931e-01\n",
      " -6.74952866e+03  1.17492834e+02 -2.66465776e-01  1.13022652e+01] [7.88296011e+04 7.88296011e+04 7.88296011e+04 5.59734950e+06\n",
      " 7.88296011e+04 6.28945100e+18 1.80364768e+05 5.59811861e+06\n",
      " 5.57761247e+06 1.15428448e+10 3.37384039e+04 4.54023853e-03\n",
      " 1.95939861e+10 8.13556662e+21 9.28413366e+04 4.70852211e-02\n",
      " 1.66935243e+10 3.78345007e+22 8.71842105e+04 1.37913454e+05\n",
      " 5.54533072e+06 3.83886962e+10 9.59190310e+03 7.27609576e-04\n",
      " 1.09946520e+10 1.75329248e+09 1.24872393e+05 1.86022618e-01\n",
      " 6.52080180e+10 5.78581044e+10 1.18172247e+05 2.22084177e+05\n",
      " 5.51391365e+06 4.87510225e+10 3.02264717e+03 3.20382678e-04\n",
      " 4.97785842e+09 3.25420512e+00 1.25955990e+05 4.81153779e-01\n",
      " 9.06421403e+10 4.74999907e+04 1.45156861e+05 2.51321580e+05\n",
      " 5.47098062e+06 3.78959929e+10 6.49110321e+02 6.53243804e-05\n",
      " 1.20913432e+09 7.29469960e-02 1.70617944e+05 2.89118304e-01\n",
      " 4.80513850e+10 3.75928884e+02 1.84154666e+05 4.43101168e+05\n",
      " 5.42745335e+06 3.60347297e+10 3.27732204e+02 2.80233883e-05\n",
      " 3.47732137e+08 7.87397061e-03 5.17504927e+05 3.93508878e-01\n",
      " 3.90815111e+10 3.89630752e+03 2.79244972e+05 1.21113630e+06\n",
      " 5.29649687e+06 1.22961232e+11 2.13813607e+02 8.35392280e-06\n",
      " 1.61745629e+08 1.32251430e-03 1.65802429e+06 1.66455127e-01\n",
      " 2.06035588e+11 4.55067045e+04 6.29001871e+05 3.57548384e+06\n",
      " 4.87978910e+06 8.55851975e+11 1.45041416e+02 5.80554863e-06\n",
      " 2.23345918e+08 4.22847266e-04 4.33551276e+06 6.05774901e-01\n",
      " 1.94079601e+12 5.29955089e+05 1.70605724e+06 8.50542880e+06\n",
      " 4.35225664e+06 2.27834112e+12 1.03512800e+02 4.34711505e-06\n",
      " 3.19672497e+08 2.35212843e-04 6.36960526e+06 1.19295123e+00\n",
      " 5.40279754e+12 1.59071318e+06 2.91277691e+06 1.15122124e+07\n",
      " 3.79571053e+06 3.87552401e+12 7.63716403e+01 3.17185580e-06\n",
      " 3.87747953e+08 1.63596872e-04 7.27656807e+06 1.74078056e+00\n",
      " 7.51259636e+12 3.42415028e+06 4.03793599e+06 1.21061185e+07\n",
      " 3.25537242e+06 5.06011767e+12 5.58342700e+01 2.23945298e-06\n",
      " 3.74851897e+08 1.17397692e-04 7.32940883e+06 2.22689485e+00\n",
      " 1.05718062e+13 5.22218438e+06 4.97055463e+06 1.16456336e+07]\n",
      "[ 2.62216242e-03  1.42662838e-02  2.62216242e-03  1.35189889e+03\n",
      "  1.42662838e-02  3.02523802e+07 -9.06755327e-05  1.05941847e+02\n",
      "  1.35189175e+03  2.41529154e+03  1.43116216e-02  5.70752106e+05\n",
      "  1.06543751e-03  9.48532399e+08  2.86585059e-02  1.41403456e+06\n",
      "  1.35761423e-03  1.95048553e+09  2.14447635e-02  3.57916478e-02\n",
      "  1.35187031e+03  4.17494398e+03  1.41212030e-02  1.77885616e-04\n",
      " -6.52138431e-03  3.91457194e-01  7.09687169e-02  1.37979218e-03\n",
      "  1.58045439e-01  2.08216422e+00  4.26981009e-02  9.95456148e-02\n",
      "  1.35183483e+03  5.07843235e+03  1.43488993e-02  6.52142989e-05\n",
      "  6.21373231e-02  6.72897370e-02  1.41663392e-01  1.77466071e-03\n",
      "  6.82049670e-01  8.50918906e-01  7.84101557e-02  2.05724649e-01\n",
      "  1.35172904e+03  6.55687507e+03  1.40800967e-02  3.12224503e-05\n",
      "  3.94530613e-02  1.26954663e-02  3.50754723e-01  4.92097796e-03\n",
      "  6.75466368e-01  5.39617483e-01  1.83929878e-01  5.20604504e-01\n",
      "  1.35155366e+03  9.02666071e+03  1.42181250e-02  2.52865928e-05\n",
      "  3.72508992e-02  3.74558019e-03  7.65250113e-01  1.54240641e-02\n",
      "  4.93949191e+00  4.43252737e-01  3.59445267e-01  1.11047726e+00\n",
      "  1.35117103e+03  1.65244503e+04  1.52378218e-02  2.38018830e-05\n",
      "  1.77493330e-01  1.14195481e-03  1.58185623e+00  5.24391635e-02\n",
      "  6.93122188e+00  8.67310012e-01  7.43090021e-01  2.30970843e+00\n",
      "  1.35038011e+03  4.11378627e+04  1.52748980e-02  2.07483226e-05\n",
      "  7.96633812e-02  3.81406293e-04  2.40291361e+00  1.45755244e-01\n",
      " -1.10612790e+02  2.41005486e+00  1.53405521e+00  3.92169392e+00\n",
      "  1.34968485e+03  7.26040532e+04  1.34633688e-02  1.57083506e-05\n",
      " -6.37571966e-01  1.59936918e-04  2.35643635e+00  1.95205704e-01\n",
      " -2.77790103e+02  2.32816573e+00  2.22749746e+00  4.57047044e+00\n",
      "  1.34917865e+03  1.05742671e+05  1.09154199e-02  1.10775683e-05\n",
      " -1.90577257e+00  7.23298628e-05  1.87947048e+00  1.88663097e-01\n",
      " -1.97984537e+00  2.44812768e+00  2.73115254e+00  4.59970760e+00\n",
      "  1.34881085e+03  1.37949957e+05  9.54063775e-03  8.55364772e-06\n",
      " -2.34574550e+00  3.09694898e-05  9.43156798e-01  1.60047212e-01\n",
      "  5.63618730e+02  3.35871431e+00  3.09757344e+00  4.03118960e+00] [9.66116596e+03 9.66116596e+03 9.66116596e+03 2.93861873e+05\n",
      " 9.66116596e+03 4.35257137e+18 2.81058682e+04 2.83711999e+05\n",
      " 2.91452144e+05 3.83534730e+08 2.63469678e+03 3.23327809e+16\n",
      " 5.29087951e+08 1.07167546e+22 5.67282118e+03 1.98457751e+17\n",
      " 6.03221757e+08 3.27907986e+22 7.68468611e+03 8.49154068e+03\n",
      " 2.89708800e+05 6.28080543e+08 4.81039307e+02 3.21485727e-04\n",
      " 1.61119640e+08 3.10908700e+01 3.61426454e+03 2.66676799e-03\n",
      " 1.13784274e+09 3.94995164e+02 6.83777608e+03 8.67316643e+03\n",
      " 2.88831164e+05 5.47802243e+08 1.41463873e+02 9.43174546e-05\n",
      " 6.06986123e+07 1.24524090e+00 4.11904065e+03 3.22363389e-03\n",
      " 1.04644334e+09 4.21566559e+01 7.06401100e+03 1.13474933e+04\n",
      " 2.87430145e+05 3.06770913e+08 3.52597311e+01 2.65784216e-05\n",
      " 1.26615747e+07 1.09913453e-01 9.88037045e+03 9.11406403e-03\n",
      " 5.93390043e+08 1.11519174e+01 9.22856566e+03 2.51939331e+04\n",
      " 2.85085961e+05 1.86556945e+08 1.75773598e+01 1.50078841e-05\n",
      " 3.47377327e+06 1.86961003e-02 3.00004516e+04 3.02046983e-02\n",
      " 3.49851616e+08 4.88216603e+00 1.51792957e+04 7.02156674e+04\n",
      " 2.77808689e+05 2.65043479e+08 1.22738313e+01 1.16173222e-05\n",
      " 1.02068284e+06 3.50343280e-03 9.84650099e+04 1.13378722e-01\n",
      " 4.81198781e+08 5.94904985e+00 3.59854774e+04 2.11796618e+05\n",
      " 2.53525305e+05 1.95930722e+09 8.56166219e+00 8.84715732e-06\n",
      " 6.61208371e+05 8.96937260e-04 2.58192235e+05 3.80140789e-01\n",
      " 5.07793160e+09 3.67024982e+01 1.00373426e+05 5.02484247e+05\n",
      " 2.22238583e+05 6.14968458e+09 6.12235937e+00 6.49387642e-06\n",
      " 9.78816062e+05 3.90763611e-04 3.78737487e+05 5.32983755e-01\n",
      " 1.49631461e+10 2.61814606e+01 1.72005244e+05 6.80979196e+05\n",
      " 1.89154049e+05 1.13189833e+10 4.50132458e+00 4.39349512e-06\n",
      " 1.24654227e+06 2.07402615e-04 4.32189447e+05 4.96458168e-01\n",
      " 1.99094668e+10 3.37748793e+01 2.37823184e+05 7.15083085e+05\n",
      " 1.56937961e+05 1.51542574e+10 3.31367252e+00 2.72933945e-06\n",
      " 1.20748796e+06 1.23150418e-04 4.34050908e+05 3.95308994e-01\n",
      " 3.11747150e+10 5.84807970e+01 2.93282598e+05 6.87350118e+05]\n",
      "[-3.66350475e-01 -1.10482166e-03 -3.66350475e-01  1.50773035e+03\n",
      " -1.10482166e-03  2.14267203e+07  8.18386415e-04 -1.14313280e+02\n",
      "  1.50773090e+03  2.70493181e+03 -1.51401487e-03  7.95539795e+05\n",
      " -3.96917411e-02  1.13299352e+09 -3.80208689e-03  2.78490077e+06\n",
      " -4.84723454e-03  1.78829843e+09 -2.06642570e-03 -4.35449772e-03\n",
      "  1.50773369e+03  4.72646855e+03 -2.00641069e-03  8.15658462e+04\n",
      " -3.77347064e-02  2.13177662e+07 -9.58466889e-03  1.72461297e+06\n",
      " -1.39329742e-01  8.84856907e+08 -5.34815522e-03 -1.29264134e-02\n",
      "  1.50773848e+03  5.86418135e+03 -1.81886381e-03  9.66075558e-05\n",
      " -1.11900020e-02  6.66236149e-02 -1.81299870e-02  2.90637838e-03\n",
      " -4.95198800e-02  8.59203775e-01 -9.95294278e-03 -2.62640660e-02\n",
      "  1.50775138e+03  7.91260567e+03 -1.54565914e-03  1.28004579e-05\n",
      " -9.86275660e-03  1.19180763e-02 -3.40664257e-02  4.77620464e-03\n",
      "  2.12114585e-01  5.51134940e-01 -2.25792812e-02 -5.51000477e-02\n",
      "  1.50776842e+03  1.13162974e+04 -1.42549274e-03  6.96032273e-06\n",
      " -8.86756325e-03  3.88005339e-03 -4.85022165e-02  1.50345322e-02\n",
      " -2.74269429e-01  5.29016288e-01 -3.94923276e-02 -8.65690514e-02\n",
      "  1.50779267e+03  2.13679115e+04 -9.83291277e-04  5.35630123e-06\n",
      " -2.91738635e-02  1.26113903e-03  3.32031644e-02  5.13356937e-02\n",
      " -1.05347280e+01  1.02004409e+00 -6.33012344e-02 -2.91147787e-02\n",
      "  1.50777606e+03  5.39779027e+04  4.74050331e-04  4.86336132e-06\n",
      " -4.34753646e-01  3.63690872e-04  6.85752063e-01  1.45222814e-01\n",
      " -9.76955697e+01  2.41532770e+00 -4.52423106e-02  6.40035702e-01\n",
      "  1.50765585e+03  9.65989117e+04  2.00729728e-03  4.81645493e-06\n",
      " -1.16255999e+00  1.53400494e-04  1.31436686e+00  1.96061640e-01\n",
      " -1.05473072e+02  2.37580563e+00  7.65090136e-02  1.38886858e+00\n",
      "  1.50743319e+03  1.41975827e+05  3.28544636e-03  4.86656814e-06\n",
      " -1.60664959e+00  7.68968831e-05  6.99461263e-01  1.88062553e-01\n",
      " -3.41723226e+02  2.94311830e+00  3.00445117e-01  9.96620934e-01\n",
      "  1.50718262e+03  1.85629889e+05  3.40692900e-03  4.28795400e-06\n",
      " -1.33675115e+00  4.25021230e-05 -8.32054777e-01  1.57947908e-01\n",
      " -8.63024333e+02  4.07889553e+00  5.51133097e-01 -2.84328609e-01] [1.08203347e+04 1.08197272e+04 1.08203347e+04 4.15706180e+05\n",
      " 1.08197272e+04 3.15898207e+18 3.12974723e+04 4.02652326e+05\n",
      " 4.13001426e+05 5.79764589e+08 2.99543854e+03 4.71387145e+16\n",
      " 7.38409065e+08 2.81818252e+22 6.46889216e+03 3.35887303e+17\n",
      " 9.85475485e+08 4.25016125e+22 8.69576920e+03 9.65176221e+03\n",
      " 4.10981010e+05 7.96209207e+08 5.62220628e+02 5.32937492e+14\n",
      " 2.37010997e+08 4.36576171e+19 4.55057245e+03 1.71016975e+17\n",
      " 1.47674869e+09 4.35469479e+22 7.88599415e+03 1.05760888e+04\n",
      " 4.09845204e+05 6.68149123e+08 1.72761927e+02 5.13926725e-04\n",
      " 8.42927888e+07 1.51717607e+00 5.68957317e+03 9.07308819e-02\n",
      " 1.20671747e+09 6.09440272e+01 8.41547375e+03 1.46677473e+04\n",
      " 4.07801225e+05 4.55779127e+08 4.49949860e+01 2.74466559e-05\n",
      " 1.69970966e+07 1.02365113e-01 1.36151899e+04 9.78381770e-03\n",
      " 7.66670747e+08 1.41545013e+01 1.15724694e+04 3.34963792e+04\n",
      " 4.04403372e+05 3.65121544e+08 2.31314677e+01 1.56244717e-05\n",
      " 4.95967752e+06 2.34699322e-02 4.02059057e+04 3.11095289e-02\n",
      " 6.22568009e+08 1.07888521e+01 1.97247589e+04 9.24334335e+04\n",
      " 3.94358518e+05 6.19108687e+08 1.60360288e+01 1.19128342e-05\n",
      " 1.54935104e+06 5.16678550e-03 1.30418894e+05 1.12065816e-01\n",
      " 1.07148648e+09 9.96511605e+00 4.70529112e+04 2.81560204e+05\n",
      " 3.61758699e+05 4.35965069e+09 1.15074392e+01 9.09757939e-06\n",
      " 1.20413359e+06 9.56161198e-04 3.51795834e+05 3.67369073e-01\n",
      " 1.02758505e+10 3.61120775e+01 1.33201100e+05 6.85743416e+05\n",
      " 3.19209751e+05 1.34499513e+10 8.37200482e+00 6.71165082e-06\n",
      " 1.89344426e+06 3.91162800e-04 5.12484395e+05 5.14275577e-01\n",
      " 3.09174644e+10 2.28418170e+01 2.32366922e+05 9.07837255e+05\n",
      " 2.74022607e+05 2.48608334e+10 6.08707421e+00 4.55036482e-06\n",
      " 2.41446342e+06 2.17486002e-04 5.71769413e+05 4.77789973e-01\n",
      " 4.48431471e+10 4.55982859e+01 3.22805998e+05 9.29589803e+05\n",
      " 2.30612963e+05 3.41052140e+10 4.37239101e+00 2.81869472e-06\n",
      " 2.42125812e+06 1.13674258e-04 5.64911323e+05 3.81158477e-01\n",
      " 6.79614091e+10 8.56526786e+01 3.95016675e+05 8.89789560e+05]\n",
      "[-3.48809675e-01 -1.77951714e-03 -3.48809675e-01  1.51989219e+03\n",
      " -1.77951714e-03  1.54763447e+07  5.79536235e-04 -1.13491153e+02\n",
      "  1.51989308e+03  2.52018531e+03 -2.06928526e-03  1.53747555e+05\n",
      "  3.62210147e-03  7.85587186e+08 -4.25788681e-03  1.53747557e+05\n",
      " -6.60500856e-03  1.21014048e+09 -2.95904383e-03 -5.14764538e-03\n",
      "  1.51989636e+03  4.43720364e+03 -2.03451309e-03  1.49137321e-04\n",
      " -1.21143527e-03  3.68922030e-01 -8.54168229e-03  1.28774624e-03\n",
      " -6.48616954e-02  2.10612007e+00 -6.19899229e-03 -1.27061615e-02\n",
      "  1.51990063e+03  5.54254129e+03 -1.66838255e-03  4.40919623e-05\n",
      " -1.63530807e-02  6.47460201e-02 -1.87135659e-02  1.67721827e-03\n",
      " -3.01009961e-01  8.01014243e-01 -1.01037029e-02 -2.71488863e-02\n",
      "  1.51991436e+03  7.60473098e+03 -1.74597222e-03  1.24049328e-05\n",
      " -2.35309711e-02  1.16335886e-02 -4.08596227e-02  4.73252222e-03\n",
      " -3.57754214e-01  5.13526227e-01 -2.39088015e-02 -6.30224519e-02\n",
      "  1.51993479e+03  1.10636461e+04 -1.50515787e-03  6.73157737e-06\n",
      " -2.42805992e-02  3.52869880e-03 -5.19001289e-02  1.47638370e-02\n",
      " -5.66449147e-01  4.80953071e-01 -4.40977984e-02 -9.44927694e-02\n",
      "  1.51996074e+03  2.11607074e+04 -1.08836905e-03  5.20144718e-06\n",
      " -5.22641424e-02  1.13864056e-03  2.03306766e-02  5.06502022e-02\n",
      " -1.08694988e+01  9.69961132e-01 -6.96310741e-02 -4.82120284e-02\n",
      "  1.51995057e+03  5.40195407e+04  3.16665417e-04  4.70711036e-06\n",
      " -4.41698995e-01  3.44076014e-04  6.55726636e-01  1.43500570e-01\n",
      " -9.18401452e+01  2.37074530e+00 -5.80607013e-02  5.97349269e-01\n",
      "  1.51983573e+03  9.70123297e+04  1.94571905e-03  4.73583551e-06\n",
      " -1.12222294e+00  1.42548102e-04  1.15221401e+00  1.93945586e-01\n",
      " -6.27299198e+01  2.39369614e+00  5.84110026e-02  1.20867929e+00\n",
      "  1.51962271e+03  1.42796251e+05  3.01899174e-03  4.65626255e-06\n",
      " -1.52851345e+00  7.58322431e-05  4.24024930e-01  1.86140568e-01\n",
      " -3.32899501e+02  3.01530873e+00  2.72504943e-01  6.93510882e-01\n",
      "  1.51940269e+03  1.86842894e+05  2.91659451e-03  3.92393675e-06\n",
      " -1.12080557e+00  4.64919643e-05 -1.07093585e+00  1.56629431e-01\n",
      " -8.69981853e+02  4.16449935e+00  4.92415384e-01 -5.81437059e-01] [1.00816766e+04 1.00807412e+04 1.00816766e+04 4.20198412e+05\n",
      " 1.00807412e+04 2.02207147e+18 2.90646424e+04 4.07337250e+05\n",
      " 4.17678578e+05 2.75958145e+08 2.81457340e+03 3.46698370e+15\n",
      " 3.91046517e+08 7.55974104e+20 6.12962385e+03 3.46698370e+15\n",
      " 4.03695057e+08 2.30252838e+21 8.14933573e+03 9.15031738e+03\n",
      " 4.15762687e+05 5.79994535e+08 5.31717352e+02 3.11436476e-04\n",
      " 1.74479491e+08 2.75796359e+01 4.42122079e+03 3.01887959e-03\n",
      " 1.06240964e+09 3.19512182e+03 7.43909004e+03 1.01954421e+04\n",
      " 4.14658696e+05 5.16862382e+08 1.66454729e+02 9.02273407e-05\n",
      " 6.65516717e+07 1.42797539e+00 5.68717829e+03 3.41771908e-03\n",
      " 9.74696054e+08 4.90533633e+01 8.04719901e+03 1.44217998e+04\n",
      " 4.12601126e+05 3.20868168e+08 4.46727607e+01 2.68934183e-05\n",
      " 1.40410428e+07 1.07447412e-01 1.38349432e+04 9.64683663e-03\n",
      " 5.95599510e+08 1.11319844e+01 1.13473824e+04 3.34094262e+04\n",
      " 4.09148717e+05 2.45115785e+08 2.29263382e+01 1.52630214e-05\n",
      " 4.07274691e+06 1.93908022e-02 4.03871097e+04 3.04861242e-02\n",
      " 4.20561809e+08 8.63956005e+00 1.95075634e+04 9.26638238e+04\n",
      " 3.99057756e+05 5.39809779e+08 1.60847788e+01 1.17430582e-05\n",
      " 1.31704557e+06 4.60362151e-03 1.31413594e+05 1.10370367e-01\n",
      " 9.08280286e+08 8.03293700e+00 4.70241955e+04 2.83447870e+05\n",
      " 3.66210036e+05 4.41455287e+09 1.15889859e+01 8.98835338e-06\n",
      " 1.16586389e+06 9.32790460e-04 3.54922730e+05 3.61747188e-01\n",
      " 1.03617088e+10 3.36469292e+01 1.33895777e+05 6.91584121e+05\n",
      " 3.23290665e+05 1.36789580e+10 8.43547127e+00 6.63522633e-06\n",
      " 1.90218336e+06 3.72922018e-04 5.17168665e+05 5.07183110e-01\n",
      " 3.14522780e+10 2.27182582e+01 2.33995335e+05 9.15956048e+05\n",
      " 2.77695238e+05 2.52772306e+10 6.13735989e+00 4.50532601e-06\n",
      " 2.44097000e+06 2.10691394e-04 5.77072497e+05 4.72222031e-01\n",
      " 4.57968486e+10 4.71884330e+01 3.25304006e+05 9.38008900e+05\n",
      " 2.33880194e+05 3.46650695e+10 4.41075585e+00 2.79563101e-06\n",
      " 2.45678025e+06 1.13982533e-04 5.70387684e+05 3.77935412e-01\n",
      " 6.92289094e+10 8.76090298e+01 3.98219680e+05 8.98064620e+05]\n",
      "[-2.88850902e-03  4.30840548e-03 -2.88850902e-03  1.79376211e+03\n",
      "  4.30840548e-03  2.45374600e+07  1.00211330e-03  4.50799996e+01\n",
      "  1.79375996e+03  2.37540089e+03  3.80734883e-03  6.47283388e+05\n",
      " -2.58044175e-02  1.33961356e+09  7.33178812e-03  2.02467790e+06\n",
      " -5.02368941e-02  1.50646821e+09  5.96155157e-03  9.48599087e-03\n",
      "  1.79375445e+03  4.85068815e+03  3.54761742e-03  1.29106001e-04\n",
      "  2.15365737e-03  3.38535825e-01  1.75662963e-02  1.25895414e-03\n",
      " -3.34637671e-03  1.66268807e+00  1.12059445e-02  2.52246234e-02\n",
      "  1.79374567e+03  6.36813152e+03  3.56943214e-03  4.00890615e-05\n",
      " -2.65321426e-04  6.21164594e-02  3.78614766e-02  1.53374623e-03\n",
      "  3.96067217e-02  8.52138757e-01  2.00109074e-02  5.43029518e-02\n",
      "  1.79371671e+03  9.02766108e+03  3.93537392e-03  1.27714579e-05\n",
      " -7.91542436e-03  1.20058184e-02  9.59522803e-02  3.77029829e-03\n",
      " -6.13278615e-01  6.41345664e-01  4.93342423e-02  1.41351149e-01\n",
      "  1.79366874e+03  1.32301270e+04  3.75622060e-03  7.53196402e-06\n",
      " -2.15578431e-02  3.91812174e-03  1.97733997e-01  1.10051596e-02\n",
      " -1.07895959e+00  5.40651396e-01  9.71312291e-02  2.91109005e-01\n",
      "  1.79356987e+03  2.51789518e+04  4.01629286e-03  6.24359365e-06\n",
      " -2.61540153e-02  1.29868944e-03  4.55920308e-01  3.66710105e-02\n",
      " -6.10755474e+00  9.93700856e-01  1.96258300e-01  6.48162315e-01\n",
      "  1.79334191e+03  6.39353682e+04  4.67809667e-03  5.56955543e-06\n",
      " -2.19058991e-01  3.98178486e-04  1.44145797e+00  1.02795976e-01\n",
      " -8.86999901e+01  1.97740632e+00  4.24880258e-01  1.86166013e+00\n",
      "  1.79304046e+03  1.13488972e+05  6.07085236e-03  5.58015695e-06\n",
      " -8.29435472e-01  1.75262777e-04  2.46465076e+00  1.45023781e-01\n",
      " -2.10091877e+01  2.71959863e+00  7.27726066e-01  3.18630597e+00\n",
      "  1.79262118e+03  1.65703410e+05  7.40558320e-03  5.79667177e-06\n",
      " -1.55239159e+00  8.27037412e-05  2.20946181e+00  1.46563880e-01\n",
      "  6.54168440e+01  3.51802085e+00  1.14833673e+00  3.35039295e+00\n",
      "  1.79217628e+03  2.16071965e+05  7.35075329e-03  5.47490838e-06\n",
      " -1.13080449e+00  4.98767989e-05  9.92083100e-01  1.29113281e-01\n",
      " -3.31026111e+02  4.47467248e+00  1.59318136e+00  2.57791371e+00] [9.50160354e+03 9.50160354e+03 9.50160354e+03 4.54167868e+05\n",
      " 9.50160354e+03 3.99369350e+18 2.49888792e+04 4.52422878e+05\n",
      " 4.51790411e+05 6.78905464e+08 3.25443535e+03 6.14591386e+16\n",
      " 9.42774244e+08 5.57413716e+22 7.75219522e+03 3.39758018e+17\n",
      " 1.01840232e+09 5.60776309e+22 8.88424578e+03 1.13709535e+04\n",
      " 4.49309824e+05 1.42967748e+09 6.37986932e+02 2.56920199e-04\n",
      " 4.02180979e+08 1.54744484e+01 6.06976651e+03 2.62108285e-03\n",
      " 2.60918622e+09 5.21788223e+02 8.78953156e+03 1.26391910e+04\n",
      " 4.47784427e+05 1.28418820e+09 1.99757329e+02 7.70910290e-05\n",
      " 1.55326721e+08 1.19411627e+00 7.38942461e+03 3.06684040e-03\n",
      " 2.47120208e+09 8.11812821e+01 9.59952428e+03 1.77483563e+04\n",
      " 4.45098756e+05 7.38267820e+08 5.38693062e+01 2.20061657e-05\n",
      " 3.27814778e+07 1.05739383e-01 1.68086277e+04 7.45341192e-03\n",
      " 1.44076129e+09 1.73693548e+01 1.37045192e+04 3.98302598e+04\n",
      " 4.40850911e+05 4.59482041e+08 2.69855343e+01 1.17242545e-05\n",
      " 8.91170565e+06 2.14888775e-02 4.77931022e+04 2.17868396e-02\n",
      " 8.68490818e+08 7.70492145e+00 2.32411253e+04 1.10321855e+05\n",
      " 4.28819397e+05 7.53499834e+08 1.92240537e+01 8.84143914e-06\n",
      " 2.54647371e+06 4.74336994e-03 1.55013243e+05 7.37228793e-02\n",
      " 1.40401266e+09 7.78552502e+00 5.60809664e+04 3.32088182e+05\n",
      " 3.89919033e+05 5.86366770e+09 1.34625979e+01 6.64599651e-06\n",
      " 1.81708673e+06 1.01681385e-03 4.06892690e+05 2.25722785e-01\n",
      " 1.42495747e+10 1.75157141e+01 1.57062375e+05 7.91337723e+05\n",
      " 3.40289936e+05 1.72957935e+10 9.64082737e+00 4.95697686e-06\n",
      " 2.63867863e+06 4.19482059e-04 5.92457863e+05 3.35161368e-01\n",
      " 4.22331686e+10 3.52833581e+01 2.70196086e+05 1.05828655e+06\n",
      " 2.88121602e+05 3.04105366e+10 7.05028849e+00 3.45213955e-06\n",
      " 3.20720046e+06 2.12811400e-04 6.67614084e+05 3.37962185e-01\n",
      " 6.03651340e+10 7.50288578e+01 3.73949294e+05 1.09781302e+06\n",
      " 2.37932941e+05 3.95713170e+10 5.12306343e+00 2.23144678e-06\n",
      " 3.10913791e+06 1.14890160e-04 6.65167377e+05 2.92420209e-01\n",
      " 8.77977937e+10 1.07311368e+02 4.58752913e+05 1.05223982e+06]\n",
      "[-1.72736875e-03  9.05598645e-04 -1.72736875e-03  1.78138583e+03\n",
      "  9.05598645e-04  2.39749938e+07 -1.40952047e-03  3.40788056e+01\n",
      "  1.78138538e+03  2.67883112e+03  1.61035888e-03  1.93827323e+06\n",
      " -2.46666082e-03  2.71605712e+09  3.62604619e-03  4.53785257e+06\n",
      "  1.04089070e-02  1.21684644e+09  2.06315820e-03  4.07884551e-03\n",
      "  1.78138249e+03  5.36723755e+03  1.97186802e-03  1.36487265e-04\n",
      " -2.33995005e-03  3.65122881e-01  1.09898777e-02  1.32407278e-03\n",
      "  7.26932796e-03  1.87853720e+00  5.30943721e-03  1.43274469e-02\n",
      "  1.78137700e+03  6.98593049e+03  2.33848940e-03  4.25776399e-05\n",
      " -2.56518119e-02  6.89898510e-02  2.87271957e-02  1.63552513e-03\n",
      " -1.50868133e-01  1.03349790e+00  1.11709975e-02  3.75597038e-02\n",
      "  1.78135506e+03  9.78084374e+03  2.81874881e-03  1.28802970e-05\n",
      " -3.46328031e-02  1.47894796e-02  6.78126871e-02  3.86187880e-03\n",
      " -6.73725662e-02  7.86439901e-01  3.35828112e-02  9.85767495e-02\n",
      "  1.78132116e+03  1.40382376e+04  2.60155120e-03  7.23646812e-06\n",
      " -1.77452084e-02  4.66159680e-03  1.44275156e-01  1.10958235e-02\n",
      " -1.09611629e+00  5.76278259e-01  6.72719571e-02  2.08945562e-01\n",
      "  1.78124902e+03  2.59776886e+04  2.98409360e-03  5.91272725e-06\n",
      " -2.39332194e-02  1.52232762e-03  4.29409024e-01  3.70876510e-02\n",
      " -3.83845133e+00  9.85267547e-01  1.39792077e-01  5.66217008e-01\n",
      "  1.78103432e+03  6.48050787e+04  4.34964872e-03  5.50322280e-06\n",
      " -2.36810874e-01  4.35110448e-04  1.15616578e+00  1.04099477e-01\n",
      " -1.69064911e+01  1.92744721e+00  3.55862145e-01  1.50767827e+00\n",
      "  1.78075055e+03  1.14593344e+05  5.08853931e-03  5.24115784e-06\n",
      " -6.07367910e-01  1.94691059e-04  1.43430520e+00  1.46499199e-01\n",
      "  1.36368244e+02  2.65422971e+00  6.40366037e-01  2.06958269e+00\n",
      "  1.78045623e+03  1.67065508e+05  5.14851087e-03  4.87331692e-06\n",
      " -5.00478871e-01  1.02324363e-04  1.06958279e+00  1.48000224e-01\n",
      " -2.27603746e+01  3.40111645e+00  9.34743895e-01  1.99917817e+00\n",
      "  1.78020951e+03  2.17586408e+05  4.44612418e-03  4.13800465e-06\n",
      "  5.93418709e-01  6.03644641e-05  5.22675095e-01  1.31100238e-01\n",
      " -2.65088316e+02  4.32845552e+00  1.18076062e+00  1.69898959e+00] [1.07153245e+04 1.07153245e+04 1.07153245e+04 4.55373510e+05\n",
      " 1.07153245e+04 3.96676201e+18 2.84835196e+04 4.54497312e+05\n",
      " 4.52694273e+05 1.12392662e+09 3.59444949e+03 2.01388345e+17\n",
      " 1.57467313e+09 2.76119527e+23 8.45943766e+03 9.92737850e+17\n",
      " 1.73095725e+09 1.18776659e+22 9.86772763e+03 1.24087544e+04\n",
      " 4.50003401e+05 1.90733154e+09 6.93695659e+02 2.73129625e-04\n",
      " 5.03745532e+08 2.65585435e+01 6.47478619e+03 2.73005253e-03\n",
      " 3.50750527e+09 4.67258292e+02 9.59759491e+03 1.36684850e+04\n",
      " 4.48380086e+05 1.62377749e+09 2.16682568e+02 8.31411498e-05\n",
      " 1.88539322e+08 1.86949812e+00 7.80813482e+03 3.28106425e-03\n",
      " 3.13601294e+09 1.40944939e+02 1.04469055e+04 1.89204844e+04\n",
      " 4.45568721e+05 8.92826943e+08 5.65669205e+01 2.31391267e-05\n",
      " 3.89043340e+07 2.38401847e-01 1.70294359e+04 7.66354688e-03\n",
      " 1.74345381e+09 3.12372113e+01 1.46146796e+04 4.07755785e+04\n",
      " 4.41286280e+05 5.39734498e+08 2.76157030e+01 1.20512622e-05\n",
      " 1.05736929e+07 3.21731878e-02 4.77555912e+04 2.19866599e-02\n",
      " 1.02771293e+09 8.07687728e+00 2.41114683e+04 1.11057836e+05\n",
      " 4.29299901e+05 7.97396725e+08 1.94099017e+01 9.00094378e-06\n",
      " 2.97241742e+06 6.27891072e-03 1.55301699e+05 7.45526988e-02\n",
      " 1.50800532e+09 7.35921252e+00 5.68963370e+04 3.33771052e+05\n",
      " 3.90346913e+05 5.97679078e+09 1.35571634e+01 6.74945239e-06\n",
      " 1.94278130e+06 1.18327345e-03 4.09006567e+05 2.28629898e-01\n",
      " 1.44623862e+10 1.66400617e+01 1.58248983e+05 7.96493994e+05\n",
      " 3.40508695e+05 1.75961972e+10 9.70893018e+00 5.03955824e-06\n",
      " 2.72488334e+06 4.47972408e-04 5.94847471e+05 3.38983337e-01\n",
      " 4.30649119e+10 3.31155026e+01 2.72205779e+05 1.06333875e+06\n",
      " 2.88163343e+05 3.09524616e+10 7.09218837e+00 3.51803472e-06\n",
      " 3.27121097e+06 2.18928857e-04 6.69829961e+05 3.42100087e-01\n",
      " 6.19407995e+10 6.95697258e+01 3.76600624e+05 1.10138424e+06\n",
      " 2.37835344e+05 4.03421153e+10 5.14627647e+00 2.27674834e-06\n",
      " 3.17555791e+06 1.19038267e-04 6.68204168e+05 2.97327008e-01\n",
      " 8.93284927e+10 1.00105722e+02 4.61680240e+05 1.05755055e+06]\n",
      "[ 3.24971625e-03  5.36378782e-03  3.24971625e-03  1.69869088e+03\n",
      "  5.36378782e-03  1.60882495e+07  4.90447942e-04  8.67651917e+01\n",
      "  1.69868820e+03  2.18553624e+03  5.11856384e-03  1.21550418e-03\n",
      "  3.54348638e-02  1.11628299e+09  1.02293428e-02  2.71893402e-03\n",
      "  2.24944338e-02  9.04798332e+08  7.80045775e-03  1.29112367e-02\n",
      "  1.69868040e+03  4.38659771e+03  5.45253554e-03  1.29364657e-04\n",
      "  8.98874305e-03  3.46180044e-01  2.57321687e-02  1.28188623e-03\n",
      "  5.61682781e-02  1.69022638e+00  1.59341087e-02  3.62137419e-02\n",
      "  1.69866753e+03  5.75958085e+03  4.81651019e-03  4.28415710e-05\n",
      "  2.70016504e-02  6.08369102e-02  4.19262927e-02  1.68505715e-03\n",
      "  1.95449889e-01  8.05368231e-01  2.81641677e-02  6.52739502e-02\n",
      "  1.69863635e+03  8.29637284e+03  4.27795164e-03  1.38179296e-05\n",
      "  4.43440173e-02  1.20141485e-02  9.22490541e-02  4.01099707e-03\n",
      " -3.88823143e-02  6.24617307e-01  5.88067324e-02  1.46777835e-01\n",
      "  1.69859022e+03  1.22566553e+04  3.82533825e-03  7.85266147e-06\n",
      "  1.94915877e-02  3.68940950e-03  1.83738615e-01  1.11664221e-02\n",
      "  1.05108520e+00  4.84726873e-01  1.04478646e-01  2.84391922e-01\n",
      "  1.69849836e+03  2.31554977e+04  3.44201037e-03  6.18388004e-06\n",
      "  1.24472303e-01  1.09574506e-03  1.98004110e-01  3.69530683e-02\n",
      "  2.74854848e+01  9.33589784e-01  1.95964625e-01  3.90526726e-01\n",
      "  1.69839935e+03  5.85164684e+04  1.95930061e-03  4.43903967e-06\n",
      "  9.24523079e-01  3.67279650e-04  4.13173587e-02  1.02678781e-01\n",
      "  9.13630522e+01  1.87453527e+00  2.93483971e-01  3.32842029e-01\n",
      "  1.69837975e+03  1.03759965e+05  1.01906779e-03  3.15308941e-06\n",
      "  1.60760248e+00  1.77562837e-04  1.66252900e-01  1.43876807e-01\n",
      "  1.14641488e+02  2.71566082e+00  3.12146397e-01  4.77380229e-01\n",
      "  1.69837869e+03  1.51397036e+05  8.84188114e-04  2.39569762e-06\n",
      "  1.37986605e+00  9.77759608e-05  7.54220986e-01  1.45650087e-01\n",
      "  4.78093490e+02  3.77657803e+00  3.13067538e-01  1.06640434e+00\n",
      "  1.69835629e+03  1.97235509e+05  1.02316783e-03  1.94273916e-06\n",
      "  1.00376974e+00  6.04158182e-05  1.23102188e+00  1.29024767e-01\n",
      "  4.67863699e+02  4.87673703e+00  3.35612920e-01  1.56561163e+00] [8.74214493e+03 8.74214493e+03 8.74214493e+03 4.34889968e+05\n",
      " 8.74214493e+03 2.75579267e+18 2.32539090e+04 4.27582506e+05\n",
      " 4.32701171e+05 6.77078746e+08 2.92859683e+03 7.86085156e-03\n",
      " 9.60922654e+08 1.42537081e+22 6.90900350e+03 2.96372395e-02\n",
      " 1.03379942e+09 1.54343166e+21 8.04276532e+03 1.01464010e+04\n",
      " 4.30492648e+05 1.18585973e+09 5.72922961e+02 2.55148870e-04\n",
      " 3.22831268e+08 1.48036362e+01 5.49204425e+03 2.60761335e-03\n",
      " 2.17550095e+09 7.12247288e+02 7.89014196e+03 1.15611231e+04\n",
      " 4.29107443e+05 1.00198598e+09 1.84065980e+02 8.12938113e-05\n",
      " 1.19629782e+08 1.06545371e+00 7.02350963e+03 3.36673947e-03\n",
      " 1.91844736e+09 6.74302704e+01 8.76099635e+03 1.66794249e+04\n",
      " 4.26543386e+05 5.46102069e+08 5.02808455e+01 2.35889673e-05\n",
      " 2.36143542e+07 1.24303933e-01 1.58410434e+04 7.95877794e-03\n",
      " 1.03856700e+09 1.30126843e+01 1.27680769e+04 3.69469792e+04\n",
      " 4.22544224e+05 3.50021619e+08 2.47065100e+01 1.20818113e-05\n",
      " 6.35544379e+06 2.07041114e-02 4.35974383e+04 2.21778975e-02\n",
      " 6.21956295e+08 5.79753417e+00 2.15040794e+04 1.00872035e+05\n",
      " 4.11576813e+05 6.54027232e+08 1.76028213e+01 9.00700622e-06\n",
      " 1.86645454e+06 3.90024173e-03 1.41498814e+05 7.48746665e-02\n",
      " 1.15353957e+09 7.95450991e+00 5.14314316e+04 3.03562761e+05\n",
      " 3.76158852e+05 5.10253780e+09 1.23301396e+01 6.71587049e-06\n",
      " 1.48932385e+06 9.25467020e-04 3.71704993e+05 2.27923911e-01\n",
      " 1.21729697e+10 1.56809361e+01 1.43709142e+05 7.24125605e+05\n",
      " 3.30876850e+05 1.50300166e+10 8.82174181e+00 4.99462775e-06\n",
      " 2.22960267e+06 3.84359455e-04 5.39470677e+05 3.35463314e-01\n",
      " 3.60935146e+10 3.43816313e+01 2.47537196e+05 9.61570159e+05\n",
      " 2.83208603e+05 2.65910930e+10 6.43030664e+00 3.47869239e-06\n",
      " 2.73613817e+06 2.00896818e-04 6.05657281e+05 3.36907941e-01\n",
      " 5.13600015e+10 8.05480477e+01 3.42331151e+05 9.91965312e+05\n",
      " 2.37390593e+05 3.49568821e+10 4.64776345e+00 2.24395407e-06\n",
      " 2.67006995e+06 1.18018478e-04 6.01867757e+05 2.90929974e-01\n",
      " 7.50556806e+10 1.20402130e+02 4.18776034e+05 9.53066947e+05]\n",
      "[ 4.32884154e-03  4.34275741e-03  4.32884154e-03  1.76798214e+03\n",
      "  4.34275741e-03  2.20069402e+07 -1.07716746e-03  1.23423522e+02\n",
      "  1.76797997e+03  2.43541131e+03  4.88134114e-03  1.16562241e-03\n",
      "  4.84725356e-03  6.35366479e+08  1.07955359e-02  2.62702394e-03\n",
      "  3.11730899e-03  7.84725715e+08  7.05271985e-03  1.29669146e-02\n",
      "  1.76797180e+03  4.83412310e+03  5.53036862e-03  1.24775683e-04\n",
      "  8.18647269e-04  3.46714990e-01  2.83608647e-02  1.19298394e-03\n",
      "  1.25158678e-02  1.66122071e+00  1.58657222e-02  3.86962183e-02\n",
      "  1.76795762e+03  6.24165248e+03  5.62445034e-03  4.00951258e-05\n",
      "  2.89199010e-03  6.30915735e-02  6.10365351e-02  1.50840287e-03\n",
      "  1.27980379e-01  8.22443476e-01  3.01402363e-02  8.55523210e-02\n",
      "  1.76791111e+03  8.71816869e+03  6.08776870e-03  1.36569652e-05\n",
      "  1.96892904e-02  1.22926258e-02  1.34515036e-01  3.79878368e-03\n",
      "  8.11780405e-01  6.14008203e-01  7.71129185e-02  2.05540186e-01\n",
      "  1.76784386e+03  1.27524073e+04  5.42892399e-03  8.26155303e-06\n",
      "  2.82724116e-02  4.03731126e-03  2.75112796e-01  1.11011760e-02\n",
      " -4.21346378e-02  5.20080676e-01  1.43711592e-01  4.13395464e-01\n",
      "  1.76770630e+03  2.44786467e+04  5.47950996e-03  6.89078407e-06\n",
      "  1.79888547e-02  1.29543447e-03  5.77385278e-01  3.68452551e-02\n",
      " -4.45249071e+00  1.01016543e+00  2.81318576e-01  8.53224344e-01\n",
      "  1.76741761e+03  6.26244486e+04  5.97749538e-03  6.14015661e-06\n",
      " -1.41768113e-01  4.15229812e-04  1.68164919e+00  1.02774488e-01\n",
      " -8.25688858e+01  1.98146200e+00  5.70509200e-01  2.24618090e+00\n",
      "  1.76705114e+03  1.11297634e+05  7.43490977e-03  6.18830782e-06\n",
      " -7.41174122e-01  1.71510363e-04  2.67265892e+00  1.44333083e-01\n",
      "  5.45335381e+01  2.73538590e+00  9.38430381e-01  3.60365439e+00\n",
      "  1.76657678e+03  1.62483824e+05  8.50011249e-03  6.30944062e-06\n",
      " -1.35055598e+00  8.66523426e-05  2.38546480e+00  1.45115803e-01\n",
      "  2.05952798e+02  3.75624181e+00  1.41385641e+00  3.79082110e+00\n",
      "  1.76610030e+03  2.11699079e+05  8.18243672e-03  5.89256189e-06\n",
      " -6.66183640e-01  5.70681039e-05  1.25279175e+00  1.27353655e-01\n",
      " -1.40900176e+02  4.87534741e+00  1.89002540e+00  3.13463471e+00] [9.74164523e+03 9.74164523e+03 9.74164523e+03 4.77425018e+05\n",
      " 9.74164523e+03 4.36475550e+18 2.60325838e+04 4.62495309e+05\n",
      " 4.74987212e+05 9.96857846e+08 3.23348958e+03 7.57218844e-03\n",
      " 1.37194791e+09 2.84392068e+20 7.57544450e+03 2.88942379e-02\n",
      " 1.51456698e+09 4.80763486e+20 8.90239531e+03 1.11193391e+04\n",
      " 4.72579446e+05 1.71435647e+09 6.14899780e+02 2.46104045e-04\n",
      " 4.69974921e+08 1.56866159e+01 5.63014176e+03 2.41059831e-03\n",
      " 3.16614133e+09 3.86263532e+02 8.55733892e+03 1.20228000e+04\n",
      " 4.71156298e+05 1.43040442e+09 1.91393544e+02 7.53538813e-05\n",
      " 1.75692739e+08 1.64304793e+00 6.87281666e+03 2.98709628e-03\n",
      " 2.74864361e+09 7.44058615e+01 9.25922251e+03 1.68584752e+04\n",
      " 4.68628126e+05 7.81807063e+08 5.15950942e+01 2.19852082e-05\n",
      " 3.52357902e+07 1.24072441e-01 1.61385598e+04 7.47800863e-03\n",
      " 1.49754767e+09 1.52216210e+01 1.31155423e+04 3.86183120e+04\n",
      " 4.64521834e+05 4.78171774e+08 2.62457454e+01 1.17450899e-05\n",
      " 9.41408332e+06 2.56516852e-02 4.69047976e+04 2.19155845e-02\n",
      " 8.71290807e+08 6.79640384e+00 2.24996541e+04 1.08251532e+05\n",
      " 4.52661778e+05 7.85006132e+08 1.88689181e+01 8.87524654e-06\n",
      " 2.66525393e+06 5.06428008e-03 1.52573969e+05 7.41087944e-02\n",
      " 1.40853961e+09 8.65877022e+00 5.48801459e+04 3.26414267e+05\n",
      " 4.14290601e+05 5.90719544e+09 1.32105076e+01 6.63706188e-06\n",
      " 1.84716780e+06 1.15955563e-03 3.99269534e+05 2.25813516e-01\n",
      " 1.41013262e+10 1.73374269e+01 1.54202219e+05 7.75412564e+05\n",
      " 3.65454823e+05 1.73034084e+10 9.45194947e+00 4.94188125e-06\n",
      " 2.62057834e+06 4.38269343e-04 5.78891384e+05 3.33038883e-01\n",
      " 4.10649976e+10 3.35849511e+01 2.65105992e+05 1.03101974e+06\n",
      " 3.14233794e+05 3.04150707e+10 6.89062533e+00 3.43434330e-06\n",
      " 3.16305532e+06 2.24803184e-04 6.48637631e+05 3.33565136e-01\n",
      " 5.77056813e+10 8.03869800e+01 3.66423932e+05 1.06334017e+06\n",
      " 2.65134859e+05 3.97383073e+10 4.98595242e+00 2.21264320e-06\n",
      " 3.03785903e+06 1.28414999e-04 6.43625795e+05 2.87015629e-01\n",
      " 8.53249593e+10 1.22091631e+02 4.48528364e+05 1.01801757e+06]\n",
      "[-8.80113499e-07  1.73056789e-06 -8.80113499e-07  1.18766472e-01\n",
      "  1.73056789e-06  7.37031621e+01  2.30745148e-08  1.88488461e-02\n",
      "  1.18765607e-01  4.10344889e-06  1.71903063e-06  5.36003177e-04\n",
      "  3.12027515e-13  3.04283962e+01  3.41165042e-06  1.29157042e-03\n",
      "  5.69038049e-12  4.12821707e+01  2.58431458e-06  4.27693437e-06\n",
      "  1.18763059e-01  8.19202342e-06  1.69091744e-06  1.12544476e-04\n",
      "  4.85823118e-11  1.02894866e+00  8.29568284e-06  1.41289141e-03\n",
      "  3.21729014e-10  5.45755493e+00  5.10431891e-06  1.17090843e-05\n",
      "  1.18758911e-01  1.25401214e-05  1.64752657e-06  5.29037732e-05\n",
      "  2.92335780e-10  2.22500107e-01  1.55912590e-05  2.69332978e-03\n",
      "  2.95606839e-09  4.13989441e+00  9.20876947e-06  2.31525019e-05\n",
      "  1.18747447e-01  2.84135370e-05  1.51025031e-06  2.93231621e-05\n",
      "  1.40657149e-09  2.21409616e-02  3.39041340e-05  1.02346169e-02\n",
      "  4.73850422e-08  4.43724444e+00  2.05352041e-05  5.29290879e-05\n",
      "  1.18730495e-01  7.04596610e-05  1.37473700e-06  2.47579226e-05\n",
      "  4.42663342e-09  7.96936763e-03  6.39046295e-05  3.42744649e-02\n",
      "  6.94486635e-08  6.19905150e+00  3.73517579e-05  9.98816503e-05\n",
      "  1.18698543e-01  2.08225112e-04  1.31696923e-06  2.20665820e-05\n",
      "  2.72200919e-09  2.32610838e-03  1.19756193e-04  1.07892749e-01\n",
      "  5.97451115e-08  1.00569706e+01  6.92463048e-05  1.87685528e-04\n",
      "  1.18638665e-01  6.29404827e-04  1.19363592e-06  1.77011205e-05\n",
      "  2.49158610e-09  5.34003367e-04  1.67761931e-04  2.18483223e-01\n",
      "  2.92989308e-07  9.54864911e+00  1.29001068e-04  2.95569363e-04\n",
      "  1.18590098e-01  1.11765597e-03  9.93002039e-07  1.30844688e-05\n",
      "  3.83290495e-09  2.06869157e-04  1.17382461e-04  2.18393929e-01\n",
      "  4.40080729e-06  5.90931480e+00  1.77367384e-04  2.93756843e-04\n",
      "  1.18554784e-01  1.57043008e-03  8.27607144e-07  9.66245604e-06\n",
      "  9.38332213e-09  1.08958579e-04  7.64304570e-05  1.84557782e-01\n",
      "  9.16118714e-06  6.60515051e+00  2.12516005e-04  2.88118854e-04\n",
      "  1.18534925e-01  1.93963431e-03  5.62104620e-07  6.61100970e-06\n",
      "  3.32570374e-08  8.95471270e-05  3.16185162e-04  1.54910407e-01\n",
      "  3.89087286e-05  7.00223923e+00  2.32109097e-04  5.47732154e-04] [1.64137926e-05 1.64137926e-05 1.64137926e-05 3.77729091e-03\n",
      " 1.64137926e-05 3.00363973e+08 4.53759205e-05 3.46453162e-03\n",
      " 3.77311764e-03 1.59168001e-08 5.06981186e-06 1.60477308e-03\n",
      " 1.71110497e-08 1.12973407e+07 1.22841406e-05 6.22559078e-03\n",
      " 3.07707007e-08 1.31239468e+07 1.42430722e-05 1.85321208e-05\n",
      " 3.76882550e-03 1.93184740e-08 1.30380549e-06 2.29074871e-04\n",
      " 7.28829589e-09 7.64125982e+03 1.73929667e-05 4.72711905e-03\n",
      " 3.50272713e-08 2.89474356e+04 1.63289224e-05 3.55457369e-05\n",
      " 3.76416963e-03 1.76162120e-08 6.10215181e-07 8.61132714e-05\n",
      " 2.58166498e-09 7.92512316e+02 3.93981778e-05 7.43800076e-03\n",
      " 2.97682870e-08 8.59622505e+03 2.44319937e-05 8.40045268e-05\n",
      " 3.74757614e-03 2.16950908e-08 3.27863956e-07 3.79748551e-05\n",
      " 5.98163336e-10 4.13950868e+00 1.68278117e-04 2.40194341e-02\n",
      " 3.46914171e-08 3.46998522e+03 6.47303297e-05 3.63518730e-04\n",
      " 3.70478079e-03 3.87904388e-08 2.51686577e-07 2.92033801e-05\n",
      " 1.98284722e-10 8.02205249e-01 5.51196616e-04 8.00661009e-02\n",
      " 5.51438270e-08 1.08712853e+03 1.80544749e-04 1.18062301e-03\n",
      " 3.56613405e-03 1.41251884e-07 1.99075067e-07 2.39831983e-05\n",
      " 1.01545561e-10 7.40052181e-02 1.68482401e-03 2.75683880e-01\n",
      " 2.18972281e-07 1.53032287e+03 5.56400572e-04 3.44929549e-03\n",
      " 3.14341204e-03 9.70836769e-07 1.33163775e-07 1.67647332e-05\n",
      " 2.06935718e-10 1.94114227e-03 3.76465886e-03 5.28123595e-01\n",
      " 1.81406087e-06 1.14736449e+03 1.59832214e-03 6.76224369e-03\n",
      " 2.65423495e-03 2.50765222e-06 8.51825868e-08 1.04653751e-05\n",
      " 3.15558614e-10 3.70770541e-04 4.40977857e-03 4.62216329e-01\n",
      " 4.57955921e-06 3.56493131e+02 2.57809815e-03 7.00733185e-03\n",
      " 2.20097315e-03 4.02300006e-06 5.27177359e-08 5.69739304e-06\n",
      " 3.38477841e-10 1.48043304e-04 4.13818933e-03 3.78026731e-01\n",
      " 8.49998164e-06 4.16721847e+02 3.24689699e-03 6.53969248e-03\n",
      " 1.83112670e-03 4.96113394e-06 3.21767141e-08 3.06246609e-06\n",
      " 2.84686230e-10 8.33767641e-05 3.68346632e-03 3.09685469e-01\n",
      " 1.19256437e-05 3.70785757e+02 3.57211252e-03 6.10638136e-03]\n",
      "[-2.16878459e-06 -7.38219233e-07 -2.16878459e-06  1.67624221e-01\n",
      " -7.38219233e-07  4.67355141e+01 -4.48887017e-09  7.75550634e-03\n",
      "  1.67624590e-01  1.34791997e-05 -7.35974798e-07  1.52429577e-03\n",
      "  1.39255115e-12  2.26818330e+01 -1.48906163e-06  6.68835566e-03\n",
      " -2.66938673e-11  3.34548576e+01 -1.10508441e-06 -1.85817125e-06\n",
      "  1.67625704e-01  3.56418913e-05 -7.27942284e-07  5.63316309e-04\n",
      " -1.88314598e-11  1.10904904e+00 -3.52041271e-06  1.98844617e-02\n",
      " -1.30350864e-10  4.75536917e+01 -2.21046223e-06 -5.00293266e-06\n",
      "  1.67627464e-01  6.18074323e-05 -7.01145672e-07  2.46174940e-04\n",
      " -3.34172138e-11  3.40200033e-01 -6.75141927e-06  2.50690477e-02\n",
      "  1.28983410e-11  1.38078626e+01 -3.94387197e-06 -9.99414557e-06\n",
      "  1.67632605e-01  1.20333091e-04 -7.14962563e-07  5.94940564e-05\n",
      " -2.21655992e-10  8.90468180e-02 -1.38184106e-05  4.09661755e-02\n",
      "  3.78991429e-08  5.33254791e+01 -9.09916591e-06 -2.22026140e-05\n",
      "  1.67639514e-01  2.21587980e-04 -4.21063383e-07  4.50289219e-05\n",
      "  3.73731254e-09  2.26429823e-02  4.80249301e-06  1.71314634e-01\n",
      "  1.46508271e-07  3.50519926e+02 -1.57144720e-05 -1.04909156e-05\n",
      "  1.67637113e-01  4.90445572e-04  2.32473019e-08  3.21361679e-05\n",
      "  3.88978717e-09  2.76143571e-03  3.68796473e-05  2.05584573e-01\n",
      "  1.32251865e-07  9.15516619e+00 -1.28689148e-05  2.39874852e-05\n",
      "  1.67618673e-01  1.24896102e-03  1.84721203e-07  1.08021461e-05\n",
      "  1.25636513e-09  4.57382290e-04  1.29136730e-05  2.61832029e-01\n",
      "  3.01662985e-07  6.19341396e+00  5.73238272e-06  1.84613346e-05\n",
      "  1.67606658e-01  2.11804367e-03  9.11583087e-08  6.50707213e-06\n",
      "  1.04771949e-09  2.01813323e-04 -1.65767119e-04  2.61149637e-01\n",
      "  7.95115718e-06  6.30399368e+00  1.76544135e-05 -1.48203864e-04\n",
      "  1.67612217e-01  2.95035263e-03 -1.21536585e-07  2.57933715e-06\n",
      "  1.14891210e-08  1.21055551e-04 -3.41007408e-04  2.12569533e-01\n",
      "  1.49130866e-05  9.26751103e+00  1.18829615e-05 -3.29002910e-04\n",
      "  1.67640623e-01  3.66645624e-03 -4.43102466e-07 -1.01924275e-06\n",
      "  4.99448964e-08  1.01875202e-04 -1.56996810e-04  1.70175445e-01\n",
      "  4.13283656e-05  1.05433094e+01 -1.68453771e-05 -1.73399084e-04] [5.39167984e-05 5.39167984e-05 5.39167984e-05 6.30637377e-03\n",
      " 5.39167984e-05 1.39891260e+08 1.23691411e-04 6.26869043e-03\n",
      " 6.29290623e-03 5.99156617e-07 2.29939429e-05 2.06317394e-02\n",
      " 9.94399270e-07 3.28189504e+06 6.26632911e-05 4.20082622e-01\n",
      " 1.02060506e-06 3.89135174e+06 5.94670867e-05 9.28178547e-05\n",
      " 6.27078105e-03 1.70437238e-06 7.00865191e-06 4.68134854e-03\n",
      " 5.25846867e-07 1.81171737e+03 1.04661891e-04 2.46443602e+00\n",
      " 1.78391308e-06 1.49959732e+07 8.43466614e-05 1.92049490e-04\n",
      " 6.24467304e-03 2.98197522e-06 2.92237172e-06 1.15597566e-03\n",
      " 1.40542686e-07 5.78471122e+02 1.62407762e-04 2.60515902e+00\n",
      " 2.77998937e-06 1.50468373e+05 1.23734666e-04 3.14415288e-04\n",
      " 6.18632309e-03 5.28644694e-06 9.85125311e-07 1.50803705e-04\n",
      " 3.83127904e-08 1.45153842e+02 4.05095162e-04 1.09036243e+00\n",
      " 5.97130354e-06 1.48575601e+07 2.22044194e-04 8.70082803e-04\n",
      " 6.08529963e-03 6.76336754e-06 5.72007798e-07 1.09258527e-04\n",
      " 1.77487807e-08 5.63682371e+00 1.07572336e-03 1.43719125e+01\n",
      " 1.11516931e-05 1.70893685e+08 4.61079875e-04 2.30475983e-03\n",
      " 5.81623095e-03 6.40822734e-06 3.83785335e-07 7.16649401e-05\n",
      " 6.61771163e-09 4.03749892e-02 3.03432495e-03 6.79678913e+00\n",
      " 1.15369625e-05 1.48845582e+03 1.13806937e-03 6.25405025e-03\n",
      " 5.05688829e-03 7.36643636e-06 2.41919718e-07 1.96165114e-05\n",
      " 2.75827408e-09 1.11038471e-03 6.80616959e-03 7.70051802e-01\n",
      " 1.42740526e-05 8.92372043e+02 2.96667257e-03 1.26260732e-02\n",
      " 4.18736140e-03 1.04578715e-05 1.58513751e-07 1.20122009e-05\n",
      " 2.02371635e-09 4.11610907e-04 8.47717514e-03 6.86662165e-01\n",
      " 2.08924889e-05 5.95993558e+02 4.74515752e-03 1.40264553e-02\n",
      " 3.35498123e-03 1.34921330e-05 1.03707286e-07 6.50989979e-06\n",
      " 1.68224255e-09 1.92762280e-04 8.35321986e-03 5.15532366e-01\n",
      " 2.94480124e-05 1.04785032e+03 6.11213232e-03 1.33488266e-02\n",
      " 2.63636265e-03 1.51440205e-05 6.56893684e-08 3.40615795e-06\n",
      " 1.30205214e-09 1.28516832e-04 7.64437727e-03 3.84919146e-01\n",
      " 3.89866706e-05 1.12002808e+03 6.91775630e-03 1.25580771e-02]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(scaler_list_new)):\n",
    "    print(scaler_list_new[i].mean_, scaler_list_new[i].var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function inverse_transform in module sklearn.preprocessing.data:\n",
      "\n",
      "inverse_transform(self, X, copy=None)\n",
      "    Scale back the data to the original representation\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape [n_samples, n_features]\n",
      "        The data used to scale along the features axis.\n",
      "    copy : bool, optional (default: None)\n",
      "        Copy the input X or not.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X_tr : array-like, shape [n_samples, n_features]\n",
      "        Transformed array.\n",
      "\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "help(StandardScaler.inverse_transform)\n",
    "print(data_features_diff_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127563, 128)\n",
      "(127563, 128)\n",
      "(127614, 128)\n",
      "(255177, 128)\n",
      "(128036, 128)\n",
      "(383213, 128)\n",
      "(127680, 128)\n",
      "(510893, 128)\n",
      "(128129, 128)\n",
      "(639022, 128)\n",
      "(127972, 128)\n",
      "(766994, 128)\n",
      "(127668, 128)\n",
      "(894662, 128)\n",
      "(127854, 128)\n",
      "(1022516, 128)\n",
      "(7785, 128)\n",
      "(1030301, 128)\n",
      "(9961, 128)\n",
      "(1040262, 128)\n",
      "(7249, 128)\n",
      "(1047511, 128)\n",
      "(7248, 128)\n",
      "(1054759, 128)\n",
      "(7785, 128)\n",
      "(1062544, 128)\n",
      "(7785, 128)\n",
      "(1070329, 128)\n",
      "(7785, 128)\n",
      "(1078114, 128)\n",
      "(146025, 128)\n",
      "(1224139, 128)\n",
      "(146010, 128)\n",
      "(1370149, 128)\n",
      "(99255, 128)\n",
      "(1469404, 128)\n",
      "(146630, 128)\n",
      "(1616034, 128)\n",
      "(146669, 128)\n",
      "(1762703, 128)\n",
      "(146690, 128)\n",
      "(1909393, 128)\n",
      "(136926, 128)\n",
      "(2046319, 128)\n",
      "(128454, 128)\n",
      "(2174773, 128)\n",
      "(146681, 128)\n",
      "(2321454, 128)\n",
      "(64450, 128)\n",
      "(2385904, 128)\n",
      "(64437, 128)\n",
      "(2450341, 128)\n"
     ]
    }
   ],
   "source": [
    "train_feature = None\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "for i in range(len(scaler_list_new)):\n",
    "    index2 += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    temp = timeseries_features_new[index1:index2,]\n",
    "    print(temp.shape)\n",
    "    temp = scaler_list_new[i].inverse_transform(temp)\n",
    "    if i == 0:\n",
    "        train_feature = temp\n",
    "    else:\n",
    "        train_feature = np.concatenate((train_feature, temp), axis = 0)\n",
    "    index1 = index2\n",
    "    print(train_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.33424271e-06  3.33067411e-06  3.33424271e-06 ... -1.10451253e-02\n",
      "   4.47285694e-04  5.72909585e-05]\n",
      " [ 3.33023637e-06  3.32666778e-06  3.33023637e-06 ... -1.10450991e-02\n",
      "   4.47284249e-04  5.72898743e-05]\n",
      " [ 3.33424271e-06  3.33067411e-06  3.33424271e-06 ... -1.10450668e-02\n",
      "   4.47284330e-04  5.72898430e-05]\n",
      " ...\n",
      " [ 3.33410445e-06  3.33053563e-06  3.33410445e-06 ... -1.10465265e-02\n",
      "   4.47326071e-04  5.73205080e-05]\n",
      " [ 3.33422634e-06  3.33065752e-06  3.33422634e-06 ... -1.10465272e-02\n",
      "   4.47326057e-04  5.73205133e-05]\n",
      " [ 3.33418061e-06  3.33061179e-06  3.33418061e-06 ... -1.10465280e-02\n",
      "   4.47326025e-04  5.73205065e-05]]\n"
     ]
    }
   ],
   "source": [
    "scaler_new = StandardScaler()\n",
    "train_feature = scaler_new.fit_transform(train_feature)\n",
    "print(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111628, 128)\n",
      "(111628, 128)\n",
      "(109186, 128)\n",
      "(220814, 128)\n",
      "(111114, 128)\n",
      "(331928, 128)\n",
      "(107748, 128)\n",
      "(439676, 128)\n",
      "(110172, 128)\n",
      "(549848, 128)\n",
      "(110005, 128)\n",
      "(659853, 128)\n",
      "(111787, 128)\n",
      "(771640, 128)\n",
      "(111117, 128)\n",
      "(882757, 128)\n",
      "(7785, 128)\n",
      "(890542, 128)\n",
      "(9961, 128)\n",
      "(900503, 128)\n",
      "(7250, 128)\n",
      "(907753, 128)\n",
      "(7249, 128)\n",
      "(915002, 128)\n",
      "(7785, 128)\n",
      "(922787, 128)\n",
      "(7785, 128)\n",
      "(930572, 128)\n",
      "(7785, 128)\n",
      "(938357, 128)\n",
      "(146025, 128)\n",
      "(1084382, 128)\n",
      "(146011, 128)\n",
      "(1230393, 128)\n",
      "(107902, 128)\n",
      "(1338295, 128)\n",
      "(146630, 128)\n",
      "(1484925, 128)\n",
      "(146670, 128)\n",
      "(1631595, 128)\n",
      "(146691, 128)\n",
      "(1778286, 128)\n",
      "(136926, 128)\n",
      "(1915212, 128)\n",
      "(128455, 128)\n",
      "(2043667, 128)\n",
      "(146682, 128)\n",
      "(2190349, 128)\n",
      "(64451, 128)\n",
      "(2254800, 128)\n",
      "(64437, 128)\n",
      "(2319237, 128)\n"
     ]
    }
   ],
   "source": [
    "test_feature = None\n",
    "index1 = 0\n",
    "index2 = 0\n",
    "for i in range(len(scaler_list_new)):\n",
    "    index2 += len(testseries_all[i]) - data_features_diff_avg\n",
    "    temp = testseries_features_new[index1:index2,]\n",
    "    print(temp.shape)\n",
    "    temp = scaler_list_new[i].inverse_transform(temp)\n",
    "    if i == 0:\n",
    "        test_feature = temp\n",
    "    else:\n",
    "        test_feature = np.concatenate((test_feature, temp), axis = 0)\n",
    "    index1 = index2\n",
    "    print(test_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.31995145e-06  3.31638251e-06  3.31995145e-06 ... -1.10462739e-02\n",
      "   4.47330416e-04  5.73421658e-05]\n",
      " [ 3.34424717e-06  3.34067823e-06  3.34424717e-06 ... -1.10462716e-02\n",
      "   4.47334149e-04  5.73448953e-05]\n",
      " [ 3.33853054e-06  3.33496160e-06  3.33853054e-06 ... -1.10462718e-02\n",
      "   4.47335700e-04  5.73460966e-05]\n",
      " ...\n",
      " [ 3.33412088e-06  3.33055199e-06  3.33412088e-06 ... -1.10461196e-02\n",
      "   4.47323109e-04  5.73197949e-05]\n",
      " [ 3.33398312e-06  3.33041424e-06  3.33398312e-06 ... -1.10461077e-02\n",
      "   4.47323015e-04  5.73197205e-05]\n",
      " [ 3.33414191e-06  3.33057303e-06  3.33414191e-06 ... -1.10460951e-02\n",
      "   4.47322982e-04  5.73196885e-05]]\n"
     ]
    }
   ],
   "source": [
    "test_feature = scaler_new.transform(test_feature)\n",
    "print(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(512, input_dim = 128))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.2))\n",
    "\n",
    "m.add(Dense(256))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('relu'))\n",
    "m.add(Dropout(0.2))\n",
    "\n",
    "m.add(Dense(1))\n",
    "m.add(BatchNormalization())\n",
    "m.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "m.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n",
      "[1. 1. 1. ... 0. 0. 0.] (2450341,)\n",
      "[1. 1. 1. ... 0. 0. 0.] 3095954.0\n",
      "[ 1.  1.  1. ... 36. 36. 36.] 5011838.0\n"
     ]
    }
   ],
   "source": [
    "ratio = round((len(timeseries_features_label_new) - sum(timeseries_features_label_new)) * 0.8 / sum(timeseries_features_label_new))\n",
    "print(ratio)\n",
    "non_anomaly = np.ones(len(timeseries_features_label_new)) - timeseries_features_label_new\n",
    "print(non_anomaly,non_anomaly.shape)\n",
    "sample_ratio = (4*ratio) * timeseries_features_label_vital_new + non_anomaly\n",
    "print(sample_ratio,sum(sample_ratio))\n",
    "sample_ratio = sample_ratio + ratio * timeseries_features_label_new\n",
    "print(sample_ratio,sum(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450341 2450341 2450341\n"
     ]
    }
   ],
   "source": [
    "print(len(train_feature),len(timeseries_features_label),len(sample_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras: start to train DNN!\n",
      "Epoch 1/10\n",
      "2450341/2450341 [==============================] - 79s 32us/step - loss: 1.3566 - binary_accuracy: 0.1367\n",
      "Epoch 2/10\n",
      "2450341/2450341 [==============================] - 73s 30us/step - loss: 1.3345 - binary_accuracy: 0.1974\n",
      "Epoch 3/10\n",
      " 545000/2450341 [=====>........................] - ETA: 56s - loss: 1.3227 - binary_accuracy: 0.2558"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-42bfa1a5734c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m h = m.fit(train_feature, timeseries_features_label_new, epochs=10, batch_size=5000, verbose=1,\n\u001b[0;32m----> 4\u001b[0;31m                    sample_weight=sample_ratio)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Keras: start to train DNN!')\n",
    "start_time = time.time()\n",
    "h = m.fit(train_feature, timeseries_features_label_new, epochs=10, batch_size=5000, verbose=1,\n",
    "                   sample_weight=sample_ratio)\n",
    "\n",
    "end_time = time.time()\n",
    "print('It took %d seconds to train the model!' %(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DMatrix in module xgboost.core:\n",
      "\n",
      "class DMatrix(builtins.object)\n",
      " |  DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      " |  \n",
      " |  Data Matrix used in XGBoost.\n",
      " |  \n",
      " |  DMatrix is a internal data structure that used by XGBoost\n",
      " |  which is optimized for both memory efficiency and training speed.\n",
      " |  You can construct DMatrix from numpy.arrays\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __init__(self, data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : string/numpy.array/scipy.sparse/pd.DataFrame/dt.Frame\n",
      " |          Data source of DMatrix.\n",
      " |          When data is string type, it represents the path libsvm format txt file,\n",
      " |          or binary file that xgboost can read from.\n",
      " |      label : list or numpy 1-D array, optional\n",
      " |          Label of the training data.\n",
      " |      missing : float, optional\n",
      " |          Value in the data which needs to be present as a missing value. If\n",
      " |          None, defaults to np.nan.\n",
      " |      weight : list or numpy 1-D array , optional\n",
      " |          Weight for each instance.\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each data\n",
      " |              point). This is because we only care about the relative ordering of\n",
      " |              data points within each group, so it doesn't make sense to assign\n",
      " |              weights to individual data points.\n",
      " |      \n",
      " |      silent : boolean, optional\n",
      " |          Whether print messages during construction\n",
      " |      feature_names : list, optional\n",
      " |          Set names for features.\n",
      " |      feature_types : list, optional\n",
      " |          Set types for features.\n",
      " |      nthread : integer, optional\n",
      " |          Number of threads to use for loading data from numpy array. If -1,\n",
      " |          uses maximum threads available on the system.\n",
      " |  \n",
      " |  get_base_margin(self)\n",
      " |      Get the base margin of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      base_margin : float\n",
      " |  \n",
      " |  get_float_info(self, field)\n",
      " |      Get float property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of float information of the data\n",
      " |  \n",
      " |  get_label(self)\n",
      " |      Get the label of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      label : array\n",
      " |  \n",
      " |  get_uint_info(self, field)\n",
      " |      Get unsigned integer property from the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      info : array\n",
      " |          a numpy array of unsigned integer information of the data\n",
      " |  \n",
      " |  get_weight(self)\n",
      " |      Get the weight of the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      weight : array\n",
      " |  \n",
      " |  num_col(self)\n",
      " |      Get the number of columns (features) in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of columns : int\n",
      " |  \n",
      " |  num_row(self)\n",
      " |      Get the number of rows in the DMatrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      number of rows : int\n",
      " |  \n",
      " |  save_binary(self, fname, silent=True)\n",
      " |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      " |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Name of the output buffer file.\n",
      " |      silent : bool (optional; default: True)\n",
      " |          If set, the output is suppressed.\n",
      " |  \n",
      " |  set_base_margin(self, margin)\n",
      " |      Set base margin of booster to start from.\n",
      " |      \n",
      " |      This can be used to specify a prediction value of\n",
      " |      existing model to be base_margin\n",
      " |      However, remember margin is needed, instead of transformed prediction\n",
      " |      e.g. for logistic regression: need to put in value before logistic transformation\n",
      " |      see also example/demo.py\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      margin: array like\n",
      " |          Prediction margin of each datapoint\n",
      " |  \n",
      " |  set_float_info(self, field, data)\n",
      " |      Set float type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_float_info_npy2d(self, field, data)\n",
      " |      Set float type property into the DMatrix\n",
      " |         for numpy 2d array input\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_group(self, group)\n",
      " |      Set group size of DMatrix (used for ranking).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : array like\n",
      " |          Group size of each group\n",
      " |  \n",
      " |  set_label(self, label)\n",
      " |      Set label of dmatrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      label: array like\n",
      " |          The label information to be set into DMatrix\n",
      " |  \n",
      " |  set_label_npy2d(self, label)\n",
      " |      Set label of dmatrix\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      label: array like\n",
      " |          The label information to be set into DMatrix\n",
      " |          from numpy 2D array\n",
      " |  \n",
      " |  set_uint_info(self, field, data)\n",
      " |      Set uint type property into the DMatrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      field: str\n",
      " |          The field name of the information\n",
      " |      \n",
      " |      data: numpy array\n",
      " |          The array of data to be set\n",
      " |  \n",
      " |  set_weight(self, weight)\n",
      " |      Set weight of each instance.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weight : array like\n",
      " |          Weight for each data point\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each data\n",
      " |              point). This is because we only care about the relative ordering of\n",
      " |              data points within each group, so it doesn't make sense to assign\n",
      " |              weights to individual data points.\n",
      " |  \n",
      " |  set_weight_npy2d(self, weight)\n",
      " |      Set weight of each instance\n",
      " |          for numpy 2D array\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weight : array like\n",
      " |          Weight for each data point in numpy 2D array\n",
      " |      \n",
      " |          .. note:: For ranking task, weights are per-group.\n",
      " |      \n",
      " |              In ranking task, one weight is assigned to each group (not each data\n",
      " |              point). This is because we only care about the relative ordering of\n",
      " |              data points within each group, so it doesn't make sense to assign\n",
      " |              weights to individual data points.\n",
      " |  \n",
      " |  slice(self, rindex)\n",
      " |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rindex : list\n",
      " |          List of indices to be selected.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      res : DMatrix\n",
      " |          A new DMatrix containing only selected indices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  feature_names\n",
      " |      Get feature names (column labels).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list or None\n",
      " |  \n",
      " |  feature_types\n",
      " |      Get feature types (column types).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_types : list or None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb.DMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_jobs=4, verbosity=1, silent = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=4,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(timeseries_features_new, timeseries_features_label_new, sample_weight = sample_ratio, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = xgb_model.predict_proba(timeseries_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.02038777459953533\n",
      "0.7979662509758392\n",
      "0.7490557883462673\n",
      "0.7727378460107001\n"
     ]
    }
   ],
   "source": [
    "train_data_check = (np.ravel(y_p[:,1:])>0.85).astype(int)\n",
    "print(train_data_check)\n",
    "print(sum(train_data_check)/len(train_data_check))\n",
    "print(precision_score(timeseries_features_label, train_data_check))\n",
    "print(recall_score(timeseries_features_label, train_data_check))\n",
    "print(f1_score(timeseries_features_label, train_data_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128562 127563\n",
      "257175 255177\n",
      "386210 383213\n",
      "514889 510893\n",
      "644017 639022\n",
      "772988 766994\n",
      "901655 894662\n",
      "1030508 1022516\n",
      "1039292 1030301\n",
      "1050252 1040262\n",
      "1058500 1047511\n",
      "1066747 1054759\n",
      "1075531 1062544\n",
      "1084315 1070329\n",
      "1093099 1078114\n",
      "1240123 1224139\n",
      "1387132 1370149\n",
      "1487386 1469404\n",
      "1635015 1616034\n",
      "1782683 1762703\n",
      "1930372 1909393\n",
      "2068297 2046319\n",
      "2197750 2174773\n",
      "2345430 2321454\n",
      "2410879 2385904\n",
      "2476315 2450341\n",
      "2476315\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "evaluation_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(timeseries_all)):\n",
    "    next_index += len(timeseries_all[i]) - data_features_diff_avg\n",
    "    evaluation_new = np.concatenate((evaluation_new, train_data_check[last_index : next_index]))\n",
    "    print(len(evaluation_new),next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(timeseries_all)-1:\n",
    "        evaluation_new = np.concatenate((evaluation_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(evaluation_new))\n",
    "assert(len(evaluation_new) == len(train_data))\n",
    "evaluation_new = evaluation_new.astype(int)\n",
    "evaluation_df = pd.DataFrame({'KPI ID': train_data['KPI ID'], \n",
    "                         'timestamp': train_data['timestamp'], \n",
    "                         'predict': evaluation_new})\n",
    "evaluation_df.to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0\n",
      "OMP: Info #156: KMP_AFFINITY: 1 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 1 threads/core (1 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 32039 tid 32039 thread 0 bound to OS proc set 0\n",
      "{\"result\": true, \"data\": 0.8365076751376767, \"message\": \"\"}\n"
     ]
    }
   ],
   "source": [
    "!python evaluation.py \"../../input/train.csv\" \"evaluation.csv\" 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = xgb_model.predict_proba(testseries_features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.007940111338341015\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_t[:,1:])>0.93).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictx4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.016826654628224715\n"
     ]
    }
   ],
   "source": [
    "predict_flagm = (np.ravel(pm_t)>0.99).astype(int)\n",
    "print(predict_flagm)\n",
    "print(sum(predict_flagm)/len(predict_flagm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.32250184 -0.42262426 -0.39404276 ... -0.45441595 -0.4576002\n",
      " -0.4510805 ]\n"
     ]
    }
   ],
   "source": [
    "temp1 = np.ravel(pm_t) - 0.49\n",
    "print(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.014869114281981531\n"
     ]
    }
   ],
   "source": [
    "predict_flag = (np.ravel(p_t[:,1:])>0.85).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23618336 -0.13017786 -0.154143   ...  0.33845142  0.36765185\n",
      "  0.3452185 ]\n"
     ]
    }
   ],
   "source": [
    "temp2 = np.ravel(p_t[:,1:])- 0.35\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "0.008550225785463064\n"
     ]
    }
   ],
   "source": [
    "predict_flag = ((temp1*0.4 + temp2 * 0.6)>0.5).astype(int)\n",
    "print(predict_flag)\n",
    "print(sum(predict_flag)/len(predict_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111628\n",
      "220814\n",
      "331928\n",
      "439676\n",
      "549848\n",
      "659853\n",
      "771640\n",
      "882757\n",
      "890542\n",
      "900503\n",
      "907753\n",
      "915002\n",
      "922787\n",
      "930572\n",
      "938357\n",
      "1084382\n",
      "1230393\n",
      "1338295\n",
      "1484925\n",
      "1631595\n",
      "1778286\n",
      "1915212\n",
      "2043667\n",
      "2190349\n",
      "2254800\n",
      "2319237\n",
      "2345211\n"
     ]
    }
   ],
   "source": [
    "last_index = 0\n",
    "predict_new = np.zeros(data_features_diff_avg).astype(int)\n",
    "next_index = 0\n",
    "for i in range(len(testseries_all)):\n",
    "    next_index += len(testseries_all[i]) - data_features_diff_avg\n",
    "    predict_new = np.concatenate((predict_new, predict_flag[last_index : next_index]))\n",
    "    print(next_index)\n",
    "    last_index = next_index\n",
    "    if i != len(testseries_all)-1:\n",
    "        predict_new = np.concatenate((predict_new,np.zeros(data_features_diff_avg)))\n",
    "print(len(predict_new))\n",
    "assert(len(predict_new) == len(test_data))\n",
    "predict_new = predict_new.astype(int)\n",
    "predict_df = pd.DataFrame({'KPI ID': test_data['KPI ID'], \n",
    "                         'timestamp': test_data['timestamp'], \n",
    "                         'predict': predict_new})\n",
    "predict_df.to_csv('predictu2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('firstModel.txt','wb')\n",
    "pickle.dump(m,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('xgboost.txt','wb')\n",
    "pickle.dump(xgb_model,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sp = np.where(timeseries_label[1:]!=timeseries_label[:-1])\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeseries_label[1:]!=timeseries_label[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([0,1,0,1,1,0,1,1,1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True,  True, False, False,  True,\n",
       "       False, False])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array[1:]!=test_array[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 5, 8])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(test_array[1:]!=test_array[:-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  max_depth : int\n",
      " |      Maximum tree depth for base learners.\n",
      " |  learning_rate : float\n",
      " |      Boosting learning rate (xgb's \"eta\")\n",
      " |  n_estimators : int\n",
      " |      Number of trees to fit.\n",
      " |  verbosity : int\n",
      " |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |  silent : boolean\n",
      " |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      " |  objective : string or callable\n",
      " |      Specify the learning task and the corresponding learning objective or\n",
      " |      a custom objective function to be used (see note below).\n",
      " |  booster: string\n",
      " |      Specify which booster to use: gbtree, gblinear or dart.\n",
      " |  nthread : int\n",
      " |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      " |  n_jobs : int\n",
      " |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      " |  gamma : float\n",
      " |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      " |  min_child_weight : int\n",
      " |      Minimum sum of instance weight(hessian) needed in a child.\n",
      " |  max_delta_step : int\n",
      " |      Maximum delta step we allow each tree's weight estimation to be.\n",
      " |  subsample : float\n",
      " |      Subsample ratio of the training instance.\n",
      " |  colsample_bytree : float\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |  colsample_bylevel : float\n",
      " |      Subsample ratio of columns for each level.\n",
      " |  colsample_bynode : float\n",
      " |      Subsample ratio of columns for each split.\n",
      " |  reg_alpha : float (xgb's alpha)\n",
      " |      L1 regularization term on weights\n",
      " |  reg_lambda : float (xgb's lambda)\n",
      " |      L2 regularization term on weights\n",
      " |  scale_pos_weight : float\n",
      " |      Balancing of positive and negative weights.\n",
      " |  base_score:\n",
      " |      The initial prediction score of all instances, global bias.\n",
      " |  seed : int\n",
      " |      Random number seed.  (Deprecated, please use random_state)\n",
      " |  random_state : int\n",
      " |      Random number seed.  (replaces seed)\n",
      " |  missing : float, optional\n",
      " |      Value in the data which needs to be present as a missing value. If\n",
      " |      None, defaults to np.nan.\n",
      " |  importance_type: string, default \"gain\"\n",
      " |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      " |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      " |  \\*\\*kwargs : dict, optional\n",
      " |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      " |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      " |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      " |      will result in a TypeError.\n",
      " |  \n",
      " |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      " |          passed via this argument will interact properly with scikit-learn.\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  A custom objective function can be provided for the ``objective``\n",
      " |  parameter. In this case, it should have the signature\n",
      " |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |  y_true: array_like of shape [n_samples]\n",
      " |      The target values\n",
      " |  y_pred: array_like of shape [n_samples]\n",
      " |      The predicted values\n",
      " |  \n",
      " |  grad: array_like of shape [n_samples]\n",
      " |      The value of the gradient for each sample point.\n",
      " |  hess: array_like of shape [n_samples]\n",
      " |      The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  evals_result(self)\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the `fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      " |      When **eval_metric** is also passed to the `fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result : dictionary\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      " |      \n",
      " |          clf = xgb.XGBClassifier(**param_dist)\n",
      " |      \n",
      " |          clf.fit(X_train, y_train,\n",
      " |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      " |                  eval_metric='logloss',\n",
      " |                  verbose=True)\n",
      " |      \n",
      " |          evals_result = clf.evals_result()\n",
      " |      \n",
      " |      The variable **evals_result** will contain\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      " |      Fit gradient boosting classifier\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix\n",
      " |      y : array_like\n",
      " |          Labels\n",
      " |      sample_weight : array_like\n",
      " |          Weight for each instance\n",
      " |      eval_set : list, optional\n",
      " |          A list of (X, y) pairs to use as a validation set for\n",
      " |          early-stopping\n",
      " |      sample_weight_eval_set : list, optional\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      " |          instance weights on the i-th validation set.\n",
      " |      eval_metric : str, callable, optional\n",
      " |          If a str, should be a built-in evaluation metric to use. See\n",
      " |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      " |          signature is func(y_predicted, y_true) where y_true will be a\n",
      " |          DMatrix object such that you may need to call the get_label\n",
      " |          method. It must return a str, value pair where the str is a name\n",
      " |          for the evaluation and value is the value of the evaluation\n",
      " |          function. This objective is always minimized.\n",
      " |      early_stopping_rounds : int, optional\n",
      " |          Activates early stopping. Validation error needs to decrease at\n",
      " |          least every <early_stopping_rounds> round(s) to continue training.\n",
      " |          Requires at least one item in evals. If there's more than one,\n",
      " |          will use the last. If early stopping occurs, the model will have\n",
      " |          three additional fields: bst.best_score, bst.best_iteration and\n",
      " |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      " |          default value in predict method if not any other value is specified).\n",
      " |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      " |          and/or num_class appears in the parameters)\n",
      " |      verbose : bool\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      " |          metric measured on the validation set to stderr.\n",
      " |      xgb_model : str\n",
      " |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      callbacks : list of callback functions\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      " |          Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      " |  \n",
      " |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      " |      Predict with `data`.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe.\n",
      " |      \n",
      " |        For each booster object, predict can only be called from one thread.\n",
      " |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |        of model object and then call ``predict()``.\n",
      " |      \n",
      " |      .. note:: Using ``predict()`` with DART booster\n",
      " |      \n",
      " |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      " |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      " |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      " |        a nonzero value, e.g.\n",
      " |      \n",
      " |        .. code-block:: python\n",
      " |      \n",
      " |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      output_margin : bool\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |  \n",
      " |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      " |      Predict the probability of each `data` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is not thread safe\n",
      " |      \n",
      " |          For each booster object, predict can only be called from one thread.\n",
      " |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      " |          of model object and then call predict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : DMatrix\n",
      " |          The dmatrix storing the input.\n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      " |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      " |          Otherwise, it is assumed that the feature_names are the same.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : numpy array\n",
      " |          a numpy array with the probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  apply(self, X, ntree_limit=0)\n",
      " |      Return the predicted leaf every tree for each sample.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      ntree_limit : int\n",
      " |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  get_booster(self)\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self)\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep=False)\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self)\n",
      " |      Get xgboost type parameters.\n",
      " |  \n",
      " |  load_model(self, fname)\n",
      " |      Load the model from a file.\n",
      " |      \n",
      " |      The model is loaded from an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or a memory buffer\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname)\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal binary format which is\n",
      " |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      " |      the Python Booster object (such as feature names) will not be loaded.\n",
      " |      Label encodings (text labels to numeric labels) will be also lost.\n",
      " |      **If you are using only the Python interface, we recommend pickling the\n",
      " |      model object for best results.**\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      " |      the full range of xgboost parameters that are not defined as member variables\n",
      " |      in sklearn grid search.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from XGBModel:\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property\n",
      " |      \n",
      " |      .. note:: Feature importance is defined only for tree boosters\n",
      " |      \n",
      " |          Feature importance is only defined when the decision tree model is chosen as base\n",
      " |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      " |          as linear learners (`booster=gblinear`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]``\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      " |          as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "help(xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
